{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aded5ace-7694-47d0-a8d9-ec0ea6a3bd54",
   "metadata": {},
   "source": [
    "# Run Qwen QwQ 32B reasoning model efficient on Amazon SageMaker AI with SGLang and auto scale down to zero\n",
    "\n",
    "> This notebook has been tested on the Python 3 kernel of a SageMaker Jupternotebook instance on a ml.m5.xlarge instance with 50GB of disk size\n",
    "\n",
    "Amazon SageMaker AI provides the ability to build Docker containers to run on SageMaker endpoints, where they listen for health checks on /ping and receive real-time inference requests on /invocations. Using SageMaker AI for inference offers several benefits:\n",
    "\n",
    "- **Scalability**: SageMaker AI can automatically scale your inference endpoints up and down based on demand, ensuring your models can handle varying workloads.\n",
    "- **High Availability**: SageMaker AI manages the infrastructure and maintains the availability of your inference endpoints, so you don't have to worry about managing the underlying resources.\n",
    "- **Monitoring and Logging**: SageMaker AI provides built-in monitoring and logging capabilities, making it easier to track the performance and health of your inference endpoints.\n",
    "- **Security**: SageMaker AI integrates with other AWS services, such as AWS Identity and Access Management (IAM), to provide robust security controls for your inference workloads.\n",
    "\n",
    "Note that SageMaker provides [pre-built SageMaker AI Docker images](https://docs.aws.amazon.com/sagemaker/latest/dg/pre-built-containers-frameworks-deep-learning.html) that can help you quickly start with the model inference on SageMaker. It also allows you to [bring your own Docker container](https://docs.aws.amazon.com/sagemaker/latest/dg/adapt-inference-container.html) and use it inside SageMaker AI for training and inference. To be compatible with SageMaker AI, your container must have the following characteristics:\n",
    "\n",
    "- Your container must have a web server listening on port 8080.\n",
    "- Your container must accept POST requests to the /invocations and /ping real-time endpoints.\n",
    "\n",
    "In this notebook, we'll demonstrate how to adapt the [**SGLang**](https://github.com/sgl-project/sglang) framework to run on SageMaker AI endpoints. SGLang is a serving framework for large language models that provides state-of-the-art performance, including a fast backend runtime for efficient serving with RadixAttention, extensive model support, and an active open-source community. For more information refer to https://docs.sglang.ai/index.html and https://github.com/sgl-project/sglang.\n",
    "\n",
    "By using SGLang and building a custom Docker container, you can run advanced AI models like the [Qwen QwQ 32B](https://huggingface.co/Qwen/QwQ-32B) on a SageMaker AI endpoint.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a1d7a83c-d175-4443-b9cf-26d8bad5fedb",
   "metadata": {},
   "source": [
    "## Prepare the SGLang SageMaker container\n",
    "\n",
    "SageMaker AI makes extensive use of Docker containers for build and runtime tasks. Using containers, you can train machine learning algorithms and deploy models quickly and reliably at any scale. See [this link](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-code.html#your-algorithms-inference-code-run-image) to understand how SageMaker AI runs your inference image. \n",
    "\n",
    "- For model inference, SageMaker AI runs the container as:\n",
    "```\n",
    "docker run image serve\n",
    "```\n",
    "\n",
    "- You can provide your entrypoint script as `exec` form to provide instruction of how to perform the inference process, for example:\n",
    "```\n",
    "ENTRYPOINT [\"python\", \"inference.py\"]\n",
    "```\n",
    "\n",
    "- When deploying ML models, one option is to archive and compress the model artifacts into a `tar.gz` format and provided the s3 path of the model artifacts as the `ModelDataUrl` in the [`CreateModel`](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateModel.html) API request. SageMaker AI will copy the model artifacts from the S3 location \n",
    " and decompresses this tar file into `/opt/ml/model` directory before your container starts for use by your inference code. However, for deploying large models, SageMaker AI allows you to [deploy uncompressed models](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-uncompressed.html). In this example, we will show you how to use the uncompressed DeepSeek R1 Distilled Llama 70B model.\n",
    "\n",
    "- To receive inference requests, the container must have a web server listening on port `8080` and must accept `POST` requests to the `/invocations` and `/ping` endpoints.\n",
    "\n",
    "The below diagram shows on a high-level, how you should prepare your own container image to be compatible for SageMaker AI hosting. \n",
    "\n",
    "![inference](./img/sagemaker-real-time-inference.png)\n",
    "\n",
    "If you already have a docker image, you can see more instructions for [adapting your own inference container for SageMaker AI](https://docs.aws.amazon.com/sagemaker/latest/dg/adapt-inference-container.html). Also it is important to note that, SageMaker AI provided containers automatically implements a web server for serving requests that responds to `/invocations` and `/ping` (for healthcheck) requests. You can find more about the [prebuilt SageMaker AI docker images for deep learning in our SageMaker doc](https://docs.aws.amazon.com/sagemaker/latest/dg/pre-built-containers-frameworks-deep-learning.html).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61f2821-1dd8-4be6-a9b6-762932165225",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install sagemaker boto3 huggingface_hub --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03e5664-d4e8-404b-91d9-2c9f3845d4a1",
   "metadata": {},
   "source": [
    "#### Create the entrypoint serve file \n",
    "The `serve` file will used as the `exec` form to be executed at the container starting time. The main command to start sglang in the SageMaker docker image is \n",
    "```\n",
    "python3 -m sglang.launch_server --model-path <your model path> --host 0.0.0.0 --port 8080\n",
    "```\n",
    "Here the `model-path` can be set as `/opt/ml/model` as this is where SageMaker AI will copy the model artifacts from s3 to the endpoint and use `port` **8080** as required by SageMaker hosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ea56ce-7062-4b5e-8194-3ecb67079ed7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile serve\n",
    "#!/bin/bash\n",
    "\n",
    "echo \"Starting server\"\n",
    "\n",
    "SERVER_ARGS=\"--host 0.0.0.0 --port 8080\"\n",
    "\n",
    "if [ -n \"$TENSOR_PARALLEL_DEGREE\" ]; then\n",
    "    SERVER_ARGS=\"${SERVER_ARGS} --tp-size ${TENSOR_PARALLEL_DEGREE}\"\n",
    "fi\n",
    "\n",
    "if [ -n \"$DATA_PARALLEL_DEGREE\" ]; then\n",
    "    SERVER_ARGS=\"${SERVER_ARGS} --dp-size ${DATA_PARALLEL_DEGREE}\"\n",
    "fi\n",
    "\n",
    "if [ -n \"$EXPERT_PARALLEL_DEGREE\" ]; then\n",
    "    SERVER_ARGS=\"${SERVER_ARGS} --ep-size ${EXPERT_PARALLEL_DEGREE}\"\n",
    "fi\n",
    "\n",
    "if [ -n \"$MEM_FRACTION_STATIC\" ]; then\n",
    "    SERVER_ARGS=\"${SERVER_ARGS} --mem-fraction-static ${MEM_FRACTION_STATIC}\"\n",
    "fi\n",
    "\n",
    "if [ -n \"$QUANTIZATION\" ]; then\n",
    "    SERVER_ARGS=\"${SERVER_ARGS} --quantization ${QUANTIZATION}\"\n",
    "fi\n",
    "\n",
    "if [ -n \"$CHUNKED_PREFILL_SIZE\" ]; then\n",
    "    SERVER_ARGS=\"${SERVER_ARGS} --chunked-prefill-size ${CHUNKED_PREFILL_SIZE}\"\n",
    "fi\n",
    "\n",
    "if [ -n \"$MODEL_ID\" ]; then\n",
    "    SERVER_ARGS=\"${SERVER_ARGS} --model-path ${MODEL_ID}\"\n",
    "else\n",
    "    SERVER_ARGS=\"${SERVER_ARGS} --model-path /opt/ml/model\"\n",
    "fi\n",
    "\n",
    "if [ -n \"$TORCH_COMPILE\" ]; then\n",
    "    SERVER_ARGS=\"${SERVER_ARGS} --enable-torch-compile\"\n",
    "fi\n",
    "\n",
    "if [ -n \"$TORCHAO_CONFIG\" ]; then\n",
    "    SERVER_ARGS=\"${SERVER_ARGS} --torchao-config ${TORCHAO_CONFIG}\"\n",
    "fi\n",
    "\n",
    "if [ -n \"$KV_CACHE_DTYPE\" ]; then\n",
    "    SERVER_ARGS=\"{$SERVER_ARGS} --kv-cache-dtype ${KV_CACHE_DTYPE}\"\n",
    "fi\n",
    "\n",
    "python3 -m sglang.launch_server $SERVER_ARGS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d076da43-2458-403c-9dde-0c27107f4f0b",
   "metadata": {},
   "source": [
    "SGLang has provided the based [Dockerfile here](https://github.com/sgl-project/sglang/blob/main/docker/Dockerfile). You can directly extend the base image with\n",
    "\n",
    "```\n",
    "# Extend from the base sglang image\n",
    "FROM lmsysorg/sglang:v0.5.3rc0-cu126\n",
    "```\n",
    "\n",
    "In this example, we have copied the whole base Dockerfile with the specific tag and added the below lines to make it compatible with SageMaker\n",
    "```\n",
    "COPY serve /usr/bin/serve\n",
    "RUN chmod 777 /usr/bin/serve\n",
    "\n",
    "ENTRYPOINT [ \"/usr/bin/serve\" ]\n",
    "```\n",
    "You can add additional layers in the container image to accomodate your specific use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2ad6dc-ae4e-4ac6-8c55-42359a04dfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Dockerfile\n",
    "FROM lmsysorg/sglang:v0.5.3rc0-cu126\n",
    "\n",
    "COPY serve /usr/bin/serve\n",
    "RUN chmod 777 /usr/bin/serve\n",
    "\n",
    "ENTRYPOINT [ \"/usr/bin/serve\" ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93158bf1-2cb4-4de3-8f82-9a351eb7dc8d",
   "metadata": {},
   "source": [
    "Next, we will need to create an ECR repository for the custom docker image and build the image locally and push to the ECR repository. Note that you need to make sure the IAM role you used here has permission to push to ECR. \n",
    "\n",
    "The below cell might take sometime, please be patient. If you have already built the docker image from other development environment, please feel free to skip the below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc3001d-88c2-4610-847e-b018dffb2074",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)\n",
    "REGION=$(aws configure get region)\n",
    "REPOSITORY_NAME=sglang-sagemaker\n",
    "\n",
    "# Create ECR repository if needed\n",
    "if aws ecr describe-repositories --repository-names \"${REPOSITORY_NAME}\" &>/dev/null; then\n",
    "    echo \"Repository ${REPOSITORY_NAME} already exists\"\n",
    "else\n",
    "    echo \"Creating ECR repository ${REPOSITORY_NAME}...\"\n",
    "    aws ecr create-repository \\\n",
    "        --repository-name \"${REPOSITORY_NAME}\" \\\n",
    "        --region \"${REGION}\"\n",
    "fi\n",
    "\n",
    "#build docker image and push to ECR repository\n",
    "docker build -t sglang .\n",
    "aws ecr get-login-password --region $REGION | docker login --username AWS --password-stdin $ACCOUNT_ID.dkr.ecr.$REGION.amazonaws.com\n",
    "docker tag sglang:latest $ACCOUNT_ID.dkr.ecr.$REGION.amazonaws.com/$REPOSITORY_NAME:latest\n",
    "docker push $ACCOUNT_ID.dkr.ecr.$REGION.amazonaws.com/$REPOSITORY_NAME:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7620d7-05d4-49be-9f4c-9dab1e25f17d",
   "metadata": {},
   "source": [
    "## Create SageMaker AI endpoint for Qwen QWQ 32B model\n",
    "\n",
    "### Introduction: [Qwen QwQ 32B](https://huggingface.co/Qwen/QwQ-32B)\n",
    "\n",
    "[QwQ](https://huggingface.co/Qwen/QwQ-32B) is the reasoning model of the Qwen series. Compared with conventional instruction-tuned models, QwQ, which is capable of thinking and reasoning, can achieve significantly enhanced performance in downstream tasks, especially hard problems. QwQ-32B is the medium-sized reasoning model, which is capable of achieving competitive performance against state-of-the-art reasoning models, e.g., DeepSeek-R1, o1-mini.\n",
    "\n",
    "- **Mathematical Reasoning**: Achieves an impressive 90.6% on MATH-500, outperforming both Claude 3.5 (78.3%) and matching OpenAI's o1-mini (90.0%)\n",
    "- **Advanced Mathematics**: Scores 50.0% on AIME (American Invitational Mathematics Examination), significantly 'higher than Claude 3.5 (16.0%)\n",
    "- **Scientific Reasoning**: Demonstrates strong performance on GPQA with 65.2%, on par with Claude 3.5 (65.0%)\n",
    "- **Programming**: Achieves 50.0% on LiveCodeBench, showing competitive performance with leading proprietary models\n",
    "\n",
    "> [NOTE]\n",
    "> QwQ-32B is released under the Apache 2.0 license, making it suitable for both research and commercial applications.\n",
    "\n",
    "Let's get started deploying one of the most capable open-source reasoning models available today!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49095ae2-0f51-49e3-acf4-e554fff52cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sagemaker\n",
    "import boto3\n",
    "import sys\n",
    "import time\n",
    "from typing import List, Dict\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "boto_region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.session.Session(boto_session=boto3.Session(region_name=boto_region))\n",
    "role = sagemaker.get_execution_role()\n",
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "sagemaker_runtime_client = boto3.client(\"sagemaker-runtime\")\n",
    "s3_client = boto3.client(\"s3\")\n",
    "model_bucket = sagemaker_session.default_bucket()  # bucket to house artifacts\n",
    "s3_model_prefix = (\n",
    "    \"hf-large-models/model_qwen\"  # folder within bucket where code artifact will go\n",
    ")\n",
    "\n",
    "prefix = sagemaker.utils.unique_name_from_base(\"DEMO\")\n",
    "print(f\"prefix: {prefix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66560cdc-9dda-478a-bac2-88ed2e6feef3",
   "metadata": {},
   "source": [
    "## Setup your SageMaker Real-time Endpoint \n",
    "### Create a SageMaker endpoint configuration\n",
    "\n",
    "We begin by creating the endpoint configuration and set MinInstanceCount to 0. This allows the endpoint to scale in all the way down to zero instances when not in use.\n",
    "\n",
    "There are a few parameters we want to setup for our endpoint. We first start by setting the variant name, and instance type we want our endpoint to use. In addition we set the *model_data_download_timeout_in_seconds* and *container_startup_health_check_timeout_in_seconds* to have some guardrails for when we deploy inference components to our endpoint. In addition we will use Managed Instance Scaling which allows SageMaker to scale the number of instances based on the requirements of the scaling of your inference components. We set a *MinInstanceCount* and *MinInstanceCount* variable to size this according to the workload you want to service and also maintain controls around cost. Lastly, we set *RoutingStrategy* for the endpoint to optimally tune how to route requests to instances and inference components for the best performance.\n",
    "\n",
    "The suggested instance types to host the QwQ 32B model can be `ml.g5.12xlarge`, `ml.g6.12xlarge`, `ml.g6e.12xlarge`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6322e5d-23ad-4eb1-9d50-c88bd0ee4996",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set an unique endpoint config name\n",
    "endpoint_config_name = f\"{prefix}-endpoint-config\"\n",
    "print(f\"Demo endpoint config name: {endpoint_config_name}\")\n",
    "\n",
    "# Set varient name and instance type for hosting\n",
    "variant_name = \"AllTraffic\"\n",
    "instance_type = \"ml.g5.12xlarge\"\n",
    "model_data_download_timeout_in_seconds = 3600\n",
    "container_startup_health_check_timeout_in_seconds = 3600\n",
    "\n",
    "min_instance_count = 0 # Minimum instance must be set to 0\n",
    "max_instance_count = 3\n",
    "\n",
    "sagemaker_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": variant_name,\n",
    "            \"InstanceType\": instance_type,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelDataDownloadTimeoutInSeconds\": model_data_download_timeout_in_seconds,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": container_startup_health_check_timeout_in_seconds,\n",
    "            \"ManagedInstanceScaling\": {\n",
    "                \"Status\": \"ENABLED\",\n",
    "                \"MinInstanceCount\": min_instance_count,\n",
    "                \"MaxInstanceCount\": max_instance_count,\n",
    "            },\n",
    "            \"RoutingConfig\": {\"RoutingStrategy\": \"LEAST_OUTSTANDING_REQUESTS\"},\n",
    "        }\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe7448b-0e70-4bfa-9d39-31757c19de6f",
   "metadata": {},
   "source": [
    "You can simply call the [`deploy` function](https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/model.py#L149) from the SageMaker Model class to deploy the model to an endpoint and it will return a [`Predictor`](https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/base_predictor.py#L98) object to perform invocation against this endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0182ef-7f2b-4ded-b407-a8411bd391e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set a unique endpoint name\n",
    "endpoint_name = f\"{prefix}-endpoint\"\n",
    "print(f\"Demo endpoint name: {endpoint_name}\")\n",
    "\n",
    "sagemaker_client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91c3d36-138d-4aa1-b334-ae347bd6f13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session.wait_for_endpoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5470ba3-d8a6-4bac-94c5-0db09359b582",
   "metadata": {},
   "source": [
    "## Download the model from Hugging Face and upload the model artifacts on Amazon S3\n",
    "In this example, we will demonstrate how to download your copy of the model from huggingface and upload it to an s3 location in your AWS account, then deploy the model with the downloaded model artifacts to an endpoint. \n",
    "\n",
    "First, download the model artifact data from HuggingFace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94bdf5e-b897-425b-8195-aa37014d41fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sagemaker\n",
    "import jinja2\n",
    "\n",
    "qwen_qwq_32b = \"Qwen/QwQ-32B\"\n",
    "\n",
    "# - This will download the model into the current directory where ever the jupyter notebook is running\n",
    "local_model_path = Path(\".\")\n",
    "local_model_path.mkdir(exist_ok=True)\n",
    "model_name = qwen_qwq_32b\n",
    "# Only download pytorch checkpoint files\n",
    "allow_patterns = [\"*.json\", \"*.safetensors\", \"*.bin\", \"*.txt\"]\n",
    "\n",
    "# - Leverage the snapshot library to donload the model since the model is stored in repository using LFS\n",
    "model_download_path = snapshot_download(\n",
    "    repo_id=model_name,\n",
    "    cache_dir=local_model_path,\n",
    "    allow_patterns=allow_patterns,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc185f5-034c-4703-ae20-8d701ddbe06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a variable to contain the s3url of the location that has the model\n",
    "pretrained_model_location = f\"s3://{model_bucket}/{s3_model_prefix}/\"\n",
    "print(f\"Pretrained model will be uploaded to ---- > {pretrained_model_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83da5ec-3088-46e2-bcc9-e55ede3671c5",
   "metadata": {},
   "source": [
    "Upload model data to s3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89e65ea-8424-493c-9736-ce964849433c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_artifact = sagemaker_session.upload_data(path=model_download_path, key_prefix=s3_model_prefix)\n",
    "print(f\"Model uploaded to --- > {model_artifact}\")\n",
    "print(f\"We will set option.s3url={model_artifact}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aeb386d-f269-4208-8ec5-ef7c3242090e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional\n",
    "# !rm -rf {model_download_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85add481-6581-4bfb-90c7-f6094064b050",
   "metadata": {},
   "source": [
    "We use the image uri created with previous step when building the SGLang container image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57465864-dfe8-411f-a909-6a6139669e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri = f'{sagemaker_session.account_id()}.dkr.ecr.{boto_region}.amazonaws.com/sglang-sagemaker:latest'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1365af9c-fe1e-44d7-85fa-1910ac257f00",
   "metadata": {},
   "source": [
    "To find our more of the SageMaker `create_model` api call, you can see the details in [the boto3 doc](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker/client/create_model.html). Note that you can use the **CompressionType** to specify how the model data is prepared.  \n",
    "\n",
    "If you choose `Gzip` and choose `S3Object` as the value of `S3DataType`, `S3Uri` identifies an object that is a gzip-compressed TAR archive. SageMaker will attempt to decompress and untar the object during model deployment.\n",
    "\n",
    "If you choose `None` and `S3Prefix` as the value of `S3DataType`, then for each S3 object under the key name pefix referenced by `S3Uri`, SageMaker will trim its key by the prefix, and use the remainder as the path (relative to `/opt/ml/model`) of the file holding the content of the S3 object. SageMaker will split the remainder by slash (/), using intermediate parts as directory names and the last part as filename of the file holding the content of the S3 object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b7154e-ad8d-447e-9156-cdbe04ef2212",
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen_sglang_model = {\n",
    "    \"Image\": image_uri,\n",
    "    'ModelDataSource': {\n",
    "                'S3DataSource': {\n",
    "                    'S3Uri': pretrained_model_location,\n",
    "                    'S3DataType': 'S3Prefix',\n",
    "                    'CompressionType': 'None',\n",
    "                }\n",
    "            },\n",
    "    \"Environment\": {\n",
    "        \"SAGEMAKER_MODEL_SERVER_WORKERS\": \"1\",\n",
    "        'TENSOR_PARALLEL_DEGREE': '4',\n",
    "    },\n",
    "}\n",
    "model_name_qwen = f\"qwen-qwq-32b-sglang-{datetime.now().strftime('%y%m%d-%H%M%S')}\"\n",
    "# create SageMaker Model\n",
    "sagemaker_client.create_model(\n",
    "    ModelName=model_name_qwen,\n",
    "    ExecutionRoleArn=role,\n",
    "    Containers=[qwen_sglang_model],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31338b4b-7102-4921-b5ea-2534331c0283",
   "metadata": {},
   "source": [
    "We can now create the Inference Components which will deployed on the endpoint that you specify. Please note here that you can provide a SageMaker model or a container to specification. If you provide a container, you will need to provide an image and artifactURL as parameters. In this example we set it to the model name we prepared in the cells above. You can also set the `ComputeResourceRequirements` to supply SageMaker what should be reserved for each copy of the inference component. You can also set the copy count of the number of Inference Components you would like to deploy. These can be managed and scaled as the capabilities become available. \n",
    "\n",
    "Note that in this example we set the `NumberOfAcceleratorDevicesRequired` to a value of `4`. By doing so we reserve 4 accelerators for each copy of this inference component so that we can use tensor parallel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ff9a03-cb78-4b81-94f4-abb07f5297ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_component_name_qwen = f\"{prefix}-IC-qwen-32b-{datetime.now().strftime('%y%m%d-%H%M%S')}\"\n",
    "variant_name = \"AllTraffic\"\n",
    "\n",
    "sagemaker_client.create_inference_component(\n",
    "    InferenceComponentName=inference_component_name_qwen,\n",
    "    EndpointName=endpoint_name,\n",
    "    VariantName=variant_name,\n",
    "    Specification={\n",
    "        \"ModelName\": model_name_qwen,\n",
    "        \"ComputeResourceRequirements\": {\n",
    "            \"NumberOfAcceleratorDevicesRequired\": 4,\n",
    "            \"NumberOfCpuCoresRequired\": 1,\n",
    "            \"MinMemoryRequiredInMb\": 1024,\n",
    "        },\n",
    "    },\n",
    "    RuntimeConfig={\"CopyCount\": 1},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56190add-87e3-4081-ae69-2d603911522e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    desc = sagemaker_client.describe_inference_component(\n",
    "        InferenceComponentName=inference_component_name_qwen\n",
    "    )\n",
    "    status = desc[\"InferenceComponentStatus\"]\n",
    "    print(status)\n",
    "    sys.stdout.flush()\n",
    "    if status in [\"InService\", \"Failed\"]:\n",
    "        break\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865e285c-b98c-4e23-87dd-739898c32dec",
   "metadata": {},
   "source": [
    "#### Invoke endpoint with boto3\n",
    "Note that you can also invoke the endpoint with boto3. If you have an existing endpoint, you don't need to recreate the `predictor` and can follow below example to invoke the endpoint with an endpoint name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6817557b-8204-4171-bcaa-54395f8b8868",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "sagemaker_runtime = boto3.client('sagemaker-runtime')\n",
    "# endpoint_name = predictor.endpoint_name # you can manually set the endpoint name with an existing endpoint\n",
    "\n",
    "prompt = {\n",
    "    'model':'mymodel',\n",
    "    'messages':[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that thinks and reasons before answering.\"},\n",
    "    {\"role\": \"user\", \"content\": \"How many R are in STRAWBERRY? Keep your answer and explanation short!\"}\n",
    "],\n",
    "    'temperature':0,\n",
    "    'max_tokens':512,\n",
    "}\n",
    "response = sagemaker_runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    InferenceComponentName=inference_component_name_qwen,\n",
    "    ContentType=\"application/json\",\n",
    "    Body=json.dumps(prompt)\n",
    ")\n",
    "response_dict = json.loads(response['Body'].read().decode(\"utf-8\"))\n",
    "response_content = response_dict['choices'][0]['message']['content']\n",
    "print(response_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015339db-9d8c-481b-bca9-5f7955991a5b",
   "metadata": {},
   "source": [
    "#### Streaming response from the endpoint\n",
    "Additionally, SGLang allows you to invoke the endpoint and receive streaming response. Below is an example of how to interact with the endpoint with streaming response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65f8ce7-fcf1-4276-a1ae-98dc3ef7638b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "\n",
    "# Example class that processes an inference stream:\n",
    "class SmrInferenceStream:\n",
    "    \n",
    "    def __init__(self, sagemaker_runtime, endpoint_name, inference_component_name=None):\n",
    "        self.sagemaker_runtime = sagemaker_runtime\n",
    "        self.endpoint_name = endpoint_name\n",
    "        self.inference_component_name = inference_component_name\n",
    "        # A buffered I/O stream to combine the payload parts:\n",
    "        self.buff = io.BytesIO() \n",
    "        self.read_pos = 0\n",
    "        \n",
    "    def stream_inference(self, request_body):\n",
    "        # Gets a streaming inference response \n",
    "        # from the specified model endpoint:\n",
    "        response = self.sagemaker_runtime\\\n",
    "            .invoke_endpoint_with_response_stream(\n",
    "                EndpointName=self.endpoint_name, \n",
    "                InferenceComponentName=self.inference_component_name,\n",
    "                Body=json.dumps(request_body), \n",
    "                ContentType=\"application/json\"\n",
    "        )\n",
    "        # Gets the EventStream object returned by the SDK:\n",
    "        event_stream = response['Body']\n",
    "        for event in event_stream:\n",
    "            # Passes the contents of each payload part\n",
    "            # to be concatenated:\n",
    "            self._write(event['PayloadPart']['Bytes'])\n",
    "            # Iterates over lines to parse whole JSON objects:\n",
    "            for line in self._readlines():\n",
    "                line = line.decode('utf-8')[len('data: '):]\n",
    "                # print(line)\n",
    "                try:\n",
    "                    resp = json.loads(line)\n",
    "                except:\n",
    "                    continue\n",
    "                if len(line)>0 and type(resp) == dict:\n",
    "                    # if len(resp.get('choices')) == 0:\n",
    "                    #     continue\n",
    "                    part = resp.get('choices')[0]['delta']['content']\n",
    "                    \n",
    "                else:\n",
    "                    part = resp\n",
    "                # Returns parts incrementally:\n",
    "                yield part\n",
    "    \n",
    "    # Writes to the buffer to concatenate the contents of the parts:\n",
    "    def _write(self, content):\n",
    "        self.buff.seek(0, io.SEEK_END)\n",
    "        self.buff.write(content)\n",
    "\n",
    "    # The JSON objects in buffer end with '\\n'.\n",
    "    # This method reads lines to yield a series of JSON objects:\n",
    "    def _readlines(self):\n",
    "        self.buff.seek(self.read_pos)\n",
    "        for line in self.buff.readlines():\n",
    "            self.read_pos += len(line)\n",
    "            yield line[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f29fdb-e4be-439c-a6e4-dc05facc6b92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "request_body = {\n",
    "    'model':'mymodel',\n",
    "    'messages':[\n",
    "        {\"role\": \"user\", \"content\": \"List 3 countries and their capitals. Give a short answer\"},\n",
    "    ],\n",
    "    'temperature':0,\n",
    "    'max_tokens':512,\n",
    "    'stream': True,\n",
    "    'stream_options': {'include_usage': True}\n",
    "}\n",
    "\n",
    "smr_inference_stream = SmrInferenceStream(\n",
    "    sagemaker_runtime, endpoint_name, inference_component_name_qwen)\n",
    "stream = smr_inference_stream.stream_inference(request_body)\n",
    "for part in stream:\n",
    "    print(part, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be0a9b3-3fa0-4f9d-8ff4-8a24c69e568e",
   "metadata": {},
   "source": [
    "## Automatically Scale To Zero\n",
    "### Scaling policies\n",
    "Once the endpoint is deployed and InService, you can then add the necessary scaling policies:\n",
    "\n",
    "* A [target tracking](https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-target-tracking.html) policy that can scale in the copy count for our inference component model copies to zero, and from 1 to n. \n",
    "* A [step scaling policy](https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-step-scaling-policies.html) policy that will allow the endpoint to scale out from zero.\n",
    "\n",
    "These policies work together to provide cost-effective scaling - the endpoint can scale to zero when idle and automatically scale out as needed to handle incoming requests.\n",
    "\n",
    "### Scaling policy for inference components copies (target tracking)\n",
    "We start with creating our target tracking policies for scaling the CopyCount of our inference component\n",
    "\n",
    "#### Register a new autoscaling target\n",
    "After you create your SageMaker endpoint and inference components, you register a new auto scaling target for Application Auto Scaling. In the following code block, you set **MinCapacity**  to **0**, which is required for your endpoint to scale down to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d39cb6e-abd6-41fe-bc41-18e0e217973a",
   "metadata": {},
   "outputs": [],
   "source": [
    "aas_client = sagemaker_session.boto_session.client(\"application-autoscaling\")\n",
    "cloudwatch_client = sagemaker_session.boto_session.client(\"cloudwatch\")\n",
    "\n",
    "# Autoscaling parameters\n",
    "resource_id = f\"inference-component/{inference_component_name_qwen}\"\n",
    "service_namespace = \"sagemaker\"\n",
    "scalable_dimension = \"sagemaker:inference-component:DesiredCopyCount\"\n",
    "\n",
    "min_copy_count = 0\n",
    "max_copy_count = 3\n",
    "\n",
    "aas_client.register_scalable_target(\n",
    "    ServiceNamespace=service_namespace,\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=scalable_dimension,\n",
    "    MinCapacity=min_copy_count,\n",
    "    MaxCapacity=max_copy_count,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b9b600-10b2-4d11-b840-d7c1f329bba8",
   "metadata": {},
   "source": [
    "#### Configure Target Tracking Scaling Policy\n",
    "Once you have registered your new scalable target, the next step is to define your target tracking policy. In the code example that follows, we set the TargetValue to 5. This setting instructs the auto-scaling system to increase capacity when the number of concurrent requests per model reaches or exceeds 5. Here we are taking advantage of the more granular auto scaling metric `PredefinedMetricType`: `SageMakerInferenceComponentConcurrentRequestsPerCopyHighResolution` to more accurately monitor and react to changes in inference traffic. Take a look this [blog](https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-inference-launches-faster-auto-scaling-for-generative-ai-models/) for more information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798da955-5835-469f-81b0-5ff6e593f34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "aas_client.describe_scalable_targets(\n",
    "    ServiceNamespace=service_namespace,\n",
    "    ResourceIds=[resource_id],\n",
    "    ScalableDimension=scalable_dimension,\n",
    ")\n",
    "\n",
    "# The policy name for the target traking policy\n",
    "target_tracking_policy_name = f\"Target-tracking-policy-qwen-qwq-scale-to-zero-aas-{inference_component_name_qwen}\"\n",
    "\n",
    "aas_client.put_scaling_policy(\n",
    "    PolicyName=target_tracking_policy_name,\n",
    "    PolicyType=\"TargetTrackingScaling\",\n",
    "    ServiceNamespace=service_namespace,\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=scalable_dimension,\n",
    "    TargetTrackingScalingPolicyConfiguration={\n",
    "        \"PredefinedMetricSpecification\": {\n",
    "            \"PredefinedMetricType\": \"SageMakerInferenceComponentConcurrentRequestsPerCopyHighResolution\",\n",
    "        },\n",
    "        # Low TPS + load TPS\n",
    "        \"TargetValue\": 5,  # you need to adjust this value based on your use case\n",
    "        \"ScaleInCooldown\": 300,  # default\n",
    "        \"ScaleOutCooldown\": 300,  # default\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6f2206-1757-456f-9a46-46854a0051f4",
   "metadata": {},
   "source": [
    "Application Auto Scaling creates two CloudWatch alarms per scaling target. The first triggers scale-out actions after 30 seconds (using 3 sub-minute data point), while the second triggers scale-in after 15 minutes (using 90 sub-minute data points). The time to trigger the scaling action is usually 1–2 minutes longer than those minutes because it takes time for the endpoint to publish metrics to CloudWatch, and it also takes time for AutoScaling to react.\n",
    "\n",
    "### Scale out from zero policy (step scaling policy )\n",
    "To enable your endpoint to scale out from zero instances, do the following:\n",
    "\n",
    "#### Configure Step Scaling Policy\n",
    "Create a step scaling policy that defines when and how to scale out from zero. This policy will add 1 model copy when triggered, enabling SageMaker to provision the instances required to handle incoming requests after being idle.  The following shows you how to define a step scaling policy. Here we have configured to scale out from 0 to 1 model copy (\"ScalingAdjustment\": 1), depending on your use case you can adjust ScalingAdjustment as required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48821bca-7fbf-4400-b965-c863eea1fd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The policy name for the step scaling policy\n",
    "step_scaling_policy_name = f\"Step-scaling-policy-qwen-qwq-scale-to-zero-aas-{inference_component_name_qwen}\"\n",
    "\n",
    "aas_client.put_scaling_policy(\n",
    "    PolicyName=step_scaling_policy_name,\n",
    "    PolicyType=\"StepScaling\",\n",
    "    ServiceNamespace=service_namespace,\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=scalable_dimension,\n",
    "    StepScalingPolicyConfiguration={\n",
    "        \"AdjustmentType\": \"ChangeInCapacity\",\n",
    "        \"MetricAggregationType\": \"Maximum\",\n",
    "        \"Cooldown\": 60,\n",
    "        \"StepAdjustments\":\n",
    "          [\n",
    "             {\n",
    "               \"MetricIntervalLowerBound\": 0,\n",
    "               \"ScalingAdjustment\": 1\n",
    "             }\n",
    "          ]\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4174f5-741f-43d1-be11-53e385e165d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = aas_client.describe_scaling_policies(\n",
    "    PolicyNames=[step_scaling_policy_name],\n",
    "    ServiceNamespace=service_namespace,\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=scalable_dimension,\n",
    ")\n",
    "step_scaling_policy_arn = resp['ScalingPolicies'][0]['PolicyARN']\n",
    "print(f\"step_scaling_policy_arn: {step_scaling_policy_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0430ca-dfb8-4341-a1bb-649f51ae59f1",
   "metadata": {},
   "source": [
    "#### Create the CloudWatch alarm that will trigger our policy\n",
    "\n",
    "Finally, create a CloudWatch alarm with the metric **NoCapacityInvocationFailures**. When triggered, the alarm initiates the previously defined scaling policy. For more information about the NoCapacityInvocationFailures metric, see [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.html#cloudwatch-metrics-inference-component).\n",
    "\n",
    "We have also set the following:\n",
    "- EvaluationPeriods to 1 \n",
    "- DatapointsToAlarm to 1 \n",
    "- ComparisonOperator to  GreaterThanOrEqualToThreshold\n",
    "\n",
    "This results in 1 min waiting for the step scaling policy to trigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff58244-9253-4bba-87bb-e62bc8ed92d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The alarm name for the step scaling alarm\n",
    "step_scaling_alarm_name = f\"step-scaling-alarm-qwen-qwq-scale-to-zero-aas-{inference_component_name_qwen}\"\n",
    "\n",
    "cloudwatch_client.put_metric_alarm(\n",
    "    AlarmName=step_scaling_alarm_name,\n",
    "    AlarmActions=[step_scaling_policy_arn],  # Replace with your actual ARN\n",
    "    MetricName='NoCapacityInvocationFailures',\n",
    "    Namespace='AWS/SageMaker',\n",
    "    Statistic='Maximum',\n",
    "    Dimensions=[\n",
    "        {\n",
    "            'Name': 'InferenceComponentName',\n",
    "            'Value': inference_component_name_qwen  # Replace with actual InferenceComponentName\n",
    "        }\n",
    "    ],\n",
    "    Period=30, # Set a lower period \n",
    "    EvaluationPeriods=1,\n",
    "    DatapointsToAlarm=1,\n",
    "    Threshold=1,\n",
    "    ComparisonOperator='GreaterThanOrEqualToThreshold',\n",
    "    TreatMissingData='missing'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d315f622-0bb3-404d-9879-a3acd15c1f08",
   "metadata": {},
   "source": [
    "From cloudwatch console, you can check the alarms created.\n",
    "\n",
    "![cloudwatch alarms](./img/cloudwatch-alarms.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce6d79a-d591-473d-980d-3efe6f598368",
   "metadata": {},
   "source": [
    "### Testing the behaviour\n",
    "Notice the `MinInstanceCount: 0` setting in the Endpoint configuration, which allows the endpoint to scale down to zero instances. With the scaling policy, CloudWatch alarm, and minimum instances set to zero, your SageMaker Inference Endpoint will now be able to automatically scale down to zero instances when not in use, helping you optimize your costs and resource utilization.\n",
    "\n",
    "### IC copy count scales in to zero\n",
    "We'll pause for a few minutes without making any invocations to our model. Based on our target tracking policy, when our SageMaker endpoint doesn't receive requests for about 10 to 15 minutes, it will automatically scale down to zero the number of model copies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffa57e7-2b71-414b-b0b2-5b354a83744c",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(600)\n",
    "start_time = time.time()\n",
    "while True:\n",
    "    desc = sagemaker_client.describe_inference_component(InferenceComponentName=inference_component_name_qwen)\n",
    "    status = desc[\"InferenceComponentStatus\"]\n",
    "    print(status)\n",
    "    sys.stdout.flush()\n",
    "    if status in [\"InService\", \"Failed\"]:\n",
    "        break\n",
    "    time.sleep(30)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nTotal time taken: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")\n",
    "\n",
    "desc = sagemaker_client.describe_inference_component(InferenceComponentName=inference_component_name_qwen)\n",
    "print(desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8f9dc3-165d-467f-b37f-494617793c16",
   "metadata": {},
   "source": [
    "### Endpoint's instances scale in to zero\n",
    "\n",
    "After 10 additional minutes of inactivity, SageMaker automatically terminates all underlying instances of the endpoint, eliminating all associated costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101fbcdb-4426-4677-8c3f-718dbb4150ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after 10mins instances will scale down to 0\n",
    "time.sleep(600)\n",
    "# verify whether CurrentInstanceCount is zero\n",
    "sagemaker_session.wait_for_endpoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3d4648-0237-4fb6-8236-c3ab4b61f21a",
   "metadata": {},
   "source": [
    "### Invoke the endpoint with a sample prompt\n",
    "\n",
    "If we try to invoke our endpoint while instances are scaled down to zero, we get a validation error: `An error occurred (ValidationError) when calling the InvokeEndpoint operation: Inference Component has no capacity to process this request. ApplicationAutoScaling may be in-progress (if configured) or try to increase the capacity by invoking UpdateInferenceComponentRuntimeConfig API.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805d2d8f-c413-49d2-befe-6c79d4ff9766",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sagemaker_runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    InferenceComponentName=inference_component_name_qwen,\n",
    "    ContentType=\"application/json\",\n",
    "    Body=json.dumps(prompt)\n",
    ")\n",
    "response_dict = json.loads(response['Body'].read().decode(\"utf-8\"))\n",
    "response_content = response_dict['choices'][0]['message']['content']\n",
    "print(response_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bbcdb8-0d97-42c0-8ea8-62311040da07",
   "metadata": {},
   "source": [
    "### Scale out from zero kicks in\n",
    "However, after 1 minutes our step scaling policy should kick in. SageMaker will then start provisioning a new instance and deploy our inference component model copy to handle requests. This demonstrates the endpoint's ability to automatically scale out from zero when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142e8698-1616-48c1-8855-4f138501fe16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "time.sleep(60)\n",
    "start_time = time.time()\n",
    "while True:\n",
    "    desc = sagemaker_client.describe_inference_component(InferenceComponentName=inference_component_name_qwen)\n",
    "    status = desc[\"InferenceComponentStatus\"]\n",
    "    print(status)\n",
    "    sys.stdout.flush()\n",
    "    if status in [\"InService\", \"Failed\"]:\n",
    "        break\n",
    "    time.sleep(30)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nTotal time taken: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")\n",
    "\n",
    "desc = sagemaker_client.describe_inference_component(InferenceComponentName=inference_component_name_qwen)\n",
    "print(desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a00dcbd-4424-481e-a2fd-d8990a3328b5",
   "metadata": {},
   "source": [
    "#### verify that our endpoint has succesfully scaled out from zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6d1a0e-b268-43b6-a8a2-8945fd5dd2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sagemaker_runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    InferenceComponentName=inference_component_name_qwen,\n",
    "    ContentType=\"application/json\",\n",
    "    Body=json.dumps(prompt)\n",
    ")\n",
    "response_dict = json.loads(response['Body'].read().decode(\"utf-8\"))\n",
    "response_content = response_dict['choices'][0]['message']['content']\n",
    "print(response_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bacb334-1383-4bab-a118-683d3328b929",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "Make sure to delete the endpoint and other artifacts that were created to avoid unnecessary cost. You can also go to SageMaker AI console to delete all the resources created in this example\n",
    "\n",
    "- Deregister scalable target\n",
    "- Delete cloudwatch alarms\n",
    "- Delete scaling policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b17f87-e0ca-4eeb-aff4-625359eb2ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Deregister the scalable target for AAS\n",
    "    aas_client.deregister_scalable_target(\n",
    "        ServiceNamespace=\"sagemaker\",\n",
    "        ResourceId=resource_id,\n",
    "        ScalableDimension=scalable_dimension,\n",
    "    )\n",
    "    print(f\"Scalable target for [b]{resource_id}[/b] deregistered. ✅\")\n",
    "except aas_client.exceptions.ObjectNotFoundException:\n",
    "    print(f\"Scalable target for [b]{resource_id}[/b] not found!.\")\n",
    "\n",
    "print(\"---\" * 10)\n",
    "\n",
    "# Delete CloudWatch alarms created for Step scaling policy\n",
    "try:\n",
    "    cloudwatch_client.delete_alarms(AlarmNames=[step_scaling_alarm_name])\n",
    "    print(f\"Deleted CloudWatch step scaling scale-out alarm [b]{step_scaling_alarm_name} ✅\")\n",
    "except cloudwatch_client.exceptions.ResourceNotFoundException:\n",
    "    print(f\"CloudWatch scale-out alarm [b]{step_scaling_alarm_name}[/b] not found.\")\n",
    "\n",
    "\n",
    "# Delete step scaling policies\n",
    "print(\"---\" * 10)\n",
    "\n",
    "try:\n",
    "    aas_client.delete_scaling_policy(\n",
    "        PolicyName=step_scaling_policy_name,\n",
    "        ServiceNamespace=\"sagemaker\",\n",
    "        ResourceId=resource_id,\n",
    "        ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n",
    "    )\n",
    "    print(f\"Deleted scaling policy [i green]{step_scaling_policy_name} ✅\")\n",
    "except aas_client.exceptions.ObjectNotFoundException:\n",
    "    print(f\"Scaling policy [i]{step_scaling_policy_name}[/i] not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696e16ca-ebb6-4d2a-84d0-69743e74793f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker_client.delete_inference_component(InferenceComponentName=inference_component_name_qwen)\n",
    "sagemaker_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "sagemaker_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30887849-feda-4f85-baf2-0705840bc81a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
