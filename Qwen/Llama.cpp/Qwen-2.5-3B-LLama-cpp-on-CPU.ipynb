{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aded5ace-7694-47d0-a8d9-ec0ea6a3bd54",
   "metadata": {},
   "source": [
    "# Run Small and Large Language Models (SLM, LLM) on AWS Graviton CPU Instances with Amazon SageMaker\n",
    "\n",
    "> This notebook has been tested on the Python 3 kernel of a SageMaker Jupternotebook instance on a ml.m5.xlarge instance with 50GB of disk size\n",
    "\n",
    "\n",
    "Small language models (SLMs) are can be a compelling choice for applications requiring lower latency, reduced compute requirements, and cost-effectiveness. This workshop will focus on deploying and scaling both SLMs and LLMs on AWS Graviton or x86 CPU-based ML instances. Attendees will gain hands-on experience in optimizing model performance, understanding the trade-offs between model size and computational efficiency, and implementing scalable inference using Amazon SageMaker.\n",
    "\n",
    "\n",
    "Amazon SageMaker AI provides the ability to build Docker containers to run on SageMaker endpoints, where they listen for health checks on /ping and receive real-time inference requests on /invocations. Using SageMaker AI for inference offers several benefits:\n",
    "\n",
    "- **Scalability**: SageMaker AI can automatically scale your inference endpoints up and down based on demand, ensuring your models can handle varying workloads.\n",
    "- **High Availability**: SageMaker AI manages the infrastructure and maintains the availability of your inference endpoints, so you don't have to worry about managing the underlying resources.\n",
    "- **Monitoring and Logging**: SageMaker AI provides built-in monitoring and logging capabilities, making it easier to track the performance and health of your inference endpoints.\n",
    "- **Security**: SageMaker AI integrates with other AWS services, such as AWS Identity and Access Management (IAM), to provide robust security controls for your inference workloads.\n",
    "\n",
    "Note that SageMaker provides [pre-built SageMaker AI Docker images](https://docs.aws.amazon.com/sagemaker/latest/dg/pre-built-containers-frameworks-deep-learning.html) that can help you quickly start with the model inference on SageMaker. It also allows you to [bring your own Docker container](https://docs.aws.amazon.com/sagemaker/latest/dg/adapt-inference-container.html) and use it inside SageMaker AI for training and inference. To be compatible with SageMaker AI, your container must have the following characteristics:\n",
    "\n",
    "- Your container must have a web server listening on port 8080.\n",
    "- Your container must accept POST requests to the /invocations and /ping real-time endpoints.\n",
    "\n",
    "In this notebook, we'll demonstrate how to adapt the [**Llama.cpp**](https://github.com/ggml-org/llama.cpp) framework to run on SageMaker AI endpoints. Llama.cpp is an open-source C++ inference engine developed by Georgi Gerganov and community, that enables efficient CPU-based inference for a large set of language model architectures, including Llama, Mistral, Qwen, and Falcon..\n",
    "\n",
    "By using Llama.cpp and building a custom Docker container, you can run Small and Large Language models like the [Qwen 2.5 3B Instruct\n",
    "](https://huggingface.co/Qwen/Qwen2.5-3B-Instruct-GGUF) on a SageMaker AI endpoint using CPU-based Graviton or x86 ML instances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a96ee70-182e-4c7c-affd-08b92b35c817",
   "metadata": {},
   "source": [
    "### Clone LLama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6c0715-99a6-46c6-8da5-caffa1fbdf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://github.com/ggml-org/llama.cpp.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599b2b8c",
   "metadata": {},
   "source": [
    "### Setup SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b448d8b",
   "metadata": {},
   "source": [
    "Install the necessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3806f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! pip install -U sagemaker boto3 \"huggingface_hub[cli]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51a67d4",
   "metadata": {},
   "source": [
    "Import the necessary packages and initialize the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb39900",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sagemaker\n",
    "import boto3\n",
    "from typing import List, Dict\n",
    "from datetime import datetime\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "session = sagemaker.session.Session(boto_session=boto3.Session(region_name=region))\n",
    "role = sagemaker.get_execution_role()\n",
    "client = boto3.client('sagemaker-runtime', region_name=region)\n",
    "sagemaker_client = boto3.client('sagemaker', region_name=region)\n",
    "bucket = session.default_bucket()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a1d7a83c-d175-4443-b9cf-26d8bad5fedb",
   "metadata": {},
   "source": [
    "### Prepare the Llama.cpp SageMaker container\n",
    "\n",
    "SageMaker AI makes extensive use of Docker containers for build and runtime tasks. Using containers, you can train machine learning algorithms and deploy models quickly and reliably at any scale. See [this link](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-code.html#your-algorithms-inference-code-run-image) to understand how SageMaker AI runs your inference image. \n",
    "\n",
    "- For model inference, SageMaker AI runs the container as:\n",
    "```\n",
    "docker run image serve\n",
    "```\n",
    "\n",
    "- You can provide your entrypoint script as `exec` form to provide instruction of how to perform the inference process, for example:\n",
    "```\n",
    "ENTRYPOINT [\"python\", \"inference.py\"]\n",
    "```\n",
    "\n",
    "- To receive inference requests, the container must have a web server listening on port `8080` and must accept `POST` requests to the `/invocations` and `/ping` endpoints.\n",
    "\n",
    "\n",
    "If you already have a docker image, you can see more instructions for [adapting your own inference container for SageMaker AI](https://docs.aws.amazon.com/sagemaker/latest/dg/adapt-inference-container.html). Also it is important to note that, SageMaker AI provided containers automatically implements a web server for serving requests that responds to `/invocations` and `/ping` (for healthcheck) requests. You can find more about the [prebuilt SageMaker AI docker images for deep learning in our SageMaker doc](https://docs.aws.amazon.com/sagemaker/latest/dg/pre-built-containers-frameworks-deep-learning.html).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d076da43-2458-403c-9dde-0c27107f4f0b",
   "metadata": {},
   "source": [
    "Llama.cpp has provided the based [Dockerfile here](https://github.com/ggml-org/llama.cpp/blob/master/docs/docker.md). You can directly extend the base image with\n",
    "\n",
    "In this example, we have copied the whole base Dockerfile and added the below lines to add the AWS CLI for mdeol download from S3 and \n",
    "You can add additional layers in the container image to accomodate your specific use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2ad6dc-ae4e-4ac6-8c55-42359a04dfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./llama.cpp/.devops/sagemaker.Dockerfile\n",
    "ARG UBUNTU_VERSION=22.04\n",
    "\n",
    "FROM ubuntu:$UBUNTU_VERSION AS build\n",
    "\n",
    "ARG TARGETARCH\n",
    "\n",
    "ARG GGML_CPU_ARM_ARCH=armv8-a\n",
    "\n",
    "RUN apt-get update && \\\n",
    "    apt-get install -y build-essential git cmake libcurl4-openssl-dev\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "COPY . .\n",
    "\n",
    "\n",
    "# Build llama.cpp for x86 or Graviton\n",
    "\n",
    "RUN ARCH=`uname -m` && \\\n",
    "    echo \"Building for architecture: $ARCH\" && \\\n",
    "    if [ \"$ARCH\" = \"x86_64\" ]; then \\\n",
    "        cmake -S . -B build -DCMAKE_BUILD_TYPE=Release -DLLAMA_CURL=ON -DGGML_NATIVE=OFF -DGGML_BACKEND_DL=ON -DGGML_CPU_ALL_VARIANTS=ON; \\\n",
    "    elif [ \"$ARCH\" = \"aarch64\" ]; then \\\n",
    "        cmake -S . -B build -DCMAKE_BUILD_TYPE=Release -DLLAMA_CURL=ON -DGGML_NATIVE=OFF -DGGML_CPU_ARM_ARCH=${GGML_CPU_ARM_ARCH}; \\\n",
    "    else \\\n",
    "        echo \"Unsupported architecture\"; \\\n",
    "        exit 1; \\\n",
    "    fi && \\\n",
    "    cmake --build build -j $(nproc)\n",
    "\n",
    "RUN mkdir -p /app/lib && \\\n",
    "    find build -name \"*.so\" -exec cp {} /app/lib \\;\n",
    "\n",
    "RUN mkdir -p /app/full \\\n",
    "    && cp build/bin/* /app/full \\\n",
    "    && cp *.py /app/full \\\n",
    "    && cp -r gguf-py /app/full \\\n",
    "    && cp -r requirements /app/full \\\n",
    "    && cp requirements.txt /app/full \\\n",
    "    && cp .devops/tools.sh /app/full/tools.sh\n",
    "\n",
    "## Base image\n",
    "FROM ubuntu:$UBUNTU_VERSION AS base\n",
    "\n",
    "RUN apt-get update \\\n",
    "    && apt-get install -y libgomp1 unzip curl\\\n",
    "    && apt autoremove -y \\\n",
    "    && apt clean -y \\\n",
    "    && rm -rf /tmp/* /var/tmp/* \\\n",
    "    && find /var/cache/apt/archives /var/lib/apt/lists -not -name lock -type f -delete \\\n",
    "    && find /var/cache -type f -delete\n",
    "\n",
    "COPY --from=build /app/lib/ /app\n",
    "\n",
    "\n",
    "### Server, Server only\n",
    "FROM base AS server\n",
    "\n",
    "ENV LLAMA_ARG_HOST=0.0.0.0\n",
    "ENV MODEL_S3_PATH=\"\"\n",
    "\n",
    "# Install AWS CLI and curl in a single RUN command to reduce layers for x86 or Graviton\n",
    "\n",
    "RUN ARCH=`uname -m` && \\\n",
    "    echo \"Installing AWS CLI for architecture: $ARCH\" && \\\n",
    "    if [ \"$ARCH\" = \"x86_64\" ]; then \\\n",
    "        curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"; \\\n",
    "    elif [ \"$ARCH\" = \"aarch64\" ]; then \\\n",
    "        curl \"https://awscli.amazonaws.com/awscli-exe-linux-aarch64.zip\" -o \"awscliv2.zip\"; \\\n",
    "    else \\\n",
    "        echo \"Unsupported architecture\"; \\\n",
    "        exit 1; \\\n",
    "    fi\n",
    "\n",
    "# Clean up unnecessary files after installation\n",
    "RUN unzip -qq awscliv2.zip && \\\n",
    "    ./aws/install && \\\n",
    "    rm -rf awscliv2.zip aws && \\\n",
    "    mkdir -p /models\n",
    "\n",
    "COPY --from=build /app/full/llama-server /app\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "# Expose port for the application to run on, has to be 8080\n",
    "EXPOSE 8080\n",
    "\n",
    "HEALTHCHECK CMD [ \"curl\", \"-f\", \"http://localhost:8080/health\" ]\n",
    "\n",
    "\n",
    "# Add serve argument to entrypoint to download model from S3 and start llama-server\n",
    "ENTRYPOINT [\"/bin/bash\", \"-c\", \"\\\n",
    "    echo \\\"Starting entrypoint with arg: $1\\\"; \\\n",
    "    if [ \\\"$1\\\" = \\\"serve\\\" ]; then \\\n",
    "        if [ ! -z \\\"$MODEL_S3_PATH\\\" ]; then \\\n",
    "            echo \\\"serve command detected and MODEL_S3_PATH is set to: $MODEL_S3_PATH\\\"; \\\n",
    "            MODEL_FILE=$(basename $MODEL_S3_PATH); \\\n",
    "            echo \\\"Downloading model file: $MODEL_FILE\\\"; \\\n",
    "            aws s3 cp $MODEL_S3_PATH /models/; \\\n",
    "            echo \\\"Starting llama-server with model: /models/$MODEL_FILE\\\"; \\\n",
    "            /app/llama-server -m /models/$MODEL_FILE; \\\n",
    "        else \\\n",
    "            echo \\\"MODEL_S3_PATH not set, starting llama-server without model\\\"; \\\n",
    "            /app/llama-server; \\\n",
    "        fi \\\n",
    "    else \\\n",
    "        echo \\\"'serve' command not provided\\\"; \\\n",
    "    fi\", \"—\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93158bf1-2cb4-4de3-8f82-9a351eb7dc8d",
   "metadata": {},
   "source": [
    "Next, we will need to create an ECR repository for the custom docker image and build the image locally and push to the ECR repository. Note that you need to make sure the IAM role you used here has permission to push to ECR. \n",
    "\n",
    "The below cell might take sometime, please be patient. If you have already built the docker image from other development environment, please feel free to skip the below cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4701f00",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "If you want to deploy the model on a Graviton CPU-based ML instance such as the c7g instance family, <b>you must build the docker image on an ARM64/AARCH64 CPU</b>, since Graviton is based on ARM64/AARCH64 architecture.\n",
    "\n",
    "If you only have access to an x86-based CPU, you can still build the image and use it to deploy the model on an x86 CPU ML instance on Amazon SageMaker like c7i instance family.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d858fdf",
   "metadata": {},
   "source": [
    "You need to adapt the llama.cpp source code to adapt for SageMaker's required paths */ping* and */invocations* on port *8080*\n",
    "\n",
    "To do that, you must follow the following steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4880ee37",
   "metadata": {},
   "source": [
    "1. Go to **./llama.cpp/tools/server/server.cpp**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5835e0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat ./llama.cpp/tools/server/server.cpp | grep 'svr->'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5ceac8",
   "metadata": {},
   "source": [
    "2. Find the API routes and add three new routes to the list\n",
    "\n",
    "```c++\n",
    "    svr->Post(\"/invocations\",         handle_chat_completions);\n",
    "    svr->Get (\"/ping\",                handle_health);\n",
    "    svr->Post(\"/ping\",                handle_health);\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc3001d-88c2-4610-847e-b018dffb2074",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)\n",
    "REGION=$(aws configure get region)\n",
    "REPOSITORY_NAME=llama.cpp-sagemaker\n",
    "\n",
    "# Create ECR repository if needed\n",
    "if aws ecr describe-repositories --repository-names \"${REPOSITORY_NAME}\" &>/dev/null; then\n",
    "    echo \"Repository ${REPOSITORY_NAME} already exists\"\n",
    "else\n",
    "    echo \"Creating ECR repository ${REPOSITORY_NAME}...\"\n",
    "    aws ecr create-repository \\\n",
    "        --repository-name \"${REPOSITORY_NAME}\" \\\n",
    "        --region \"${REGION}\"\n",
    "fi\n",
    "\n",
    "#build docker image and push to ECR repository\n",
    "cd ./llama.cpp\n",
    "sudo docker build -t llama.cpp-sagemaker --target server -f .devops/sagemaker.Dockerfile .\n",
    "aws ecr get-login-password --region $REGION | docker login --username AWS --password-stdin $ACCOUNT_ID.dkr.ecr.$REGION.amazonaws.com\n",
    "docker tag llama.cpp-sagemaker:latest $ACCOUNT_ID.dkr.ecr.$REGION.amazonaws.com/$REPOSITORY_NAME:latest\n",
    "docker push $ACCOUNT_ID.dkr.ecr.$REGION.amazonaws.com/$REPOSITORY_NAME:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbec90c",
   "metadata": {},
   "source": [
    "Now, you can download a model from Hugging Face. Llama.cpp expects models to be in GGUF format. You can either convert your favorite model from Safetensors to GGUF, or you can just download a model in GGUF format from Hugging Face.\n",
    "\n",
    "In this example, we will download the model a GGUF Qwen 2.5 3B model in 8-bit precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42df847",
   "metadata": {},
   "outputs": [],
   "source": [
    "! huggingface-cli download Qwen/Qwen2.5-3B-Instruct-GGUF qwen2.5-3b-instruct-q8_0.gguf --local-dir ./models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b3f30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws s3 cp ./models/qwen2.5-3b-instruct-q8_0.gguf s3://{bucket}/models/qwen2.5-3b-instruct-q8_0.gguf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85f2f8f",
   "metadata": {},
   "source": [
    "### Prepare and deploy the model on SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001effa6",
   "metadata": {},
   "source": [
    "Choose an appropriate image URI, model name and endpoint name for hosting your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b888f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_cpp_inference_image_uri = f'{session.account_id()}.dkr.ecr.{region}.amazonaws.com/llama.cpp-sagemaker:latest'\n",
    "# add datetime to names\n",
    "\n",
    "model_name = f\"qwen-2-5-3b-llama-cpp-{datetime.now().strftime('%m-%d-%Y-%Hh%Mm')}\"\n",
    "endpoint_name = f\"{model_name}-ep-{datetime.now().strftime('%m-%d-%Y-%Hh%Mm')}\"\n",
    "s3_model_path = f's3://{bucket}/models/qwen2.5-3b-instruct-q8_0.gguf'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7723f2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "In the SageMaker Llama.cpp container image, you can use the environment variables of Llama.cpp's llama-server to choose GGUF model from Hugging Face using <b><i>LLAMA_ARG_HF_FILE</i></b> and <b><i>LLAMA_ARG_HF_REPO</i></b>.\n",
    "<br>\n",
    "<br>\n",
    "    \n",
    "⚠️ However, for better download performance, we recommend using S3 to store and download the GGUF model for better download performance. Once you upload your model to your S3 bucket of choice, replace <b><i>MODEL_S3_PATH</i></b> environment variable value with the URI of your GGUF model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b66cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_cpp_model = sagemaker.Model(\n",
    "    image_uri=llama_cpp_inference_image_uri,\n",
    "    env={\n",
    "        \"LLAMA_ARG_PORT\": \"8080\",\n",
    "#        \"LLAMA_ARG_HF_FILE\": \"qwen2.5-3b-instruct-q8_0.gguf\",\n",
    "#        \"LLAMA_ARG_HF_REPO\": \"Qwen/Qwen2.5-3B-Instruct-GGUF\",\n",
    "##################\n",
    "#       OR       #\n",
    "##################\n",
    "        \"MODEL_S3_PATH\": s3_model_path,\n",
    "    },\n",
    "    role=role,\n",
    "    name=model_name,\n",
    "    sagemaker_session=sagemaker.Session()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ba5ff3-9c86-44ba-b1e3-3de3ff053b22",
   "metadata": {},
   "source": [
    "> Make sure you have enough quota for the ML instance types you want to use. In this section below, we will use memory-optimized ML instances like r7i or r8g which provide better memory bandwidth, which is essential for token generation, with respect to general purpose ML instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42de6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_llama_cpp_predictor = llama_cpp_model.deploy(\n",
    "    endpoint_name=endpoint_name,\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.r7i.2xlarge\",\n",
    "##################\n",
    "#       OR       #\n",
    "##################\n",
    "#    instance_type=\"ml.r8g.2xlarge\",\n",
    "    container_startup_health_check_timeout=1200,\n",
    "    wait=True\n",
    ")\n",
    "print(f\"Your Llama.cpp Model Endpoint: {endpoint_name} is being deployed! \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e44a0d3-176a-452a-8d95-a62270e4cbcd",
   "metadata": {},
   "source": [
    "> The endpoint startup time should be around 3 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9655e071",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"What is artificial intelligence?\"\n",
    "\n",
    "\n",
    "response = client.invoke_endpoint_with_response_stream(\n",
    "                EndpointName=endpoint_name,\n",
    "                ContentType=\"application/json\",\n",
    "                Body=json.dumps({\n",
    "                    \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                    \"max_tokens\": 4000,\n",
    "                    \"temperature\": 0.6,\n",
    "                    \"top_p\": 0.9,\n",
    "                    \"stream\": True,\n",
    "                    \"logprobs\": False,\n",
    "                    \"stream_options\":{\n",
    "                        \"include_usage\": False\n",
    "                    }\n",
    "                })\n",
    "            )\n",
    "\n",
    "full_response = \"\"\n",
    "for event in response['Body']:\n",
    "    try:\n",
    "        chunk = event['PayloadPart']['Bytes'].decode()\n",
    "        if 'finish_reason\":\"stop\"' in chunk:\n",
    "            break\n",
    "        chunk = chunk.replace(\"data: \", \"\")\n",
    "        chunk = json.loads(chunk)\n",
    "        \n",
    "        if 'choices' in chunk and len(chunk['choices']) > 0:\n",
    "            content = chunk['choices'][0].get('delta', {}).get('content', '')\n",
    "            print(content, end='')\n",
    "            full_response += content\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb9bb18",
   "metadata": {},
   "source": [
    "### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cfd364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete model and endpoint\n",
    "try:\n",
    "    print(f\"Deleting model: {model_name} ✅\")\n",
    "    sagemaker_client.delete_model(ModelName=model_name)\n",
    "except Exception as e:\n",
    "    print(f\"{e}\")\n",
    "\n",
    "try:\n",
    "    print(f\"Deleting endpoint: {endpoint_name} ✅\")\n",
    "    sagemaker_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "except Exception as e:\n",
    "    print(f\"{e}\")\n",
    "\n",
    "try:\n",
    "    print(f\"Deleting endpoint config: {endpoint_name} ✅\")\n",
    "    sagemaker_client.delete_endpoint_config(EndpointConfigName=endpoint_name)\n",
    "except Exception as e:\n",
    "    print(f\"{e}\")\n",
    "\n",
    "print(f\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37fd707-4fe7-4429-9232-8f5cd43d9c3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
