{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27ddeb2c-7877-46e3-9a4e-b2efb0d1b7a4",
   "metadata": {},
   "source": [
    "# ðŸš€ Deploy Qwen3 32B FP8 Model on Amazon SageMaker AI \n",
    "\n",
    "## Introduction: [Qwen3-32B-FP8](https://huggingface.co/Qwen/Qwen3-32B-FP8)\n",
    "\n",
    "`Qwen3-32B-FP8` is part of the [latest generation of Qwen language models](https://qwenlm.github.io/blog/qwen3/) with:\n",
    "\n",
    "- **Total Parameters**: 30.8B parameters\n",
    "- **Number of Paramaters (Non-Embedding)**: 31.2B parameters\n",
    "- **Architecture Details**:\n",
    "    - 64 layers\n",
    "    - 64 attention heads for queries and 8 for key/values (GQA)\n",
    "    - context length of 32,768 tokens (expandable to 131,072 with YaRN)\n",
    "\n",
    "### Note on FP8\n",
    "For convenience and performance, we have provided fp8-quantized model checkpoint for Qwen3, whose name ends with -FP8. The quantization method is fine-grained fp8 quantization with block size of 128. You can find more details in the quantization_config field in config.json\n",
    "\n",
    "### Key Features\n",
    "1. Hybrid Thinking Modes:\n",
    "\n",
    "- Thinking Mode: Enables step-by-step reasoning for complex problems\n",
    "Non-Thinking Mode: Provides quick responses for simpler queries\n",
    "Seamless switching between modes for optimal performance\n",
    "\n",
    "2. Strong Capabilities:\n",
    "\n",
    "- Advanced reasoning and problem-solving\n",
    "- Excellent instruction following\n",
    "- Enhanced agent capabilities for tool integration\n",
    "- Support for 119+ languages and dialects\n",
    "\n",
    "3. Model Architecture:\n",
    "\n",
    "- MoE architecture enabling efficient parameter usage\n",
    "- Only activates ~10% of parameters during inference\n",
    "- Optimized for both performance and computational efficiency\n",
    "  \n",
    "This model represents a significant advancement in open-source language models, offering competitive performance while maintaining efficient resource utilization through its MoE architecture. It's particularly well-suited for deployment in production environments where both performance and cost efficiency are crucial considerations.\n",
    "Let's get started deploying one of the most capable open-source reasoning models available today!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a645403-0c3e-4062-9d16-ef0b1041fbe3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -Uq sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5631d3-1c16-4ad5-a42c-85a28cf9dd3e",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65310881-31a9-453e-9f7b-c79876824cd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import sys\n",
    "import boto3\n",
    "import logging\n",
    "import time\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "print(sagemaker.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83937110-ffc0-4c42-b67d-0021b829f25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    boto_region = boto3.Session().region_name\n",
    "    sagemaker_session = sagemaker.session.Session(boto_session=boto3.Session(region_name=boto_region))\n",
    "    role = sagemaker.get_execution_role()\n",
    "    sagemaker_client = boto3.client(\"sagemaker\",region_name=boto_region)\n",
    "    prefix = sagemaker.utils.unique_name_from_base(\"DEMO\")\n",
    "    \n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6288f49a-0114-4d5b-8d7d-e8ba16dbddfe",
   "metadata": {},
   "source": [
    "## Setup your SageMaker Real-time Endpoint \n",
    "### Create a SageMaker endpoint configuration\n",
    "\n",
    "We begin by creating the endpoint configuration and set MinInstanceCount to 0. This allows the endpoint to scale in all the way down to zero instances when not in use. See the [notebook example for SageMaker AI endpoint scale down to zero](https://github.com/aws-samples/sagemaker-genai-hosting-examples/tree/02236395d44cf54c201eefec01fd8da0a454092d/scale-to-zero-endpoint).\n",
    "\n",
    "There are a few parameters we want to setup for our endpoint. We first start by setting the variant name, and instance type we want our endpoint to use. In addition we set the *model_data_download_timeout_in_seconds* and *container_startup_health_check_timeout_in_seconds* to have some guardrails for when we deploy inference components to our endpoint. In addition we will use Managed Instance Scaling which allows SageMaker to scale the number of instances based on the requirements of the scaling of your inference components. We set a *MinInstanceCount* and *MinInstanceCount* variable to size this according to the workload you want to service and also maintain controls around cost. Lastly, we set *RoutingStrategy* for the endpoint to optimally tune how to route requests to instances and inference components for the best performance.\n",
    "\n",
    "The suggested instance types to host the Qwen3-32B-FP8 model can be `ml.g5.24xlarge`, `ml.g6.12xlarge`, `ml.g6e.12xlarge`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c806a8-f261-49b5-ba45-bc012b246745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set an unique endpoint config name\n",
    "endpoint_config_name = f\"{prefix}-endpoint-config\"\n",
    "print(f\"Demo endpoint config name: {endpoint_config_name}\")\n",
    "\n",
    "# Set varient name and instance type for hosting\n",
    "variant_name = \"AllTraffic\"\n",
    "instance_type = \"ml.g6.12xlarge\"\n",
    "model_data_download_timeout_in_seconds = 3600\n",
    "container_startup_health_check_timeout_in_seconds = 3600\n",
    "\n",
    "min_instance_count = 0 # Minimum instance must be set to 0\n",
    "max_instance_count = 1\n",
    "\n",
    "sagemaker_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": variant_name,\n",
    "            \"InstanceType\": instance_type,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelDataDownloadTimeoutInSeconds\": model_data_download_timeout_in_seconds,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": container_startup_health_check_timeout_in_seconds,\n",
    "            \"ManagedInstanceScaling\": {\n",
    "                \"Status\": \"ENABLED\",\n",
    "                \"MinInstanceCount\": min_instance_count,\n",
    "                \"MaxInstanceCount\": max_instance_count,\n",
    "            },\n",
    "            \"RoutingConfig\": {\"RoutingStrategy\": \"LEAST_OUTSTANDING_REQUESTS\"},\n",
    "        }\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a95c7e-4677-4317-bf71-194563515876",
   "metadata": {},
   "source": [
    "### Create the SageMaker endpoint\n",
    "Next, we create our endpoint using the above endpoint config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3de66b-26df-4359-8165-c9338acbcc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a unique endpoint name\n",
    "endpoint_name = f\"{prefix}-endpoint\"\n",
    "print(f\"Demo endpoint name: {endpoint_name}\")\n",
    "\n",
    "sagemaker_client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b43b64-3f29-47c8-9c8f-8ea6a7638121",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session.wait_for_endpoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3efba05-7edd-4841-be76-abdad5db6c6d",
   "metadata": {},
   "source": [
    "## Download the model from Hugging Face and upload the model artifacts on Amazon S3\n",
    "If you are deploying a model hosted on the HuggingFace Hub, you must specify the `option.model_id=<hf_hub_model_id>` configuration. When using a model directly from the hub, we recommend you also specify the model revision (commit hash or branch) via `option.revision=<commit hash/branch>`. \n",
    "\n",
    "Since model artifacts are downloaded at runtime from the Hub, using a specific revision ensures you are using a model compatible with package versions in the runtime environment. Open Source model artifacts on the hub are subject to change at any time. These changes may cause issues when instantiating the model (updated model artifacts may require a newer version of a dependency than what is bundled in the container). If a model provides custom model (modeling.py) and/or custom tokenizer (tokenizer.py) files, you need to specify option.trust_remote_code=true to load and use the model.\n",
    "\n",
    "In this example, we will demonstrate how to download your copy of the model from huggingface and upload it to an s3 location in your AWS account, then deploy the model with the downloaded model artifacts to an endpoint.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426074b5-369e-4c33-9d1c-7cee3b9a458a",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_MODEL_ID = \"Qwen/Qwen3-32B-FP8\"\n",
    "\n",
    "base_name = HF_MODEL_ID.split('/')[-1].replace('.', '-').lower()\n",
    "model_lineage = HF_MODEL_ID.split(\"/\")[0]\n",
    "base_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4797fed2-e02e-429a-92c5-dcf8ae6cf624",
   "metadata": {},
   "source": [
    "**Best Practices**:\n",
    ">\n",
    "> **Store Models in Your Own S3 Bucket**\n",
    "For production use-cases, always download and store model files in your own S3 bucket to ensure validated artifacts. This provides verified provenance, improved access control, consistent availability, protection against upstream changes, and compliance with organizational security protocols.\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d204f8e2-30b0-47e7-b3e3-c7e2d951fe66",
   "metadata": {},
   "source": [
    "First, let's download the model artifact data from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d91f91-8ced-495d-9465-bda1f90cd670",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sagemaker\n",
    "import jinja2\n",
    "\n",
    "# - This will download the model into the current directory where ever the jupyter notebook is running\n",
    "local_model_path = Path(\".\")\n",
    "local_model_path.mkdir(exist_ok=True)\n",
    "model_name = HF_MODEL_ID\n",
    "# Only download pytorch checkpoint files\n",
    "allow_patterns = [\"*.json\", \"*.safetensors\", \"*.bin\", \"*.txt\"]\n",
    "\n",
    "# - Leverage the snapshot library to donload the model since the model is stored in repository using LFS\n",
    "model_download_path = snapshot_download(\n",
    "    repo_id=model_name,\n",
    "    cache_dir=local_model_path,\n",
    "    allow_patterns=allow_patterns,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b349167-cb34-49ce-a68f-db618e94adb3",
   "metadata": {},
   "source": [
    "### Upload model files to S3\n",
    "SageMaker AI allows us to provide [uncompressed](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-uncompressed.html) files. Thus, we directly upload the folder that contains model files to s3\n",
    "> **Note**: The default SageMaker bucket follows the naming pattern: `sagemaker-{region}-{account-id}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc558c2d-974b-430e-ac0c-739f5eb4cdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_model_prefix = (\n",
    "    \"hf-large-models/model_qwen3_32B_FP8\"  # folder within bucket where model artifact will go\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75875042-6ebe-48a1-954b-38ecbf1212ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_artifact = sagemaker_session.upload_data(path=model_download_path, key_prefix=s3_model_prefix)\n",
    "print(f\"Model uploaded to --- > {model_artifact}\")\n",
    "print(f\"We will set option.s3url={model_artifact}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d5d428-e250-47e8-b751-c48f38fd6b55",
   "metadata": {},
   "source": [
    "### Configure Model Serving Properties\n",
    "\n",
    "Now we'll create a `serving.properties` file that configures how the model will be served. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c753dfbe-803b-478a-8dd7-97c8928eaf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the directory that will contain the configuration files\n",
    "from pathlib import Path\n",
    "\n",
    "model_dir = Path('config-Qwen3-32B-FP8')\n",
    "model_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0094ec7a-5409-4ba8-8b23-b99c4d4a3bae",
   "metadata": {},
   "source": [
    "**Best Practices**:\n",
    ">\n",
    ">**Separate Configuration from Model Artifacts**\n",
    "> The LMI container supports separating configuration files from model artifacts. While you can store serving.properties with your model files, placing configurations in a distinct S3 location allows for better management of all your configurations files.\n",
    ">\n",
    "> **Note**: When your model and configuration files are in different S3 locations, set `option.model_id=<s3_model_uri>` in your serving.properties file, where `s3_model_uri` is the S3 object prefix containing your model artifacts. SageMaker AI will automatically download the model files by looking at the S3URI in model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb092ff-a52e-442f-890b-c7c6a7e3d08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = f\"\"\"engine=Python\n",
    "option.async_mode=true\n",
    "option.rolling_batch=disable\n",
    "option.entryPoint=djl_python.lmi_vllm.vllm_async_service\n",
    "option.model_loading_timeout=1500\n",
    "fail_fast=true\n",
    "option.max_rolling_batch_size=8\n",
    "option.trust_remote_code=false\n",
    "option.model_id={model_artifact}\n",
    "option.enable_auto_tool_choice=true\n",
    "option.tool_call_parser=hermes\n",
    "\"\"\"\n",
    "with open(\"config-Qwen3-32B-FP8/serving.properties\", \"w\") as f:\n",
    "    f.write(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff13de0-159d-4d77-95bd-362735c2ef08",
   "metadata": {},
   "source": [
    "#### Optional configuration files\n",
    "\n",
    "(Optional) You can also specify a `requirements.txt` to install additional libraries. \n",
    "We update vllm to version vllm==0.8.5 for FP8 quantized model files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ff5d49-e3be-4d9e-bb18-698aced5ffda",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile config-Qwen3-32B-FP8/requirements.txt\n",
    "vllm==0.8.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6dd00a-d351-4825-a8e4-6e7629e1c1fc",
   "metadata": {},
   "source": [
    "### Upload config files to S3\n",
    "Here we will upload our config files to a different path to keep model files and config separate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f34b933-26ad-4017-9cda-a4450d90f905",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "sagemaker_default_bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "config_files_uri = S3Uploader.upload(\n",
    "    local_path=\"config-Qwen3-32B-FP8\",\n",
    "    desired_s3_uri=f\"s3://{sagemaker_default_bucket}/lmi/{base_name}/config-files\"\n",
    ")\n",
    "\n",
    "print(f\"code_model_uri: {config_files_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c78d08f-407c-4c57-aa61-172fc28729f0",
   "metadata": {},
   "source": [
    "## Configure Model Container and Instance\n",
    "\n",
    "For deploying Qwen3-32B-FP8, we'll use:\n",
    "- **LMI (Deep Java Library) Inference Container**: A container optimized for large language model inference\n",
    "> **Note**: The region in the container URI should match your AWS region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c013786-ac4e-4213-b4a0-29c851077aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTAINER_VERSION = '0.33.0-lmi15.0.0-cu128'\n",
    "image_uri = \"763104351884.dkr.ecr.{}.amazonaws.com/djl-inference:0.33.0-lmi15.0.0-cu128\".format(sagemaker_session.boto_session.region_name, CONTAINER_VERSION)\n",
    "print(image_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4104f5e-883a-4ab3-a82e-93b3b85b43f4",
   "metadata": {},
   "source": [
    "## Create SageMaker Model\n",
    "\n",
    "Now we'll create a SageMaker Model object that combines our:\n",
    "- Container image (LMI)\n",
    "- Model artifacts (configuration files)\n",
    "- IAM role (for permissions)\n",
    "\n",
    "This step defines the model configuration but doesn't deploy it yet. The Model object represents the combination of:\n",
    "\n",
    "1. **Container Image** (`image_uri`): DJL Inference optimized for LLMs\n",
    "2. **Model Data** (`model_data`): points to our configuration files in S3\n",
    "3. **IAM Role** (`role`): Permissions for model execution\n",
    "\n",
    "### Required Permissions\n",
    "The IAM role needs:\n",
    "- S3 read access for model artifacts\n",
    "- CloudWatch permissions for logging\n",
    "- ECR permissions to pull the container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28c8d82-2371-4313-92ae-1e73e7b01445",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "qwen3_32b_fp8_model={\n",
    "    \"Image\": image_uri,\n",
    "    'ModelDataSource': {\n",
    "                'S3DataSource': {\n",
    "                    'S3Uri': f\"{config_files_uri}/\", # Specify the S3 URI for your config files\n",
    "                    'S3DataType': 'S3Prefix',\n",
    "                    'CompressionType': 'None',\n",
    "                }\n",
    "            },\n",
    "    \"Environment\": {\n",
    "        \"SAGEMAKER_MODEL_SERVER_WORKERS\": \"1\",\n",
    "    },\n",
    "}\n",
    "\n",
    "model_name = name_from_base(base_name, short=True)\n",
    "\n",
    "# create SageMaker Model\n",
    "sagemaker_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    Containers=[qwen3_32b_fp8_model],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc44fc9-f542-4841-8787-bdabc98e9728",
   "metadata": {},
   "source": [
    "> **Note**: Here S3 URI points to the configuration files S3 location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6fe9cf-a47c-4406-acbd-fd335ac08253",
   "metadata": {},
   "source": [
    "## Deploy Model to SageMaker Endpoint\n",
    "We can now create the Inference Components which will deployed on the endpoint that you specify. Please note here that you can provide a SageMaker model or a container to specification. If you provide a container, you will need to provide an image and artifactURL as parameters. In this example we set it to the model name we prepared in the cells above. You can also set the `ComputeResourceRequirements` to supply SageMaker what should be reserved for each copy of the inference component. You can also set the copy count of the number of Inference Components you would like to deploy. These can be managed and scaled as the capabilities become available. \n",
    "\n",
    "Note that in this example we set the `NumberOfAcceleratorDevicesRequired` to a value of `4`. By doing so we reserve 4 accelerators for each copy of this inference component so that we can use tensor parallel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90991e6-2fe3-4f2f-bc13-07b47325d640",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "prefix = sagemaker.utils.unique_name_from_base(\"DEMO\")\n",
    "\n",
    "inference_component_name_qwen = f\"{prefix}-IC-qwen3-32b-fp8-{datetime.now().strftime('%y%m%d-%H%M%S')}\"\n",
    "variant_name = \"AllTraffic\"\n",
    "\n",
    "sagemaker_client.create_inference_component(\n",
    "    InferenceComponentName=inference_component_name_qwen,\n",
    "    EndpointName=endpoint_name,\n",
    "    VariantName=variant_name,\n",
    "    Specification={\n",
    "        \"ModelName\": model_name,\n",
    "        \"ComputeResourceRequirements\": {\n",
    "            \"NumberOfAcceleratorDevicesRequired\": 4,\n",
    "            \"MinMemoryRequiredInMb\": 104096,\n",
    "        },\n",
    "    },\n",
    "    RuntimeConfig={\"CopyCount\": 1},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9bb18b-9f9f-4838-a81d-6d08d3091fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# Let's see how much it takes\n",
    "start_time = time.time()\n",
    "while True:\n",
    "    desc = sagemaker_client.describe_inference_component(\n",
    "        InferenceComponentName=inference_component_name_qwen\n",
    "    )\n",
    "    status = desc[\"InferenceComponentStatus\"]\n",
    "    print(status)\n",
    "    sys.stdout.flush()\n",
    "    if status in [\"InService\", \"Failed\"]:\n",
    "        break\n",
    "    time.sleep(30)\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nTotal time taken: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3158db2a-96db-42c5-8c28-4942513a6950",
   "metadata": {},
   "source": [
    "#### Invoke endpoint with boto3\n",
    "Now you can invoke the endpoint with boto3 `invoke_endpoint` or `invoke_endpoint_with_response_stream` runtime api calls. If you have an existing endpoint, you don't need to recreate the `predictor` and can follow below example to invoke the endpoint with an endpoint name.\n",
    "\n",
    "Note that based on the [Qwen3 hugging face page description](https://huggingface.co/Qwen/Qwen3-30B-A3B), by default, Qwen3 has thinking capabilities enabled, similar to QwQ-32B. This means the model will use its reasoning abilities to enhance the quality of generated responses. In this mode, the model will generate think content wrapped in a \\<think\\>...\\</think\\> block, followed by the final response. For thinking mode, use Temperature=0.6, TopP=0.95, TopK=20, and MinP=0 (the default setting in generation_config.json).\n",
    "\n",
    "It also allows a hard switch to strictly disable the model's thinking behavior, aligning its functionality with the previous Qwen2.5-Instruct models. This mode is particularly useful in scenarios where disabling thinking is essential for enhancing efficiency. For non-thinking mode, we suggest using Temperature=0.7, TopP=0.8, TopK=20, and MinP=0.\n",
    "\n",
    "**Advanced Usage**: You can also switch Between `Thinking` and `Non-Thinking` Modes via User Input\n",
    "Qwen3 provides a soft switch mechanism that allows users to dynamically control the model's behavior when `enable_thinking=True`. Specifically, you can add `/think` and `/no_think` to user prompts or system messages to switch the model's thinking mode from turn to turn. The model will follow the most recent instruction in multi-turn conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6ca443-c231-4935-a8d0-b101b2624835",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "sagemaker_runtime_client = boto3.client(\"sagemaker-runtime\", region_name=boto_region)\n",
    "\n",
    "prompt = {\n",
    "    'messages':[\n",
    "    {\"role\": \"user\", \"content\": \"How many R are in STRAWBERRY? Keep your answer and explanation short!\"}\n",
    "],\n",
    "    'temperature':0.7,\n",
    "    'top_p':0.8,\n",
    "    'top_k':20,\n",
    "    'max_tokens':512,\n",
    "}\n",
    "response = sagemaker_runtime_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    InferenceComponentName=inference_component_name_qwen,\n",
    "    ContentType=\"application/json\",\n",
    "    Body=json.dumps(prompt)\n",
    ")\n",
    "response_dict = json.loads(response['Body'].read().decode(\"utf-8\"))\n",
    "response_content = response_dict['choices'][0]['message']['content']\n",
    "print(response_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9ce721-ec1a-4143-8c23-059d909b4d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch to no thinking using chat_template_kwargs\n",
    "prompt = {\n",
    "    'messages':[\n",
    "    {\"role\": \"user\", \"content\": \"How many R are in STRAWBERRY? Keep your answer and explanation short!\"}\n",
    "],\n",
    "    'temperature':0.7,\n",
    "    'top_p':0.8,\n",
    "    'top_k':20,\n",
    "    'max_tokens':512,\n",
    "    \"chat_template_kwargs\": {\"enable_thinking\": False},\n",
    "}\n",
    "response = sagemaker_runtime_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    InferenceComponentName=inference_component_name_qwen,\n",
    "    ContentType=\"application/json\",\n",
    "    Body=json.dumps(prompt)\n",
    ")\n",
    "response_dict = json.loads(response['Body'].read().decode(\"utf-8\"))\n",
    "response_content = response_dict['choices'][0]['message']['content']\n",
    "print(response_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a161ae-57ac-4d37-b74b-4f3b4744ad89",
   "metadata": {},
   "source": [
    "#### Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f0fd4c-9af4-42a7-81fc-af16e9f765b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "body = {\n",
    "    'messages':[\n",
    "        {\"role\": \"user\", \"content\": \"How many R are in STRAWBERRY? Keep your answer and explanation short!\"},\n",
    "    ],\n",
    "    'temperature':0.9,\n",
    "    'max_tokens':512,\n",
    "    'stream': True,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e3b708-cf53-402d-8dff-d3b465775fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "# Create SageMaker Runtime client\n",
    "sagemaker_runtime_client = boto3.client(\"sagemaker-runtime\", region_name=boto_region)\n",
    "\n",
    "## Add your endpoint here \n",
    "# endpoint_name = \"DEMO-1749690961-f05b-endpoint\"\n",
    "\n",
    "# Invoke the model\n",
    "response_stream = sagemaker_runtime_client.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/json\",\n",
    "    InferenceComponentName=inference_component_name_qwen,\n",
    "    Body=json.dumps(body)\n",
    ")\n",
    "\n",
    "first_token_received = False\n",
    "ttft = None\n",
    "token_count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"Response:\", end=' ', flush=True)\n",
    "full_response = \"\"\n",
    "\n",
    "for event in response_stream['Body']:\n",
    "    if 'PayloadPart' in event:\n",
    "        chunk = event['PayloadPart']['Bytes'].decode()\n",
    "        \n",
    "        try:\n",
    "            # Handle SSE format (data: prefix)\n",
    "            if chunk.startswith('data: '):\n",
    "                data = json.loads(chunk[6:])  # Skip \"data: \" prefix\n",
    "            else:\n",
    "                data = json.loads(chunk)\n",
    "            \n",
    "            # Extract token based on OpenAI format\n",
    "            if 'choices' in data and len(data['choices']) > 0:\n",
    "                if 'delta' in data['choices'][0] and 'content' in data['choices'][0]['delta']:\n",
    "                    token_count += 1\n",
    "                    token_text = data['choices'][0]['delta']['content']\n",
    "                                    # Record time to first token\n",
    "                    if not first_token_received:\n",
    "                        ttft = time.time() - start_time\n",
    "                        first_token_received = True\n",
    "                    full_response += token_text\n",
    "                    print(token_text, end='', flush=True)\n",
    "        \n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "            \n",
    "# Print metrics after completion\n",
    "end_time = time.time()\n",
    "total_latency = end_time - start_time\n",
    "\n",
    "print(\"\\n\\nMetrics:\")\n",
    "print(f\"Time to First Token (TTFT): {ttft:.2f} seconds\" if ttft else \"TTFT: N/A\")\n",
    "print(f\"Total Tokens Generated: {token_count}\")\n",
    "print(f\"Total Latency: {total_latency:.2f} seconds\")\n",
    "if token_count > 0 and total_latency > 0:\n",
    "    print(f\"Tokens per second: {token_count/total_latency:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9783d3-a8e4-4c86-81f8-054e2175ce5a",
   "metadata": {},
   "source": [
    "# Clean up\n",
    "\n",
    "Make sure to delete the endpoint and other artifacts that were created to avoid unnecessary cost. You can also go to SageMaker AI console to delete all the resources created in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7984819c-e3ec-47d9-92a8-d91fa4998b55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker_client.delete_inference_component(InferenceComponentName=inference_component_name_qwen)\n",
    "sagemaker_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "sagemaker_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
