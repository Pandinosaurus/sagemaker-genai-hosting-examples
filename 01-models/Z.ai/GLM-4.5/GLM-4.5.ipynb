{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "993dcf59-dd85-4a31-a8cc-02c5d4be5dfe",
   "metadata": {},
   "source": [
    "# Deploy zai-org/GLM-4.5 model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718c67ba-30c7-421d-8502-c21634c6a8df",
   "metadata": {},
   "source": [
    "The GLM-4.5 series models are foundation models designed for intelligent agents. GLM-4.5 has 355 billion total parameters with 32 billion active parameters, while GLM-4.5-Air adopts a more compact design with 106 billion total parameters and 12 billion active parameters. GLM-4.5 models unify reasoning, coding, and intelligent agent capabilities to meet the complex demands of intelligent agent applications.\n",
    "\n",
    "Both GLM-4.5 and GLM-4.5-Air are hybrid reasoning models that provide two modes: thinking mode for complex reasoning and tool usage, and non-thinking mode for immediate responses.\n",
    "\n",
    "See [technical blog](https://z.ai/blog/glm-4.5) for more details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1649adc6-4e2d-4ebe-badb-aa47ee4c48f4",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "First, we'll upgrade the SageMaker SDK to ensure compatibility with the latest features, particularly those needed for large language model deployment and streaming inference.\n",
    "\n",
    "> **Note**: The `--quiet` and `--no-warn-conflicts` flags are used to minimize unnecessary output while installing dependencies.\n",
    "\n",
    "> ⚠️ **Important**: After running the installation cell below, you may need to restart your notebook kernel to ensure the updated packages are properly loaded. To do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c01a4b2-b0d3-420c-8335-4efd984bb5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet --no-warn-conflicts sagemaker sagemaker-core python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9183bf15-3f3f-4d70-91da-fe269ff421b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {bucket}\")\n",
    "print(f\"sagemaker session region: {region}\")\n",
    "print(f\"sagemaker version: {sagemaker.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebdb7fb-319f-4db9-92ea-0c7a94212365",
   "metadata": {},
   "source": [
    "## Configure Model Container and Instance\n",
    "\n",
    "For deploying GLM-4.5, we'll use:\n",
    "- **vLLM v0.10.1**: A container optimized for large language model inference\n",
    "- **P5 Instance**: AWS's latest GPU instance type optimized for large model inference\n",
    "\n",
    "Key configurations:\n",
    "- The container URI points to the DJL inference container in ECR (Elastic Container Registry)\n",
    "- We use `ml.p5en.48xlarge` instance which offer:\n",
    "  - 8 NVIDIA H200 GPUs\n",
    "  - 1128 GB of memory\n",
    "  - High network bandwidth for optimal inference performance\n",
    "\n",
    "> **Note**: The region in the container URI should match your AWS region. Replace `us-east-1` with your region if different."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1634e71f-a190-408f-bcb9-a314cd057e30",
   "metadata": {},
   "source": [
    "## Deployment of GLM-4.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1350fac7-a513-4944-b728-df7bb2797720",
   "metadata": {},
   "source": [
    "Amazon SageMaker AI provides the ability to build Docker containers to run on SageMaker endpoints, where they listen for health checks on `/ping` and receive real-time inference requests on `/invocations`.\n",
    "\n",
    "Below, we'll demonstrate how to adapt the [vLLM](https://github.com/vllm-project/vllm) framework to run on SageMaker AI endpoints.\n",
    "\n",
    "#### Container preparation:\n",
    "1. Enable Docker access in your Studio domain. Please see [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-updated-local-get-started.html#studio-updated-local-enable) for details\n",
    "2. Install Docker in your Studion environment. See this [link](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-updated-local-get-started.html#studio-updated-local-docker-installation) for details\n",
    "3. Please modify `build.sh` to change the account_id, region, repository name, and tag if required\n",
    "4. Build the image and push to ECR repository using `build.sh` in docker directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cade5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile docker/build.sh\n",
    "export REGION=YOUR_REGION\n",
    "export ACCOUNT_ID=YOUR_ACCOUNT_ID\n",
    "export REPOSITORY_NAME=vllm\n",
    "export TAG=v0.10.1\n",
    "\n",
    "full_name=\"${ACCOUNT_ID}.dkr.ecr.${REGION}.amazonaws.com/${REPOSITORY_NAME}:${TAG}\"\n",
    "\n",
    "DOCKER_BUILDKIT=1 docker build . --tag $REPOSITORY_NAME:$TAG --file Dockerfile\n",
    "\n",
    "aws ecr get-login-password --region $REGION | docker login --username AWS --password-stdin $ACCOUNT_ID.dkr.ecr.$REGION.amazonaws.com\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --region ${REGION} --repository-names \"${REPOSITIRY_NAME}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --region ${REGION} --repository-name \"${REPOSITORY_NAME}\" > /dev/null\n",
    "fi\n",
    "\n",
    "docker tag $REPOSITORY_NAME:$TAG ${full_name}\n",
    "docker push ${full_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851edddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cd docker; ./build.sh; cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e40e8c9",
   "metadata": {},
   "source": [
    "Let's deploy full version of the model using vLLM container. Please see [HugginFace Repo](https://huggingface.co/zai-org/GLM-4.5) for more details about the model.\n",
    "\n",
    "Please note you will need access to `ml.p5en.48xlarge` instance to deploy the full version of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfe1787-fbc1-4705-a2f7-c281736bf9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = {\n",
    "    \"OPTION_MODEL\": \"zai-org/GLM-4.5\",\n",
    "    \"OPTION_MAX_MODEL_LEN\": \"16384\",\n",
    "    \"OPTION_TENSOR_PARALLEL_SIZE\": \"8\"\n",
    "}\n",
    "inference_image = f\"736221153822.dkr.ecr.{region}.amazonaws.com/vllm:v0.10.1\"\n",
    "model_name = sagemaker.utils.name_from_base(\"glm4-5\")\n",
    "endpoint_config_name = model_name\n",
    "endpoint_name = model_name\n",
    "instance_type = \"ml.p5en.48xlarge\"\n",
    "timeout = 3600"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9a52f1-e90d-4744-ab68-997812e3f826",
   "metadata": {},
   "source": [
    "We are using sagemaker_core SDK for model deployment. \n",
    "\n",
    "This [SDK](https://github.com/aws/sagemaker-core) offers more \"Pythonic\" syntax compared to boto3\n",
    "\n",
    "**Key Features:**\n",
    "- Object-Oriented Interface: Provides a structured way to interact with SageMaker resources, making it easier to manage them using familiar object-oriented programming techniques.\n",
    "- Resource Chaining: Allows seamless connection of SageMaker resources by passing outputs as inputs between them, simplifying workflows and reducing the complexity of parameter management.\n",
    "- Full Parity with SageMaker APIs: Ensures access to all SageMaker capabilities through the SDK, providing a comprehensive toolset for building and deploying machine learning models.\n",
    "- Abstraction of Low-Level Details: Automatically handles resource state transitions and polling logic, freeing developers from managing these intricacies and allowing them to focus on higher-level tasks.\n",
    "- Auto Code Completion: Enhances the developer experience by offering real-time suggestions and completions in popular IDEs, reducing syntax errors and speeding up the coding process.\n",
    "- Comprehensive Documentation and Type Hints: Provides detailed guidance and type hints to help developers understand functionalities, write code faster, and reduce errors without complex API navigation.\n",
    "- Incorporation of Intelligent Defaults: Integrates the previous SageMaker SDK feature of intelligent defaults, allowing developers to set default values for parameters like IAM roles and VPC configurations. This streamlines the setup process, enabling developers to focus on customizations specific to their use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf216653-f9d3-447b-b79a-7975d5abe5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker_core.shapes import ContainerDefinition, ProductionVariant\n",
    "from sagemaker_core.resources import Model, EndpointConfig, Endpoint\n",
    "from sagemaker_core.shapes import ProductionVariantRoutingConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43c0efc-963d-4f1d-a170-7a962c31ff0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model.create(\n",
    "    model_name=model_name,\n",
    "    primary_container=ContainerDefinition(image=inference_image, environment=env),\n",
    "    execution_role_arn=role,\n",
    "    session=sess.boto_session,\n",
    "    region=region,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c002105-b0aa-4722-8dca-7083542f13ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config = EndpointConfig.create(\n",
    "        endpoint_config_name=endpoint_config_name,\n",
    "        production_variants=[\n",
    "            ProductionVariant(\n",
    "                variant_name=model_name,\n",
    "                initial_instance_count=1,\n",
    "                instance_type=instance_type,\n",
    "                model_name=model,\n",
    "                container_startup_health_check_timeout_in_seconds=timeout,\n",
    "                model_data_download_timeout_in_seconds=timeout,\n",
    "                routing_config=ProductionVariantRoutingConfig(routing_strategy=\"LEAST_OUTSTANDING_REQUESTS\"),\n",
    "            )\n",
    "        ],\n",
    "    ),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1338644a-e3ed-46c6-9618-6fceda72f750",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = Endpoint.create(\n",
    "    endpoint_name=endpoint_name,\n",
    "    endpoint_config_name=endpoint_config_name\n",
    ")\n",
    "endpoint.wait_for_status(\"InService\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520be053-b13c-4b7b-be56-cdac055d4d55",
   "metadata": {},
   "source": [
    "## Inference examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00164cd-0842-4242-8446-5f2c57ed96df",
   "metadata": {},
   "source": [
    "### Invocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03135bd9-db71-494b-9abc-8eac0ea4a2d0",
   "metadata": {},
   "source": [
    "This is hybrid (reasoning/non-reasonin) model. \n",
    "\n",
    "You can enable or disable \"thinking\" mode by specifying `{\"chat_template_kwargs\": {\"enable_thinking\": False}}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f40b2de-e08a-4fc3-990c-1478c7474e6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T22:07:54.528516Z",
     "iopub.status.busy": "2025-08-27T22:07:54.528182Z",
     "iopub.status.idle": "2025-08-27T22:07:54.807244Z",
     "shell.execute_reply": "2025-08-27T22:07:54.806718Z",
     "shell.execute_reply.started": "2025-08-27T22:07:54.528495Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############\n",
      "The letter \"b\" appears **two** times in \"blueberry\".\n",
      "###############\n",
      "\n",
      "{'prompt_tokens': 21, 'total_tokens': 37, 'completion_tokens': 16, 'prompt_tokens_details': None}\n"
     ]
    }
   ],
   "source": [
    "payload = json.dumps(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"How many times does the letter b appear in blueberry?\"}\n",
    "        ],\n",
    "        \"chat_template_kwargs\": {\n",
    "            \"enable_thinking\": False\n",
    "        }\n",
    "    }\n",
    ")\n",
    "res = smr_client.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                 Body=payload,\n",
    "                                 ContentType=\"application/json\")\n",
    "\n",
    "response = json.loads(res[\"Body\"].read().decode(\"utf8\"))\n",
    "content = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "usage = response[\"usage\"]\n",
    "\n",
    "print(\"###############\\n\" + content + \"\\n###############\\n\")\n",
    "print(usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509b8555-3db0-4082-b6f6-19f754f51760",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T22:07:57.577751Z",
     "iopub.status.busy": "2025-08-27T22:07:57.577409Z",
     "iopub.status.idle": "2025-08-27T22:08:12.959531Z",
     "shell.execute_reply": "2025-08-27T22:08:12.958991Z",
     "shell.execute_reply.started": "2025-08-27T22:07:57.577728Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############\n",
      "\n",
      "<think>First, the question is: \"How many times does the letter 'b' appear in 'blueberry'?\"\n",
      "\n",
      "I need to count the occurrences of the letter 'b' in the word \"blueberry\".\n",
      "\n",
      "Let me write down the word: B-L-U-E-B-E-R-R-Y.\n",
      "\n",
      "Now, I'll go through each letter and check if it's a 'b'.\n",
      "\n",
      "- Position 1: B – yes, that's a 'b'.\n",
      "\n",
      "- Position 2: L – not 'b'.\n",
      "\n",
      "- Position 3: U – not 'b'.\n",
      "\n",
      "- Position 4: E – not 'b'.\n",
      "\n",
      "- Position 5: B – yes, that's another 'b'.\n",
      "\n",
      "- Position 6: E – not 'b'.\n",
      "\n",
      "- Position 7: R – not 'b'.\n",
      "\n",
      "- Position 8: R – not 'b'.\n",
      "\n",
      "- Position 9: Y – not 'b'.\n",
      "\n",
      "So, I found 'b' at positions 1 and 5. That's two times.\n",
      "\n",
      "I should confirm the spelling. Is \"blueberry\" spelled correctly? Yes, it's B-L-U-E-B-E-R-R-Y. No 'b's after that.\n",
      "\n",
      "The question is about the letter 'b', and it's case-sensitive? The word \"blueberry\" is typically written in lowercase, but in this context, it's probably not case-sensitive, or we should consider it as given. The question says \"the letter b\", and in \"blueberry\", it's all lowercase, so 'b' is lowercase. I think we should count all instances of 'b', regardless of case, but since it's all lowercase, it's fine.\n",
      "\n",
      "In the word, both 'b's are at the beginning and after 'e', so they are lowercase 'b's.\n",
      "\n",
      "No uppercase here.\n",
      "\n",
      "So, only two 'b's.\n",
      "\n",
      "I recall that \"blueberry\" has two 'b's: one in \"blue\" and one in \"berry\".\n",
      "\n",
      "\"Blue\" has one 'b', and \"berry\" has one 'b', so total two.\n",
      "\n",
      "If I think about it phonetically, but spelling-wise, it's clear.\n",
      "\n",
      "I can count the letters:\n",
      "\n",
      "- B: 1\n",
      "\n",
      "- L: 1\n",
      "\n",
      "- U: 1\n",
      "\n",
      "- E: 2 (positions 4 and 6, but that's 'e', not 'b')\n",
      "\n",
      "- B: 1 (position 5)\n",
      "\n",
      "- R: 2\n",
      "\n",
      "- Y: 1\n",
      "\n",
      "So, 'b' appears twice.\n",
      "\n",
      "The word has 9 letters, but we're only counting 'b's.\n",
      "\n",
      "Is there any trick here? Like, is \"blueberry\" written with a capital B? But in the question, it's given as \"blueberry\", so I think it's lowercase.\n",
      "\n",
      "The question says: \"the letter b appear in blueberry\" – and \"blueberry\" is in lowercase in the text.\n",
      "\n",
      "In the user's message, it's \"blueberry\", so all lowercase.\n",
      "\n",
      "So, no issue.\n",
      "\n",
      "Perhaps I should consider if 'b' is part of a digraph or something, but no, we're counting individual letters.\n",
      "\n",
      "For example, in \"blueberry\", 'b' is standalone in both cases.\n",
      "\n",
      "No \"bb\" or anything.\n",
      "\n",
      "So, I think it's straightforward.\n",
      "\n",
      "Therefore, the letter 'b' appears twice in \"blueberry\".\n",
      "\n",
      "But let me double-check the spelling.\n",
      "\n",
      "Blueberry: B-L-U-E-B-E-R-R-Y. Yes.\n",
      "\n",
      "I can write it out: b l u e b e r r y.\n",
      "\n",
      "'b' at index 1 and index 5, if we start counting from 1.\n",
      "\n",
      "In programming, indices might start from 0, but for counting occurrences, it doesn't matter.\n",
      "\n",
      "So, positions: first character b, fifth character b.\n",
      "\n",
      "Two times.\n",
      "\n",
      "I think I'm overthinking this.\n",
      "\n",
      "The answer should be 2.\n",
      "\n",
      "The question is phrased simply, so probably no trick.\n",
      "\n",
      "But just to be thorough, is \"blueberry\" sometimes misspelled? Like, some might think it's \"blueberry\" with only one 'b', but no, standard spelling has two 'b's.\n",
      "\n",
      "Blue + berry, both have 'b'.\n",
      "\n",
      "Yes.\n",
      "\n",
      "In a dictionary, it's clear.\n",
      "\n",
      "So, I think it's two.\n",
      "\n",
      "Therefore, the letter 'b' appears two times in \"blueberry\".</think>The word \"blueberry\" is spelled: B-L-U-E-B-E-R-R-Y.\n",
      "\n",
      "- The first letter is \"b\".\n",
      "- The fifth letter is \"b\".\n",
      "\n",
      "No other letters in \"blueberry\" are \"b\". Therefore, the letter \"b\" appears **2 times** in \"blueberry\".\n",
      "###############\n",
      "\n",
      "{'prompt_tokens': 17, 'total_tokens': 978, 'completion_tokens': 961, 'prompt_tokens_details': None}\n"
     ]
    }
   ],
   "source": [
    "payload = json.dumps(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"How many times does the letter b appear in blueberry?\"}\n",
    "        ],\n",
    "        \"chat_template_kwargs\": {\n",
    "            \"enable_thinking\": True\n",
    "        }\n",
    "    }\n",
    ")\n",
    "res = smr_client.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                 Body=payload,\n",
    "                                 ContentType=\"application/json\")\n",
    "\n",
    "response = json.loads(res[\"Body\"].read().decode(\"utf8\"))\n",
    "content = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "usage = response[\"usage\"]\n",
    "\n",
    "print(\"###############\\n\" + content + \"\\n###############\\n\")\n",
    "print(usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa22075-a64a-46e4-a543-731501d19886",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# By default for chat completion the 'thinking' mode is enabled.\n",
    "#\n",
    "payload = json.dumps(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"What is bigger 9.8 or 9.11?\"}\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "res = smr_client.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                 Body=payload,\n",
    "                                 ContentType=\"application/json\")\n",
    "\n",
    "response = json.loads(res[\"Body\"].read().decode(\"utf8\"))\n",
    "content = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "usage = response[\"usage\"]\n",
    "\n",
    "print(\"###############\\n\" + content + \"\\n###############\\n\")\n",
    "print(usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e26058-22d2-47af-b3fd-7d71bd08d9e1",
   "metadata": {},
   "source": [
    "### Streaming invocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e33b46-53a8-4e6d-8d52-76a553193a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import time\n",
    "import boto3\n",
    "from IPython.display import clear_output\n",
    "\n",
    "class LineIterator:\n",
    "    def __init__(self, stream):\n",
    "        self.byte_iterator = iter(stream)\n",
    "        self.buffer = io.BytesIO()\n",
    "        self.read_pos = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        while True:\n",
    "            self.buffer.seek(self.read_pos)\n",
    "            line = self.buffer.readline()\n",
    "            if line and line[-1] == ord(\"\\n\"):\n",
    "                self.read_pos += len(line)\n",
    "                return line[:-1]\n",
    "            try:\n",
    "                chunk = next(self.byte_iterator)\n",
    "            except StopIteration:\n",
    "                if self.read_pos < self.buffer.getbuffer().nbytes:\n",
    "                    continue\n",
    "                raise\n",
    "            if \"PayloadPart\" not in chunk:\n",
    "                print(\"Unknown event type:\" + chunk)\n",
    "                continue\n",
    "            self.buffer.seek(0, io.SEEK_END)\n",
    "            self.buffer.write(chunk[\"PayloadPart\"][\"Bytes\"])\n",
    "\n",
    "def stream_response(endpoint_name, inputs, max_tokens=8189, temperature=0.7, top_p=0.9):\n",
    "    body = {\n",
    "      \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": inputs}]}\n",
    "        ],\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p,\n",
    "        \"stream\": True,\n",
    "    }\n",
    "\n",
    "    resp = smr_client.invoke_endpoint_with_response_stream(\n",
    "        EndpointName=endpoint_name,\n",
    "        Body=json.dumps(body),\n",
    "        ContentType=\"application/json\",\n",
    "    )\n",
    "\n",
    "    event_stream = resp[\"Body\"]\n",
    "    start_json = b\"{\"\n",
    "    full_response = \"\"\n",
    "    start_time = time.time()\n",
    "    token_count = 0\n",
    "\n",
    "    for line in LineIterator(event_stream):\n",
    "        if line != b\"\" and start_json in line:\n",
    "            data = json.loads(line[line.find(start_json):].decode(\"utf-8\"))\n",
    "            token_text = data['choices'][0]['delta'].get('content', '')\n",
    "            full_response += token_text\n",
    "            token_count += 1\n",
    "\n",
    "            # Calculate tokens per second\n",
    "            elapsed_time = time.time() - start_time\n",
    "            tps = token_count / elapsed_time if elapsed_time > 0 else 0\n",
    "\n",
    "            # Clear the output and reprint everything\n",
    "            clear_output(wait=True)\n",
    "            print(full_response)\n",
    "            print(f\"\\nTokens per Second: {tps:.2f}\", end=\"\")\n",
    "\n",
    "    print(\"\\n\") # Add a newline after response is complete\n",
    "\n",
    "    return full_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017e0db2-4f50-4307-981e-4c59dea3e9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = \"What is greater 9.11 or 9.8?\"\n",
    "output = stream_response(endpoint_name, inputs, max_tokens=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66050d7-9825-45b1-ba96-eec0f85c349f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = \"How many times does the letter b appear in blueberry?\"\n",
    "output = stream_response(endpoint_name, inputs, max_tokens=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6549f6dc-8a52-47f3-9c06-cf32a26199d8",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b152277a-9f1d-4f10-b561-0ab30d54a421",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T22:08:26.774196Z",
     "iopub.status.busy": "2025-08-27T22:08:26.773852Z",
     "iopub.status.idle": "2025-08-27T22:08:27.362423Z",
     "shell.execute_reply": "2025-08-27T22:08:27.361871Z",
     "shell.execute_reply.started": "2025-08-27T22:08:26.774175Z"
    }
   },
   "outputs": [],
   "source": [
    "sess.delete_endpoint(endpoint_name)\n",
    "sess.delete_endpoint_config(endpoint_config_name)\n",
    "sess.delete_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d169a2a-8614-4d83-b5b4-fb5945ec0053",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
