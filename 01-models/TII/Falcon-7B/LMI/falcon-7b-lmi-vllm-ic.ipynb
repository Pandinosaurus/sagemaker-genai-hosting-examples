{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71a329f0",
   "metadata": {},
   "source": [
    "# Deploy Falcon 7B on Amazon SageMaker using LMI and vLLM\n",
    "\n",
    "## Resources\n",
    "- [Falcon-7B model card](https://huggingface.co/tiiuae/falcon-7b)\n",
    "- [LMI Configuration Documentation](https://docs.djl.ai/docs/serving/serving/docs/lmi/configurations_large_model_inference_containers.html)\n",
    "- [DJL-Demo Samples](https://github.com/deepjavalibrary/djl-demo/tree/2a5152f578f5954b8b68acdee18eed4e2a75c81f/aws/sagemaker/large-model-inference/sample-llm)\n",
    "- [vLLM documentation](https://docs.vllm.ai/en/latest/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8974eee7-cc31-4dac-8795-aec6b8765051",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fa3208",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9ac353",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import json\n",
    "print(f\"boto3 version: {boto3.__version__}\")\n",
    "print(f\"sagemaker version: {sagemaker.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94eba2f6-e1b6-41c6-94d1-2b2bfbe3308b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "region = sess._region_name\n",
    "\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e58cf33",
   "metadata": {},
   "source": [
    "## Step 2: Create a model, endpoint configuration and endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbef0bb-14ce-47e9-b99f-7cdd4afe7f7c",
   "metadata": {},
   "source": [
    "Retrieve the ECR image URI for the DJL TensorRT accelerated large language model framework. The image URI is looked up based on the framework name, AWS region, and framework version. This allows us to dynamically select the right Docker image for our environment.\n",
    "\n",
    "Functions for generating ECR image URIs for pre-built SageMaker Docker images. See available Large Model Inference DLC's [here](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#large-model-inference-containers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a174b36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "version = \"0.26.0\"\n",
    "inference_image_uri = sagemaker.image_uris.retrieve(\n",
    "    \"djl-deepspeed\", region=region, version=version\n",
    ")\n",
    "print(f\"Image going to be used is ----> {inference_image_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66472473-8b8f-4db4-996c-e7de9487b100",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = sagemaker.utils.name_from_base(\"falcon7b-lmi-vllm\")\n",
    "print(model_name)\n",
    "\n",
    "env = {\n",
    "    \"SERVING_LOAD_MODELS\": \"test::Python=/opt/ml/model\",\n",
    "    \"OPTION_MODEL_ID\": \"tiiuae/falcon-7b\",\n",
    "    \"OPTION_ROLLING_BATCH\": \"vllm\",\n",
    "    \"OPTION_TENSOR_PARALLEL_DEGREE\": \"1\",\n",
    "}\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    PrimaryContainer = {\n",
    "        \"Image\": inference_image_uri, \n",
    "        \"Environment\": env,\n",
    "    },\n",
    ")\n",
    "model_arn = create_model_response[\"ModelArn\"]\n",
    "\n",
    "print(f\"Created Model: {model_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88456f4c-0336-4b69-b015-4c220520edfb",
   "metadata": {},
   "source": [
    "These two cells below deploy the model to a SageMaker endpoint for real-time inference. The instance_type defines the machine instance for the endpoint. The endpoint name is programmatically generated based on the base name. The model is deployed with a large container startup timeout specified, as the TensorRT model takes time to initialize on the GPU instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0499885a-e95f-4342-88cc-450ca076dc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_name = f\"{model_name}-config\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d67461-7753-4c92-8179-88c47cd11120",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set varient name and instance type for hosting\n",
    "variant_name = \"AllTraffic\"\n",
    "instance_type = \"ml.g5.12xlarge\"\n",
    "model_data_download_timeout_in_seconds = 1200\n",
    "container_startup_health_check_timeout_in_seconds = 1200\n",
    "\n",
    "initial_instance_count = 1\n",
    "max_instance_count = 2 # will use for managed instance scaling later\n",
    "print(f\"Initial instance count: {initial_instance_count}\")\n",
    "print(f\"Max instance count: {max_instance_count}\")\n",
    "\n",
    "sm_client.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    ProductionVariants = [\n",
    "        {\n",
    "            \"VariantName\": variant_name,\n",
    "            \"InstanceType\": instance_type,\n",
    "            \"InitialInstanceCount\": initial_instance_count,\n",
    "            \"ModelDataDownloadTimeoutInSeconds\": model_data_download_timeout_in_seconds,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": container_startup_health_check_timeout_in_seconds,\n",
    "            \"ManagedInstanceScaling\": {\n",
    "                \"Status\": \"ENABLED\",\n",
    "                \"MinInstanceCount\": initial_instance_count,\n",
    "                \"MaxInstanceCount\": max_instance_count,\n",
    "            },\n",
    "            \"RoutingConfig\": {\"RoutingStrategy\": \"LEAST_OUTSTANDING_REQUESTS\"},\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ecb90b-4443-4e35-bea2-a6bc77e3f193",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = f\"{model_name}-endpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e0a6c8-a1dc-4e4d-81a8-526c723231dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName = endpoint_name, EndpointConfigName = endpoint_config_name\n",
    ")\n",
    "print(f\"Created Endpoint: {create_endpoint_response['EndpointArn']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079b56f3-ba5a-4423-ba75-bdc40f597ba6",
   "metadata": {},
   "source": [
    "### This step can take ~ 10 min or longer so please be patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d993dd7b-792e-4992-add7-8d3d49492cbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Using helper function to wait for the endpoint to be ready\n",
    "#\n",
    "sess.wait_for_endpoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbf6932-735f-435b-95e2-c7c28e9e8a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_component_name = f\"{model_name}-ic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7c1c01-cdca-46ab-b19c-3e78e379cf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Test inference component name: {inference_component_name}\")\n",
    "\n",
    "initial_copy_count = 1\n",
    "max_copy_count_per_instance = 4  # will use later for autoscaling\n",
    "\n",
    "variant_name = \"AllTraffic\"\n",
    "\n",
    "min_memory_required_in_mb = 1024 \n",
    "number_of_accelerator_devices_required = 1\n",
    "\n",
    "sm_client.create_inference_component(\n",
    "    InferenceComponentName = inference_component_name,\n",
    "    EndpointName = endpoint_name,\n",
    "    VariantName = variant_name,\n",
    "    Specification={\n",
    "        \"ModelName\": model_name,\n",
    "        \"StartupParameters\": {\n",
    "            \"ModelDataDownloadTimeoutInSeconds\": model_data_download_timeout_in_seconds,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": container_startup_health_check_timeout_in_seconds,\n",
    "        },\n",
    "        \"ComputeResourceRequirements\": {\n",
    "            \"MinMemoryRequiredInMb\": min_memory_required_in_mb,\n",
    "            \"NumberOfAcceleratorDevicesRequired\": number_of_accelerator_devices_required,\n",
    "        },\n",
    "    },\n",
    "    RuntimeConfig={\n",
    "        \"CopyCount\": initial_copy_count,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f32dda-db04-459c-97af-010f214473d0",
   "metadata": {},
   "source": [
    "### This step can take ~ 10 min or longer so please be patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318969f4-ee22-421a-91bc-60db8975ae10",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.wait_for_inference_component(inference_component_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58be20f4-15ba-4659-b8a9-358e79e7c119",
   "metadata": {},
   "source": [
    "## Step 3: Invoke the Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993ac417-6a69-4ad9-a311-0bc7f0de5b72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "response_model = smr_client.invoke_endpoint(\n",
    "    EndpointName = endpoint_name,\n",
    "    InferenceComponentName = inference_component_name,\n",
    "    Body = json.dumps(\n",
    "        {\n",
    "            \"inputs\": \"What is AWS re:invent? Where does it happen every year?\", \n",
    "            \"parameters\": {\"max_new_tokens\": 256, \"do_sample\": True}\n",
    "        }\n",
    "    ),\n",
    "    ContentType = \"application/json\",\n",
    ")\n",
    "\n",
    "response_model[\"Body\"].read().decode(\"utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9251aed4-20fe-4b98-aaf9-ee7eb4cdfc27",
   "metadata": {},
   "source": [
    "## (Optional) Step 4: Define and test autoscaling policy\n",
    "\n",
    "We define the scaling policy for desired copy count of inference component instances.\n",
    "\n",
    "**Please note:**\n",
    "- SageMaker endpoint will have to perform JIT compilation for every IC we start\n",
    "- We created our endpoint with managed instance scaling thus SageMaker endpoint will start additional instances automatically to satisfy the requested number of inference component instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaa0bcc-3e33-49d4-87e3-6680fd3e1bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "aas_client = sess.boto_session.client(\"application-autoscaling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ccf1f4-06c6-45fd-bb7d-80590898e762",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_copy_count = max_copy_count_per_instance * max_instance_count\n",
    "print(f\"Initial copy count: {initial_copy_count}\")\n",
    "print(f\"Max copy county: {max_copy_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f38563e-b112-491b-9f11-d4fe31baa1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoscaling parameters\n",
    "resource_id = f\"inference-component/{inference_component_name}\"\n",
    "service_namespace = \"sagemaker\"\n",
    "scalable_dimension = \"sagemaker:inference-component:DesiredCopyCount\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a421aa-203f-45c8-b169-22165278db95",
   "metadata": {},
   "outputs": [],
   "source": [
    "aas_client.register_scalable_target(\n",
    "    ServiceNamespace=service_namespace,\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=scalable_dimension,\n",
    "    MinCapacity=initial_copy_count,\n",
    "    MaxCapacity=max_copy_count,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420d7e1f-4ccf-49a2-9eb2-dc5bfe1c4422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "#aas_client.describe_scalable_targets(\n",
    "#    ServiceNamespace=service_namespace,\n",
    "#    ResourceIds=[resource_id],\n",
    "#    ScalableDimension=scalable_dimension,\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fe62c8-5f12-4f08-9425-e7b2f16aac6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Scalable policy\n",
    "#\n",
    "aas_client.put_scaling_policy(\n",
    "    PolicyName=endpoint_name,\n",
    "    PolicyType=\"TargetTrackingScaling\",\n",
    "    ServiceNamespace=service_namespace,\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=scalable_dimension,\n",
    "    TargetTrackingScalingPolicyConfiguration={\n",
    "        \"PredefinedMetricSpecification\": {\n",
    "            \"PredefinedMetricType\": \"SageMakerInferenceComponentInvocationsPerCopy\",\n",
    "        },\n",
    "        \"TargetValue\": 1,  # you need to adjust this value based on your use case\n",
    "        \"ScaleInCooldown\": 60,\n",
    "        \"ScaleOutCooldown\": 300,\n",
    "        \"DisableScaleIn\": False\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8d47b2-6216-4c9c-9c87-af2f9cda3bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "#aas_client.describe_scaling_policies(\n",
    "#    PolicyNames=[endpoint_name],\n",
    "#    ServiceNamespace=service_namespace,\n",
    "#    ResourceId=resource_id,\n",
    "#    ScalableDimension=scalable_dimension,\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a270428b-674b-4aaf-aa21-0110211b44ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Initial state\n",
    "#\n",
    "endpoint_desc = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "print(f\"EndpointStatus: {endpoint_desc['EndpointStatus']}\")\n",
    "print(f\"\\tCurrentInstanceCount: {endpoint_desc['ProductionVariants'][0]['CurrentInstanceCount']}\")\n",
    "print(f\"\\tDesiredInstanceCount: {endpoint_desc['ProductionVariants'][0]['DesiredInstanceCount']}\")\n",
    "\n",
    "ic_desc = sm_client.describe_inference_component(InferenceComponentName=inference_component_name)\n",
    "print(f\"InferenceComponentStatus: {ic_desc['InferenceComponentStatus']}\")\n",
    "print(f\"\\tCurrentCopyCount: {ic_desc['RuntimeConfig']['CurrentCopyCount']}\")\n",
    "print(f\"\\tDesiredCopyCount: {ic_desc['RuntimeConfig']['DesiredCopyCount']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef733e4a-7232-4e20-b3fc-8ec55fe9b469",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Test the timing only\n",
    "#\n",
    "#sm_client.update_inference_component(\n",
    "#    InferenceComponentName = inference_component_name,\n",
    "#    RuntimeConfig = {\n",
    "#        'CopyCount': 6\n",
    "#    }\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f79cc4-82e7-46f2-a943-f3e872e12984",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet locust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dd9bcb-9baf-4513-87e1-735143dea700",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat distributed.sh #adjust users and workers to increase traffic, users are a multiple of the workers in locust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14442648-b7a3-4f4e-83a2-18a1b77346ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# We recommend you run this command in a terminal (it generates a lot of output)\n",
    "#\n",
    "#%%bash -s \"$endpoint_name/$inference_component_name\"\n",
    "#./distributed.sh $1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d86a31c-5534-415a-b46d-5043fb7fe47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "# define some helper functions\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "\n",
    "@dataclass\n",
    "class AutoscalingStatus:\n",
    "    status_name: str  # endpoint status or inference component status\n",
    "    start_time: datetime  # when was the status changed\n",
    "    current_instance_count: int\n",
    "    desired_instance_count: int\n",
    "    current_copy_count: int\n",
    "    desired_copy_count: int"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f526e6-ff86-4b6a-bb71-75b77988f58e",
   "metadata": {},
   "source": [
    "Helper code to illustrate scaling out and scaling in timings.\n",
    "Stop the cell execution when done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3c79ae-05a5-4323-ab98-792d8d26e791",
   "metadata": {},
   "outputs": [],
   "source": [
    "statuses = []\n",
    "\n",
    "while True:\n",
    "    endpoint_desc = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = endpoint_desc['EndpointStatus']\n",
    "    current_instance_count = endpoint_desc['ProductionVariants'][0]['CurrentInstanceCount']\n",
    "    desired_instance_count = endpoint_desc['ProductionVariants'][0]['DesiredInstanceCount']\n",
    "    ic_desc = sm_client.describe_inference_component(InferenceComponentName=inference_component_name)\n",
    "    ic_status = ic_desc['InferenceComponentStatus']\n",
    "    current_copy_count = ic_desc['RuntimeConfig']['CurrentCopyCount']\n",
    "    desired_copy_count = ic_desc['RuntimeConfig']['DesiredCopyCount']\n",
    "    status_name = f\"{status}_{ic_status}\"\n",
    "    if not statuses or statuses[-1].status_name != status_name:\n",
    "        statuses.append(AutoscalingStatus(\n",
    "            status_name=status_name,\n",
    "            start_time=datetime.utcnow(),\n",
    "            current_instance_count=current_instance_count,\n",
    "            desired_instance_count=desired_instance_count,\n",
    "            current_copy_count=current_copy_count,\n",
    "            desired_copy_count=desired_copy_count,\n",
    "        ))\n",
    "        print(statuses[-1])\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228deadb-92df-4d85-9be3-8c0c39ff1a75",
   "metadata": {},
   "source": [
    "## Step 5: Autoscaling cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52328dbe-4ade-40fe-95ca-5fbd85d717b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "aas_client.delete_scaling_policy(\n",
    "    PolicyName=endpoint_name,\n",
    "    ServiceNamespace=service_namespace,\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=scalable_dimension,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71fddf0-505f-4fe5-b588-aa845c824b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "aas_client.deregister_scalable_target(\n",
    "    ServiceNamespace=service_namespace,\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=scalable_dimension,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cd9042",
   "metadata": {},
   "source": [
    "## Step 6: Clean up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b1abd0-eb91-4717-a116-a87f81654fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.delete_inference_component(inference_component_name, wait = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d674b41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sess.delete_endpoint(endpoint_name)\n",
    "sess.delete_endpoint_config(endpoint_config_name)\n",
    "sess.delete_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee9f6c1-24ab-4654-9a11-bef835fc1641",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Helper code - find my IP to use in locust_script.py (localhost does not work)\n",
    "#\n",
    "\n",
    "#import socket\n",
    "#s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "#s.connect((\"8.8.8.8\", 80))\n",
    "#print(s.getsockname()[0])\n",
    "#s.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0c773d-5631-4a0e-8156-82e860dc5892",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
