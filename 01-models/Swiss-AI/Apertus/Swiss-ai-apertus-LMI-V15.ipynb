{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f4cae57",
   "metadata": {},
   "source": [
    "\n",
    "# Deploying Swiss LLM Apertus on SageMaker with LMI v15 powered by vLLM\n",
    "\n",
    "This notebook demonstrates deploying and running inference with the Apertus model. We will cover \n",
    "\n",
    "1. Installing SageMaker python SDK, Setting up SageMaker resources and permissions\n",
    "2. Deploying the model using SageMaker LMI (Large Model Inference Container powered by Vllm)\n",
    "3. Invoking the model using streaming responses\n",
    "\n",
    "## Environment Setup\n",
    "\n",
    "First, we'll install the SageMaker SDK to ensure compatibility with the latest features, particularly those needed for large language model deployment and streaming inference.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12e49ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install sagemaker --upgrade --quiet --no-warn-conflicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446b6729-c48f-432e-a3f1-52cd5b60ca9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_mode = False  # if you have a local GPU you can also run the model locally using SageMaker SDK, e.g. for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4320f48a-5d13-4bdc-a07f-73c3c851b646",
   "metadata": {},
   "outputs": [],
   "source": [
    "if local_mode:\n",
    "    %pip install sagemaker[local] --upgrade --quiet --no-warn-conflicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bca4505-aa3b-477b-9b90-aa3ef8f26576",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker import Model, Session, get_execution_role \n",
    "from sagemaker.utils import name_from_base\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "role = get_execution_role()  # execution role for the endpoint\n",
    "\n",
    "if local_mode:\n",
    "    from sagemaker.local import entities, LocalSession\n",
    "\n",
    "    # Extend LocalMode’s health-check timeout to 15 minutes\n",
    "    entities.HEALTH_CHECK_TIMEOUT_LIMIT = 15 * 60  # seconds\n",
    "\n",
    "    sess = LocalSession()\n",
    "    sess.config = {\"local\": {\"local_code\": True}}\n",
    "else:\n",
    "    sess = Session() # sagemaker session for interacting with different AWS APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1feb6e81",
   "metadata": {},
   "source": [
    "## Configure Model Container and Instance\n",
    "\n",
    "For deploying Apertus, we'll use:\n",
    "- **LMI (Deep Java Library) Inference Container with vLLM** : A container optimized for large language model inference\n",
    "- **G6 Instance**: AWS's GPU instance type optimized for large model inference\n",
    "\n",
    "Key configurations:\n",
    "- The container URI points to the DJL inference container in ECR (Elastic Container Registry)\n",
    "- We use `ml.g6.48xlarge` instances which offer:\n",
    "  - 8 NVIDIA L4 GPUs with 192 GB GPU memory\n",
    "  - 768 GB of memory\n",
    "  - High network bandwidth for optimal inference performance\n",
    "\n",
    "> **Note**: The region in the container URI should match your AWS region. REPLACE `eu-central-2` with your region if different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6a91b2-29c3-4056-944f-130759ec5f91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define region where you have capacity\n",
    "REGION = \"eu-central-2\"\n",
    "INSTANCE_TYPE = (\n",
    "    \"local_gpu\" if local_mode else \"ml.g6.48xlarge\"\n",
    ")  # Alternative: \"ml.p5e.48xlarge\" or \"ml.p4d.24xlarge\"\n",
    "\n",
    "# Select the latest container. Check the link for the latest available version https://github.com/aws/deep-learning-containers/blob/master/available_images.md#large-model-inference-containers\n",
    "CONTAINER_VERSION = \"0.33.0-lmi15.0.0-cu128\"\n",
    "\n",
    "# Construct container URI\n",
    "if REGION == \"eu-central-2\":\n",
    "    container_account = 380420809688\n",
    "else:\n",
    "    container_account = 763104351884\n",
    "\n",
    "container_uri = f\"{container_account}.dkr.ecr.{REGION}.amazonaws.com/djl-inference:{CONTAINER_VERSION}\"\n",
    "\n",
    "\n",
    "# Validate region and print configuration\n",
    "if REGION != sess.boto_region_name:\n",
    "    print(\n",
    "        f\"⚠️ Warning: Container region ({REGION}) differs from session region ({sess.boto_region_name})\"\n",
    "    )\n",
    "else:\n",
    "    print(f\"✅ Region validation passed: {REGION}\")\n",
    "\n",
    "print(f\"📦 Container URI: {container_uri}\")\n",
    "print(f\"🖥️ Instance Type: {INSTANCE_TYPE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dfeb11-df3b-4b21-bb96-21c2390fad60",
   "metadata": {},
   "source": [
    "## Create SageMaker Model\n",
    "\n",
    "Now we'll create a SageMaker Model object that combines our:\n",
    "- vllm env variables\n",
    "- Container image (LMI)\n",
    "- Model artifacts (configuration files)\n",
    "- IAM role (for permissions)\n",
    "\n",
    "This step defines the model configuration but doesn't deploy it yet. The Model object represents the combination of:\n",
    "\n",
    "1. **Container Image** (`image_uri`): DJL Inference optimized for LLMs\n",
    "2. **Env Variables** (`env`): Our variables for the model server\n",
    "3. **IAM Role** (`role`): Permissions for model execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a55d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2a02ee-90b1-431a-a8ef-5d8c0ec6823e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the model locally first\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "print(\"📥 Downloading model locally...\")\n",
    "model_name = \"Saesara/swissai\"\n",
    "local_model_path = \"./apertus\"\n",
    "\n",
    "try:\n",
    "    model_path = snapshot_download(\n",
    "        repo_id=model_name, local_dir=local_model_path, local_files_only=False\n",
    "    )\n",
    "    print(f\"✅ Model downloaded to: {model_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error downloading model: {e}\")\n",
    "    # Fallback: you can manually download the model or use a different approach\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8371045b",
   "metadata": {},
   "source": [
    "> **Note**: Apertus is implemented in transformers v4.56.0. At the time of writing VLLM has not yet released a new version to PyPI containing the Apertus implementation which is why we are installing a VLLM nightly release. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b406b26d-5a1c-4a73-a336-c39c6bb04095",
   "metadata": {},
   "outputs": [],
   "source": [
    "requirements = \"\"\"git+https://github.com/huggingface/transformers.git@v4.56.0\n",
    "https://vllm-wheels.s3.us-west-2.amazonaws.com/1cf3753b901ba874a830c19555bb31fe37f91231/vllm-1.0.0.dev-cp38-abi3-manylinux1_x86_64.whl\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb11867-8b40-41fa-9611-4652c2f72e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store requirements >requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba534968",
   "metadata": {},
   "source": [
    "We need to patch the VLLMHandler in DeepJavaLibrary Serving to not pass parameters that are deprecated in the latest VLLM version.\n",
    "\n",
    "For this we create our own custom inference Python script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2885907b",
   "metadata": {},
   "outputs": [],
   "source": [
    "patched_vllm_service = \"\"\"#!/usr/bin/env python\n",
    "\n",
    "import logging\n",
    "from typing import Optional, Union, AsyncGenerator\n",
    "\n",
    "# Patch CLI args if needed\n",
    "from djl_python.properties_manager.vllm_rb_properties import VllmRbProperties\n",
    "_orig_args = VllmRbProperties.generate_vllm_engine_arg_dict\n",
    "def _patched_args(self, passthrough_args):\n",
    "    args = _orig_args(self, passthrough_args)\n",
    "    for key in (\"device\", \"use_v2_block_manager\"):\n",
    "        args.pop(key, None)\n",
    "    return args\n",
    "VllmRbProperties.generate_vllm_engine_arg_dict = _patched_args\n",
    "\n",
    "# Custom initialize\n",
    "from djl_python.lmi_vllm.vllm_async_service import VLLMHandler\n",
    "from vllm import AsyncLLMEngine\n",
    "from djl_python.properties_manager.hf_properties import HuggingFaceProperties\n",
    "from vllm.entrypoints.openai.serving_models import OpenAIServingModels, BaseModelPath\n",
    "from vllm.entrypoints.openai.serving_completion import OpenAIServingCompletion\n",
    "from vllm.entrypoints.openai.serving_chat import OpenAIServingChat\n",
    "\n",
    "async def _custom_initialize(self, properties: dict):\n",
    "    # 1. Load HF and vLLM properties\n",
    "    self.hf_configs = HuggingFaceProperties(**properties)\n",
    "    self.vllm_properties = VllmRbProperties(**properties)\n",
    "\n",
    "    # 2. Build AsyncLLMEngine\n",
    "    self.vllm_engine_args = self.vllm_properties.get_engine_args(async_engine=True)\n",
    "    self.vllm_engine = AsyncLLMEngine.from_engine_args(self.vllm_engine_args)\n",
    "    self.tokenizer = await self.vllm_engine.get_tokenizer()\n",
    "    model_config = await self.vllm_engine.get_model_config()\n",
    "\n",
    "    # 3. Prepare model registry and completion service\n",
    "    model_names = self.vllm_engine_args.served_model_name or \"lmi\"\n",
    "    if not isinstance(model_names, list):\n",
    "        model_names = [model_names]\n",
    "    # Users can provide multiple names that refer to the same model\n",
    "    base_model_paths = [\n",
    "        BaseModelPath(model_name, self.vllm_engine_args.model)\n",
    "        for model_name in model_names\n",
    "    ]\n",
    "    \n",
    "    # Use the first model name as default.\n",
    "    # This is needed to be backwards compatible since LMI never required the model name in payload\n",
    "    self.model_name = model_names[0]\n",
    "\n",
    "    self.model_registry = OpenAIServingModels(\n",
    "        self.vllm_engine,\n",
    "        model_config,\n",
    "        base_model_paths,\n",
    "    )\n",
    "    self.completion_service = OpenAIServingCompletion(\n",
    "        self.vllm_engine,\n",
    "        model_config,\n",
    "        self.model_registry,\n",
    "        request_logger=None,\n",
    "    )\n",
    "\n",
    "    \n",
    "    \n",
    "    # Construct OpenAIServingChat with only supported kwargs\n",
    "    chat_kwargs = {\n",
    "        'request_logger': None,\n",
    "        'chat_template': getattr(self.vllm_properties, 'chat_template', None),\n",
    "        'chat_template_content_format': getattr(\n",
    "            self.vllm_properties, 'chat_template_content_format', None\n",
    "        ),\n",
    "        # do NOT include reasoning_parser or enable_reasoning\n",
    "        'enable_auto_tools': getattr(self.vllm_properties, 'enable_auto_tool_choice', False),\n",
    "        'tool_parser': getattr(self.vllm_properties, 'tool_call_parser', None),\n",
    "        'reasoning_parser' : getattr(self.vllm_properties, 'reasoning_parser', \"\"),\n",
    "    }\n",
    "\n",
    "    logging.getLogger(__name__).info(\"Initializing OpenAIServingChat without reasoning flags\")\n",
    "    self.chat_completion_service = OpenAIServingChat(\n",
    "        self.vllm_engine,\n",
    "        model_config,\n",
    "        self.model_registry,\n",
    "        \"assistant\",  # response_role\n",
    "        **chat_kwargs,\n",
    "    )\n",
    "\n",
    "    self.initialized = True\n",
    "\n",
    "VLLMHandler.initialize = _custom_initialize\n",
    "\n",
    "# Delegate to the existing handle function\n",
    "from djl_python.lmi_vllm.vllm_async_service import handle\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4797009b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store patched_vllm_service >entrypoint.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16d58ac",
   "metadata": {},
   "source": [
    "We combine the requirements file with the model weights into a single archive to upload to an Amazon S3 bucket. The Amazon SageMaker inference enpoint will download the archive from the Amazon S3 bucket and extract it into the inference container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cd2331-33c6-4822-8508-917daddef0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "mv requirements.txt apertus/\n",
    "mv entrypoint.py apertus/\n",
    "tar czvf apertus.tar.gz apertus/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73775db7",
   "metadata": {},
   "source": [
    "Replace `<your-bucket-name>` with your own Amazon S3 bucket name in the same region in which you plan to deploy the endpoint in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ba416c-354e-4762-8ce2-6c786f7d8e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload model artifacts to S3\n",
    "bucket = \"apertus-checkpoints\" # REPLACE with the name of you Amazon S3 bucket\n",
    "\n",
    "if not bucket or bucket == \"<your-bucket-name>\": # DO NOT replace this string\n",
    "    raise ValueError(\"❌ Please set a valid S3 bucket name. Replace bucket='<your-bucket-name>'.\")\n",
    "s3_code_prefix = \"apertus-lmi\"\n",
    "code_artifact = sess.upload_data(\"apertus.tar.gz\", bucket, s3_code_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca723ce-e5f7-42fc-ac31-5e16c2393676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated vLLM configuration to use local model\n",
    "vllm_config = {\n",
    "    \"OPTION_MAX_MODEL_LEN\": \"4096\",\n",
    "    \"OPTION_MAX_ROLLING_BATCH_SIZE\": \"8\",\n",
    "    \"OPTION_MODEL_LOADING_TIMEOUT\": \"1500\",\n",
    "    \"SERVING_FAIL_FAST\": \"true\",\n",
    "    \"OPTION_ROLLING_BATCH\": \"disable\",\n",
    "    \"OPTION_ASYNC_MODE\": \"true\",\n",
    "    \"OPTION_ENTRYPOINT\": \"entrypoint\",\n",
    "    \"OPTION_TRUST_REMOTE_CODE\": \"true\",\n",
    "    \"OPTION_MODEL_ID\": \"./\",  \n",
    "    \"VLLM_USE_PRECOMPILED\": \"1\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bada9f4",
   "metadata": {},
   "source": [
    "The Model object combines all the information on how to deploy the model to an endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ecd197-91b2-4f36-bcec-2afb8c7c370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(\n",
    "    image_uri=container_uri,\n",
    "    role=role,\n",
    "    model_data=code_artifact,\n",
    "    sagemaker_session=sess,\n",
    "    env=vllm_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba06783-5e04-4a7e-9dc9-0346535e85bc",
   "metadata": {},
   "source": [
    "## Deploy Model to SageMaker Endpoint\n",
    "\n",
    "Now we'll deploy our model to a SageMaker endpoint for real-time inference. This is a significant step that:\n",
    "1. Provisions the specified compute resources (G6 instance)\n",
    "2. Deploys the model container\n",
    "3. Sets up the endpoint for API access\n",
    "\n",
    "### Deployment Configuration\n",
    "- **Instance Count**: 1 instance for single-node deployment\n",
    "- **Instance Type**: `ml.g6.48xlarge` for high-performance inference\n",
    "- **Health Check Timeout**: 1800 seconds \n",
    "  - Extended timeout needed for large model loading\n",
    "  - Includes time for container setup and model initialization\n",
    "\n",
    "> ⚠️ **Important**: \n",
    "> - Deployment can take upto 15 minutes\n",
    "> - Monitor the endpoint status in SageMaker Console and CloudWatch logs for progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c369f52-af94-4710-94f9-2d28bfd4d14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if local_mode:\n",
    "    # To see progress\n",
    "    !docker pull $container_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2741a6-b7e1-4b75-8cb2-95555fd27968",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = name_from_base(\"Apertus\")\n",
    "\n",
    "print(endpoint_name)\n",
    "\n",
    "try:\n",
    "    model.deploy(\n",
    "        initial_instance_count=1,\n",
    "        instance_type=INSTANCE_TYPE,\n",
    "        endpoint_name=endpoint_name,\n",
    "        container_startup_health_check_timeout=1800,\n",
    "    )\n",
    "    print(f\"\\n✅ Endpoint '{endpoint_name}' deployed successfully\")\n",
    "except ClientError as e:\n",
    "    error_code = e.response['Error']['Code']\n",
    "    if error_code == 'ResourceLimitExceeded':\n",
    "        print(\n",
    "            \"❌ Resource limit exceeded.\"\n",
    "            + f\"Did you request the necessary Service Quotas for {INSTANCE_TYPE} in {REGION}?\"\n",
    "            + \"See also https://repost.aws/knowledge-center/sagemaker-resource-limit-exceeded-error\"\n",
    "        )\n",
    "    elif error_code == 'InsufficientInstanceCapacity':\n",
    "        print(\n",
    "            \"❌ Insufficient instance capacity. Try a different AZ or instance type\"\n",
    "            + \"See also https://repost.aws/knowledge-center/sagemaker-insufficient-capacity-error\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"❌ Deployment failed: {e}\")\n",
    "    raise e\n",
    "except Exception as e:\n",
    "    print(f\"❌ Unexpected deployment error: {e}\")\n",
    "    print(\"💡 Check CloudWatch logs for detailed error information\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3e6565-53d5-4595-a0e1-4ed9ac799a71",
   "metadata": {},
   "source": [
    "## Running Inference requests to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1799ae2f",
   "metadata": {},
   "source": [
    "Once you have deployed the model to the Amazon SageMaker inference endpoint you can invoke it. Replace `<your_endpoint_name>` below with the name of your SageMaker inference endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a02b4d-1e4e-4fe6-b662-9e25e1c0f6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Invoke model with response streaming\n",
    "from json import dumps as json_dumps, loads as json_loads, JSONDecodeError\n",
    "from boto3 import client\n",
    "from time import time\n",
    "\n",
    "# Create SageMaker Runtime client\n",
    "smr_client = client(\"sagemaker-runtime\")\n",
    "\n",
    "endpoint_name = \"<your_endpoint_name>\" # REPLACE with your endpoint\n",
    "\n",
    "print(f\"Endpoint name: {endpoint_name}\")\n",
    "if endpoint_name == \"<your_endpoint_name>\": # DO NOT replace this string\n",
    "    raise ValueError(\"❌ Please set a valid endpoint name\")\n",
    "\n",
    "# Invoke with messages format\n",
    "body = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Name popular places to visit in London?\"}\n",
    "    ],\n",
    "    \"temperature\": 0.9,\n",
    "    \"max_tokens\": 256,\n",
    "    \"stream\": True,\n",
    "}\n",
    "\n",
    "start_time = time()\n",
    "first_token_received = False\n",
    "ttft = None\n",
    "token_count = 0\n",
    "full_response = \"\"\n",
    "\n",
    "print(f\"Prompt: {body['messages'][0]['content']}\\n\")\n",
    "print(\"Response:\", end=\" \", flush=True)\n",
    "\n",
    "# Invoke endpoint with streaming\n",
    "\n",
    "try:\n",
    "    resp = smr_client.invoke_endpoint_with_response_stream(\n",
    "        EndpointName=endpoint_name,\n",
    "        Body=json_dumps(body),\n",
    "        ContentType=\"application/json\",\n",
    "    )\n",
    "except ClientError as e:\n",
    "    error_code = e.response['Error']['Code']\n",
    "    if error_code == 'ValidationException':\n",
    "        print(\"❌ Validation Exception. Invalid request format or parameters\")\n",
    "    elif error_code == 'ModelError':\n",
    "        print(\"❌ Model error. Check model logs\")\n",
    "    else:\n",
    "        print(f\"❌ Inference failed: {e}\")\n",
    "    raise e\n",
    "except Exception as e:\n",
    "    print(f\"❌ Unexpected inference error: {e}\")\n",
    "    raise e\n",
    "\n",
    "# Process streaming response\n",
    "for event in resp[\"Body\"]:\n",
    "    if \"PayloadPart\" in event:\n",
    "        payload = event[\"PayloadPart\"][\"Bytes\"].decode()\n",
    "\n",
    "        try:\n",
    "\n",
    "            if payload.startswith(\"data: \"):\n",
    "                data = json_loads(payload[6:])  # Skip \"data: \" prefix\n",
    "            else:\n",
    "                data = json_loads(payload)\n",
    "\n",
    "            token_count += 1\n",
    "            if not first_token_received:\n",
    "                ttft = time() - start_time\n",
    "                first_token_received = True\n",
    "\n",
    "            # Handle different streaming response formats\n",
    "            if \"choices\" in data and len(data[\"choices\"]) > 0:\n",
    "                # Messages-compatible format\n",
    "                if (\n",
    "                    \"delta\" in data[\"choices\"][0]\n",
    "                    and \"content\" in data[\"choices\"][0][\"delta\"]\n",
    "                ):\n",
    "                    token_text = data[\"choices\"][0][\"delta\"][\"content\"]\n",
    "                    full_response += token_text\n",
    "                    print(token_text, end=\"\", flush=True)\n",
    "            elif \"token\" in data and \"text\" in data[\"token\"]:\n",
    "                # TGI format\n",
    "                token_text = data[\"token\"][\"text\"]\n",
    "                full_response += token_text\n",
    "                print(token_text, end=\"\", flush=True)\n",
    "\n",
    "        except JSONDecodeError:\n",
    "            # Skip invalid JSON\n",
    "            continue\n",
    "\n",
    "end_time = time()\n",
    "total_latency = end_time - start_time\n",
    "\n",
    "print(\"\\n\\nMetrics:\")\n",
    "if ttft:\n",
    "    print(\n",
    "        f\"Time to First Token (TTFT): {ttft:.2f} seconds\"\n",
    "    )\n",
    "else:\n",
    "    print('No tokens received')\n",
    "print(f\"Total Tokens Generated: {token_count}\")\n",
    "print(f\"Total Latency: {total_latency:.2f} seconds\")\n",
    "# print(f\"\\nFull Response:\\n{full_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae097cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Option 2: Invoke without streaming\n",
    "# from json import dumps as json_dumps, loads as json_loads, JSONDecodeError\n",
    "# from boto3 import client\n",
    "\n",
    "\n",
    "\n",
    "# # Create SageMaker Runtime client for invocation\n",
    "# smr_client = client('sagemaker-runtime')\n",
    "\n",
    "# endpoint_name = \"<your_endpoint_name>\" # REPLACE with your endpoint\n",
    "\n",
    "# print(f\"Endpoint name: {endpoint_name}\")\n",
    "# if endpoint_name == \"<your_endpoint_name>\": # DO NOT replace this string\n",
    "#     raise ValueError(\"❌ Please set a valid endpoint name\")\n",
    "\n",
    "# print(f\"Prompt: {body['messages'][0]['content']}\\n\")\n",
    "\n",
    "# # Invoke with messages format\n",
    "# body = {\n",
    "#     \"messages\": [\n",
    "#         {\"role\": \"user\", \"content\": \"Name popular places to visit in London?\"}\n",
    "#     ],\n",
    "#     \"temperature\": 0.9,\n",
    "#     \"max_tokens\": 256,\n",
    "#     \"stream\": False,\n",
    "# }\n",
    "\n",
    "\n",
    "# try:\n",
    "#     # Non-streaming invocation\n",
    "#     response = smr_client.invoke_endpoint(\n",
    "#         EndpointName=endpoint_name,\n",
    "#         ContentType='application/json',\n",
    "#         Body=json_dumps(body)\n",
    "#     )\n",
    "# except ClientError as e:\n",
    "#     error_code = e.response['Error']['Code']\n",
    "#     if error_code == 'ValidationException':\n",
    "#         print(\"❌ Validation Exception. Invalid request format or parameters\")\n",
    "#     elif error_code == 'ModelError':\n",
    "#         print(\"❌ Model error. Check model logs\")\n",
    "#     else:\n",
    "#         print(f\"❌ Inference failed: {e}\")\n",
    "#     raise e\n",
    "# except Exception as e:\n",
    "#     print(f\"❌ Unexpected inference error: {e}\")\n",
    "#     raise e\n",
    "\n",
    "\n",
    "# result = json_loads(response['Body'].read().decode())\n",
    "# print(result[\"choices\"][0][\"message\"][\"content\"])\n",
    "# print(f\"\\nFull Response:\\n{result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce17e4f7",
   "metadata": {},
   "source": [
    "## Cleanup: Delete Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c37be7-fba1-4af0-a317-31436331a899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sagemaker import Session\n",
    "\n",
    "# # Initialize session\n",
    "# sess = Session()\n",
    "\n",
    "\n",
    "print(f\"Deleting SageMaker resources for endpoint: {endpoint_name}\")\n",
    "sess.delete_endpoint(endpoint_name)\n",
    "sess.delete_endpoint_config(endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd402b07",
   "metadata": {},
   "source": [
    "Remove the local artifacts which contain the model weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e9c846-fdc4-4167-af9d-67d7193d5115",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf apertus\n",
    "!rm apertus.tar.gz"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
