{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94d68fb3-00f2-447c-b5e0-28fa18562392",
   "metadata": {},
   "source": [
    "# Deploy the Qwen3-VL-2B-Instruct for inference using Amazon SageMakerAI\n",
    "**Recommended kernel(s):** This notebook can be run with any Amazon SageMaker Studio kernel.\n",
    "\n",
    "In this notebook, you will learn how to deploy the Qwen3-VL-2B-Instruct model (HuggingFace model ID: [Qwen/Qwen3-VL-2B-Instruct](https://huggingface.co/Qwen/Qwen3-VL-2B-Instruct)) using Amazon SageMaker AI. \n",
    "\n",
    "Let's install or upgrade these dependencies using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed1c92f-9fbc-47c9-b940-7dd07f962ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -Uq huggingface==4.49 sagemaker transformers==4.57.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc56cad7-ea97-43c4-835d-a56729549023",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c128cf97-3f1f-4176-87b4-1193cd9e9c0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T19:19:08.231499Z",
     "iopub.status.busy": "2025-10-28T19:19:08.231306Z",
     "iopub.status.idle": "2025-10-28T19:19:09.646052Z",
     "shell.execute_reply": "2025-10-28T19:19:09.645642Z",
     "shell.execute_reply.started": "2025-10-28T19:19:08.231484Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "2.253.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import sagemaker\n",
    "import boto3\n",
    "import logging\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "import tarfile\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "print(sagemaker.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bba6d4bf-5df9-4e3e-8a9e-a03897e0cb0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T19:19:09.646609Z",
     "iopub.status.busy": "2025-10-28T19:19:09.646472Z",
     "iopub.status.idle": "2025-10-28T19:19:10.114037Z",
     "shell.execute_reply": "2025-10-28T19:19:10.113669Z",
     "shell.execute_reply.started": "2025-10-28T19:19:09.646594Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen-Qwen3-VL-2B-Instruct-endpoint-1761679149-892584\n",
      "Saving model artifacts to sagemaker-us-east-1-329542461890/models/Qwen_Qwen3-VL-2B-Instruct\n"
     ]
    }
   ],
   "source": [
    "session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "instance_type = \"ml.g5.4xlarge\"\n",
    "instance_count = 1\n",
    "\n",
    "model_id = \"Qwen/Qwen3-VL-2B-Instruct\"\n",
    "model_id_filesafe = model_id.replace(\"/\", \"_\").replace(\".\", \"_\")\n",
    "endpoint_name = f\"{model_id_filesafe.replace(\"_\", \"-\")}-endpoint-{str(datetime.datetime.now().timestamp()).replace(\".\", \"-\")}\"\n",
    "print(endpoint_name)\n",
    "\n",
    "image_uri = \"763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.34.0-lmi16.0.0-cu128-v1.2\"\n",
    "\n",
    "base_name = model_id.split('/')[-1].replace('.', '-').lower()\n",
    "model_lineage = model_id.split('/')[0]\n",
    "base_name\n",
    "\n",
    "bucket_name = session.default_bucket()\n",
    "default_prefix = session.default_bucket_prefix or f\"models/{model_id_filesafe}\"\n",
    "print(f\"Saving model artifacts to {bucket_name}/{default_prefix}\")\n",
    "\n",
    "os.makedirs(\"code\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7d4737-12ca-4cb7-aee4-06f7c0328b6d",
   "metadata": {},
   "source": [
    "## Local Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51db134d-19bb-4702-949e-cf8160fe917f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Qwen3VLForConditionalGeneration, AutoProcessor\n",
    "\n",
    "model = Qwen3VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen3-VL-2B-Instruct\",\n",
    "    dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"sdpa\"\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen3-VL-2B-Instruct\")\n",
    "messages = [\n",
    "    {\n",
    "        \"role\":\"user\",\n",
    "        \"content\":[\n",
    "            {\n",
    "                \"type\":\"image\",\n",
    "                \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n",
    "            },\n",
    "            {\n",
    "                \"type\":\"text\",\n",
    "                \"text\":\"Describe this image.\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "]\n",
    "\n",
    "inputs = processor.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs.pop(\"token_type_ids\", None)\n",
    "\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "generated_ids_trimmed = [\n",
    "            out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "       generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7245e684-aa6e-44ca-9451-10e0bee4b96e",
   "metadata": {},
   "source": [
    "## Create SageMaker Model\n",
    "Here we define the custom requirements and inference logic to be run by this model. We download the model assets from HuggingFace, zip them up and upload them to S3. We then deploy the model as a `HuggingFaceModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "410cb9a8-9fd0-408d-bebb-af8b8c09d398",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T19:19:13.048393Z",
     "iopub.status.busy": "2025-10-28T19:19:13.048051Z",
     "iopub.status.idle": "2025-10-28T19:19:13.051031Z",
     "shell.execute_reply": "2025-10-28T19:19:13.050649Z",
     "shell.execute_reply.started": "2025-10-28T19:19:13.048378Z"
    }
   },
   "outputs": [],
   "source": [
    "env = {\n",
    "    'HF_MODEL_ID': model_id,\n",
    "    'HF_TASK':'image-text-to-text',\n",
    "    'SM_NUM_GPUS': json.dumps(1),\n",
    "    'OPTION_TRUST_REMOTE_CODE': 'true',\n",
    "    'OPTION_MODEL_LOADING_TIMEOUT': '3600',\n",
    "    \"OPTION_ROLLING_BATCH\": \"disable\",\n",
    "    \"OPTION_TENSOR_PARALLEL_DEGREE\": \"1\",\n",
    "    \"OPTION_MAX_MODEL_LEN\": \"5000\",\n",
    "    \"OPTION_ASYNC_MODE\": \"true\",\n",
    "    \"OPTION_TRUST_REMOTE_CODE\": \"true\",\n",
    "    \"SERVING_FAIL_FAST\": \"true\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d441971-621c-4d52-a7db-d67020a8667e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T19:19:13.916283Z",
     "iopub.status.busy": "2025-10-28T19:19:13.915921Z",
     "iopub.status.idle": "2025-10-28T19:19:13.919378Z",
     "shell.execute_reply": "2025-10-28T19:19:13.919024Z",
     "shell.execute_reply.started": "2025-10-28T19:19:13.916253Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/requirements.txt\n",
    "transformers==4.57.0\n",
    "torch\n",
    "torchvision\n",
    "torchaudio\n",
    "pillow\n",
    "requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6468cba-5d10-484d-898a-19802612925c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T19:51:30.543812Z",
     "iopub.status.busy": "2025-10-28T19:51:30.543609Z",
     "iopub.status.idle": "2025-10-28T19:51:30.546908Z",
     "shell.execute_reply": "2025-10-28T19:51:30.546525Z",
     "shell.execute_reply.started": "2025-10-28T19:51:30.543795Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/inference.py\n",
    "# This code comes from HuggingFace\n",
    "# https://huggingface.co/Qwen/Qwen3-VL-2B-Instruct\n",
    "import logging\n",
    "import torch\n",
    "from transformers import Qwen3VLForConditionalGeneration, AutoProcessor\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "def model_fn(model_dir):\n",
    "\n",
    "    model = Qwen3VLForConditionalGeneration.from_pretrained(\n",
    "        model_dir,\n",
    "        dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        attn_implementation=\"sdpa\"\n",
    "    )\n",
    "\n",
    "    processor = AutoProcessor.from_pretrained(\n",
    "        model_dir,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    return {\"processor\": processor, \"model\": model}\n",
    "\n",
    "\n",
    "def predict_fn(data, model_obj):\n",
    "    processor = model_obj[\"processor\"]\n",
    "    model = model_obj[\"model\"]\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n",
    "                },\n",
    "                {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    inputs = processor.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_dict=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    inputs = inputs.to(model.device)\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ba9db6b-3a56-462a-a513-770ed7f4bbcc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T19:51:31.602686Z",
     "iopub.status.busy": "2025-10-28T19:51:31.602482Z",
     "iopub.status.idle": "2025-10-28T19:51:31.605398Z",
     "shell.execute_reply": "2025-10-28T19:51:31.604909Z",
     "shell.execute_reply.started": "2025-10-28T19:51:31.602672Z"
    }
   },
   "outputs": [],
   "source": [
    "def filter_function(tarinfo):\n",
    "    \"\"\"Filter function to exclude .cache files and directories\"\"\"\n",
    "    if '.cache' in tarinfo.name or '.gitattributes' in tarinfo.name:\n",
    "        return None\n",
    "    return tarinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae9b8cdf-ebf9-4d05-837d-8a3f412b73d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T19:51:33.065578Z",
     "iopub.status.busy": "2025-10-28T19:51:33.065191Z",
     "iopub.status.idle": "2025-10-28T19:57:37.284971Z",
     "shell.execute_reply": "2025-10-28T19:57:37.284528Z",
     "shell.execute_reply.started": "2025-10-28T19:51:33.065543Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dc899529b1b47008ef16bc6303b5b18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded to /home/sagemaker-user/sagemaker-genai-hosting-examples/01-models/Qwen3/Qwen3-VL/model\n",
      "Building gzipped tarball...\n",
      "Successfully tarred the ball.\n",
      "Uploading tarball to sagemaker-us-east-1-329542461890/models/Qwen_Qwen3-VL-2B-Instruct...\n",
      "Successfully uploaded, working directory cleaned\n"
     ]
    }
   ],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "key = f\"{default_prefix}/model.tar.gz\"\n",
    "force_rebuild_tarball = True\n",
    "\n",
    "if force_rebuild_tarball or not s3_client.head_object(Bucket=bucket_name, Key=key):\n",
    "    try:\n",
    "        model_path = snapshot_download(repo_id=model_id, local_dir=\"./model\")\n",
    "        print(f\"Successfully downloaded to {model_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download after retries: {str(e)}\")\n",
    "    \n",
    "    print(\"Building gzipped tarball...\")\n",
    "    with tarfile.open(\"./model.tar.gz\", \"w:gz\") as tar:\n",
    "        tar.add(model_path, arcname=\".\", filter=filter_function)\n",
    "        tar.add(\"./code\", filter=filter_function)\n",
    "    print(\"Successfully tarred the ball.\")\n",
    "    \n",
    "    print(f\"Uploading tarball to {bucket_name}/{default_prefix}...\")\n",
    "    s3_client.upload_file(\"./model.tar.gz\", bucket_name, f\"{default_prefix}/model.tar.gz\")\n",
    "    # shutil.rmtree(\"./model\")\n",
    "    # os.remove(\"./model.tar.gz\")\n",
    "    print(\"Successfully uploaded, working directory cleaned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40af4b17-30e9-4138-93f3-0589e4770815",
   "metadata": {},
   "source": [
    "## Deploy Model to SageMaker Endpoint\n",
    "\n",
    "Now we'll deploy our model to a SageMaker endpoint for real-time inference. This is a significant step that:\n",
    "1. Provisions the specified compute resources (G5 instance)\n",
    "2. Deploys the model container\n",
    "3. Sets up the endpoint for API access\n",
    "\n",
    "### Deployment Configuration\n",
    "- **Instance Count**: 1 instance for single-node deployment\n",
    "- **Instance Type**: `ml.g5.4xlarge` for high-performance inference\n",
    "\n",
    "> ⚠️ **Important**: \n",
    "> - Deployment can take up to 15 minutes\n",
    "> - Monitor the CloudWatch logs for progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7207d0d4-f2be-41c6-a989-3db36da256cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T19:57:37.288133Z",
     "iopub.status.busy": "2025-10-28T19:57:37.287995Z",
     "iopub.status.idle": "2025-10-28T20:10:17.947197Z",
     "shell.execute_reply": "2025-10-28T20:10:17.946723Z",
     "shell.execute_reply.started": "2025-10-28T19:57:37.288117Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------!"
     ]
    }
   ],
   "source": [
    "# Hub Model configuration. https://huggingface.co/models\n",
    "hub = {\n",
    "\t'HF_MODEL_ID':'Qwen/Qwen3-VL-2B-Instruct',\n",
    "\t'HF_TASK':'image-text-to-text'\n",
    "}\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    model_data=f\"s3://{bucket_name}/{default_prefix}/model.tar.gz\",\n",
    "\ttransformers_version='4.49.0',\n",
    "\tpytorch_version='2.6.0',\n",
    "\tpy_version='py312',\n",
    "\tenv=env,\n",
    "\trole=role, \n",
    "    entry_point=\"inference.py\",\n",
    "    enable_network_isolation=False\n",
    ")\n",
    "\n",
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "\tinitial_instance_count=1, # number of instances\n",
    "\tinstance_type='ml.g5.4xlarge'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ef9603-a16d-4195-9702-b920762e2e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DJL Serving\n",
    "# UNDER CONSTRUCTION\n",
    "\n",
    "# model = HuggingFaceModel(\n",
    "#     model_data=f\"s3://{bucket_name}/{default_prefix}/model.tar.gz\",\n",
    "#     image_uri=image_uri,\n",
    "#     env=env,\n",
    "#     role=role,\n",
    "#     entry_point=\"inference.py\",\n",
    "#     enable_network_isolation=False\n",
    "# )\n",
    "\n",
    "# predictor = model.deploy(\n",
    "#     initial_instance_count=instance_count,\n",
    "#     instance_type=instance_type,\n",
    "#     endpoint_name=endpoint_name\n",
    "# )\n",
    "\n",
    "# predictor.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b85c32-d4df-45f1-84f4-86a296ad1c0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T19:14:57.135928Z",
     "iopub.status.busy": "2025-09-15T19:14:57.135661Z",
     "iopub.status.idle": "2025-09-15T19:14:57.139468Z",
     "shell.execute_reply": "2025-09-15T19:14:57.138566Z",
     "shell.execute_reply.started": "2025-09-15T19:14:57.135907Z"
    }
   },
   "source": [
    "# Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcef37c7-6b47-4c3b-b6b7-b266245b492e",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_model.delete_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
