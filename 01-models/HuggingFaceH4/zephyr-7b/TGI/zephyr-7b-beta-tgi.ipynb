{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eba763b2-d0f2-4e99-b375-8b4873116bcf",
   "metadata": {},
   "source": [
    "# Deploy HuggingFaceH4/zephyr-7b-beta on Amazon SageMaker using Hugging Face Text Generation Inference (TGI) container\n",
    "\n",
    "## Resources\n",
    "- [Zephyr-7B-beta model card](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta)\n",
    "- [TGI documentation](https://huggingface.co/docs/text-generation-inference/en/index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944fbc1a-b23a-43c5-af27-76fb7fdaa25f",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dd04a8-6181-441e-b557-ee996281e3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4702f11-0670-4d8b-9c62-f325d77be6f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import json\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd76e135-46bb-4d47-a546-5de73abb5ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "account_id = sess.account_id()  # account_id of the current SageMaker Studio environment\n",
    "\n",
    "sm_client = boto3.client(\"sagemaker\")  # client to intreract with SageMaker\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")  # client to intreract with SageMaker Endpoints\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n",
    "print(f\"boto3 version: {boto3.__version__}\")\n",
    "print(f\"sagemaker version: {sagemaker.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a74f0e5-966c-4620-8bb9-a33e84c9ceeb",
   "metadata": {},
   "source": [
    "## Step 2: Endpoint Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8b2c21-5edc-445d-938f-634e704c3173",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "\n",
    "# retrieve the llm image uri\n",
    "latest_version = \"1.4.2\" \n",
    "llm_image = get_huggingface_llm_image_uri(\"huggingface\", version=latest_version)\n",
    "print(f\"llm image uri: {llm_image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f4cab6-c228-4fa3-a7d0-67feadcff891",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "# sagemaker config\n",
    "instance_type = \"ml.g5.2xlarge\"\n",
    "number_of_gpu = 1 # deploying on g5.2xlarge that has only one GPU\n",
    "model_name = \"Zephyr-7b-beta\"\n",
    "\n",
    "# TGI config\n",
    "config = {\n",
    "    'HF_MODEL_ID': \"HuggingFaceH4/zephyr-7b-beta\", # model_id from hf.co/models\n",
    "    'SM_NUM_GPUS': json.dumps(number_of_gpu), # Number of GPU used per replica\n",
    "    'MAX_INPUT_LENGTH': json.dumps(1024),  # Max length of input text\n",
    "    'MAX_TOTAL_TOKENS': json.dumps(2048),  # Max length of the generation (including input text)\n",
    "}\n",
    "\n",
    "# create HuggingFaceModel\n",
    "llm_model = HuggingFaceModel(\n",
    "    role = role,\n",
    "    image_uri = llm_image,\n",
    "    env = config,\n",
    "    name = model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9c1edd-768a-4fa5-abad-91807651512e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Deploy model to an endpoint\n",
    "# https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.deploy\n",
    "endpoint_name = f\"{model_name}-TGI-EP\"\n",
    "health_check_timeout = 600\n",
    "\n",
    "llm = llm_model.deploy(\n",
    "    initial_instance_count = 1,\n",
    "    instance_type = instance_type,\n",
    "    endpoint_name=endpoint_name,\n",
    "    container_startup_health_check_timeout = health_check_timeout, # timeout for loading the model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe50cfa-65fa-4fa3-b05b-0c1465a9ca09",
   "metadata": {},
   "source": [
    "## Step 3: Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64184c88-ad55-461b-9509-752eaee1a2f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define payload\n",
    "prompt = \"\"\"You are an helpful Assistant, called Zephyr. Knowing everyting about AWS.\n",
    "\n",
    "User: Can you tell me something about Amazon SageMaker?\n",
    "Zephyr:\"\"\"\n",
    "\n",
    "# hyperparameters for llm (remove \"\\nUser:\" from stop conditions)\n",
    "payload = {\n",
    "  \"inputs\": prompt,\n",
    "  \"parameters\": {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.9,\n",
    "    \"temperature\": 0.8,\n",
    "    \"max_new_tokens\": 512,\n",
    "    \"repetition_penalty\": 1.03,\n",
    "    \"stop\": [\"<|endoftext|>\",\"</s>\"]\n",
    "  }\n",
    "}\n",
    "\n",
    "# send request to endpoint\n",
    "response = llm.predict(payload)\n",
    "\n",
    "# print assistant respond\n",
    "assistant = response[0][\"generated_text\"][len(prompt):]\n",
    "print(assistant)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af81f0f4-4c11-4337-8ff5-33e23d065ab0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Step 3.1: Run inference (streaming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de5d1d6-8d6b-499f-9137-6e117e6ff4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Streaming\n",
    "#\n",
    "import io\n",
    "\n",
    "class LineIterator:\n",
    "    \"\"\"\n",
    "    A helper class for parsing the byte stream input. \n",
    "    \n",
    "    The output of the model will be in the following format:\n",
    "    ```\n",
    "    b'{\"outputs\": [\" a\"]}\\n'\n",
    "    b'{\"outputs\": [\" challenging\"]}\\n'\n",
    "    b'{\"outputs\": [\" problem\"]}\\n'\n",
    "    ...\n",
    "    ```\n",
    "    \n",
    "    While usually each PayloadPart event from the event stream will contain a byte array \n",
    "    with a full json, this is not guaranteed and some of the json objects may be split across\n",
    "    PayloadPart events. For example:\n",
    "    ```\n",
    "    {'PayloadPart': {'Bytes': b'{\"outputs\": '}}\n",
    "    {'PayloadPart': {'Bytes': b'[\" problem\"]}\\n'}}\n",
    "    ```\n",
    "    \n",
    "    This class accounts for this by concatenating bytes written via the 'write' function\n",
    "    and then exposing a method which will return lines (ending with a '\\n' character) within\n",
    "    the buffer via the 'scan_lines' function. It maintains the position of the last read \n",
    "    position to ensure that previous bytes are not exposed again. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, stream):\n",
    "        self.byte_iterator = iter(stream)\n",
    "        self.buffer = io.BytesIO()\n",
    "        self.read_pos = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        while True:\n",
    "            self.buffer.seek(self.read_pos)\n",
    "            line = self.buffer.readline()\n",
    "            if line and line[-1] == ord('\\n'):\n",
    "                self.read_pos += len(line)\n",
    "                return line[:-1]\n",
    "            try:\n",
    "                chunk = next(self.byte_iterator)\n",
    "            except StopIteration:\n",
    "                if self.read_pos < self.buffer.getbuffer().nbytes:\n",
    "                    continue\n",
    "                raise\n",
    "            if 'PayloadPart' not in chunk:\n",
    "                print('Unknown event type:' + chunk)\n",
    "                continue\n",
    "            self.buffer.seek(0, io.SEEK_END)\n",
    "            self.buffer.write(chunk['PayloadPart']['Bytes'])\n",
    "\n",
    "\n",
    "body = {\n",
    "    \"inputs\":\"tWhat is Amazon SageMaker\",\n",
    "    \"parameters\":{\n",
    "        \"max_new_tokens\": 400,\n",
    "        \"return_full_text\": False\n",
    "    },\n",
    "    \"stream\": True\n",
    "}\n",
    "\n",
    "stop_token = '<|endoftext|>'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d93c145-143b-416d-9163-6006a88c0e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.base_deserializers import StreamDeserializer\n",
    "\n",
    "llm.deserializer=StreamDeserializer()\n",
    "resp = smr_client.invoke_endpoint_with_response_stream(\n",
    "    EndpointName = llm.endpoint_name, \n",
    "    Body = json.dumps(body), \n",
    "    ContentType = 'application/json'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a02bbd2-dea0-48f5-8e4b-776840f47fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_stream = resp['Body']\n",
    "start_json = b'{'\n",
    "for line in LineIterator(event_stream):\n",
    "    if line != b'' and start_json in line:\n",
    "        data = json.loads(line[line.find(start_json):].decode('utf-8'))\n",
    "        if data['token']['text'] != stop_token:\n",
    "            print(data['token']['text'],end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729981f4-b014-40b6-8cbe-3098c2b4e022",
   "metadata": {},
   "source": [
    "## Step 3.2: Test inference performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923327ab-222b-4d23-a3f0-e8050faa1d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Calculate runtime performance\n",
    "# \n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# define payload\n",
    "prompt = \"\"\"You are an helpful Assistant, called Zephyr. Knowing everyting about AWS.\n",
    "\n",
    "User: Can you tell me something about Amazon SageMaker?\n",
    "Zephyr:\"\"\"\n",
    "\n",
    "# hyperparameters for llm (remove \"\\nUser:\" from stop conditions)\n",
    "payload = {\n",
    "  \"inputs\": prompt,\n",
    "  \"parameters\": {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.9,\n",
    "    \"temperature\": 0.8,\n",
    "    \"max_new_tokens\": 512,\n",
    "    \"repetition_penalty\": 1.03,\n",
    "    \"stop\": [\"<|endoftext|>\",\"</s>\"]\n",
    "  }\n",
    "}\n",
    "\n",
    "results = []\n",
    "for i in range(0, 10):\n",
    "    start = time.time()\n",
    "    response_model = smr_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        Body=json.dumps(payload),\n",
    "        ContentType=\"application/json\",\n",
    "    )\n",
    "    results.append((time.time() - start) * 1000)\n",
    "\n",
    "print(\"\\nPredictions for model latency: \\n\")\n",
    "print(\"\\nP95: \" + str(np.percentile(results, 95)) + \" ms\\n\")\n",
    "print(\"P90: \" + str(np.percentile(results, 90)) + \" ms\\n\")\n",
    "print(\"Average: \" + str(np.average(results)) + \" ms\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4255ad-e07d-451a-945c-4b79889e8189",
   "metadata": {},
   "source": [
    "## Step 4: Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04cd1d9-d71f-409f-b6a4-d9a9624d6432",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm.delete_model()\n",
    "llm.delete_endpoint(delete_endpoint_config=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daaa34f-b756-42f0-b64b-24c17a68696c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
