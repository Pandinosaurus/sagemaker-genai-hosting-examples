{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eba763b2-d0f2-4e99-b375-8b4873116bcf",
   "metadata": {},
   "source": [
    "# Deploy HuggingFaceH4/zephyr-7b-beta on Amazon SageMaker using Hugging Face Text Generation Inference (TGI) container\n",
    "\n",
    "## Resources\n",
    "- [Zephyr-7B-beta model card](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta)\n",
    "- [Deep Learning Containers](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-dlc.html)\n",
    "- [Deep Java Library - Large Model Inference](https://docs.djl.ai/docs/serving/serving/docs/large_model_inference.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944fbc1a-b23a-43c5-af27-76fb7fdaa25f",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dd04a8-6181-441e-b557-ee996281e3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4702f11-0670-4d8b-9c62-f325d77be6f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import boto3\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd76e135-46bb-4d47-a546-5de73abb5ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "account_id = sess.account_id()  # account_id of the current SageMaker Studio environment\n",
    "\n",
    "sm_client = boto3.client(\"sagemaker\")  # client to intreract with SageMaker\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")  # client to intreract with SageMaker Endpoints\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n",
    "print(f\"boto3 version: {boto3.__version__}\")\n",
    "print(f\"sagemaker version: {sagemaker.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a74f0e5-966c-4620-8bb9-a33e84c9ceeb",
   "metadata": {},
   "source": [
    "## Step 2: Endpoint Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8b2c21-5edc-445d-938f-634e704c3173",
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"0.26.0\"\n",
    "deepspeed_image = sagemaker.image_uris.retrieve(\n",
    "    \"djl-deepspeed\", region=region, version=version\n",
    ")\n",
    "print(f\"SeepSpeed image going to be used is ----> {deepspeed_image}\")\n",
    "\n",
    "trtllm_image = sagemaker.image_uris.retrieve(\n",
    "    \"djl-tensorrtllm\", region=region, version=version\n",
    ")\n",
    "print(f\"TensorRT-LLM image going to be used is ----> {trtllm_image}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3eee02-514a-40dc-b572-20e770a0c218",
   "metadata": {},
   "source": [
    "### LMI container configuration\n",
    "The notebook contains configurations for 2 use cases:\n",
    "1. Open-ended generation (vllm_config and deepspeed_image)\n",
    "2. Summarization (trtllm_config and trtllm_image)\n",
    "\n",
    "Please pick ***one*** based on your use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f4cab6-c228-4fa3-a7d0-67feadcff891",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Please pick _one_ bases on your use case\n",
    "#\n",
    "\n",
    "number_of_gpu = 1\n",
    "model_name = \"Zephyr-7b-beta\"\n",
    "\n",
    "# vLLM config\n",
    "vllm_config = {\n",
    "    \"SERVING_LOAD_MODELS\": \"test::Python=/opt/ml/model\",\n",
    "    \"OPTION_MODEL_ID\": \"HuggingFaceH4/zephyr-7b-beta\",\n",
    "    \"OPTION_ROLLING_BATCH\": \"vllm\",\n",
    "    \"OPTION_TENSOR_PARALLEL_DEGREE\": \"max\",\n",
    "    \"OPTION_MAX_ROLLING_BATCH_SIZE\": \"32\",\n",
    "    \"OPTION_MAX_INPUT_LEN\": \"1024\",\n",
    "    \"OPTION_MAX_OUTPUT_LEN\": \"2048\",\n",
    "    \"OPTION_MAX_MODEL_LEN\": \"2048\",\n",
    "    \"OPTION_DTYPE\": \"fp16\",\n",
    "    #\"OPTION_OUTPUT_FORMATTER\": \"jsonlines\",\n",
    "    #\"OPTION_ENABLE_STREAMING\": \"True\"\n",
    "}\n",
    "\n",
    "trtllm_config = {\n",
    "    \"SERVING_LOAD_MODELS\": \"test::MPI=/opt/ml/model\",\n",
    "    \"OPTION_MODEL_ID\": \"HuggingFaceH4/zephyr-7b-beta\",\n",
    "    \"OPTION_TENSOR_PARALLEL_DEGREE\": \"max\",\n",
    "    \"OPTION_ROLLING_BATCH\": \"trtllm\",\n",
    "    \"OPTION_MAX_INPUT_LEN\": \"1024\",\n",
    "    \"OPTION_MAX_OUTPUT_LEN\": \"2048\",\n",
    "    \"OPTION_MAX_ROLLING_BATCH_SIZE\": \"64\"\n",
    "}\n",
    "\n",
    "image_uri = deepspeed_image\n",
    "#image_uri = trtllm_image\n",
    "model_name = \"Zephyr-7b-beta-vLLM\"\n",
    "#model_name = \"Zephyr-7b-beta-TRTLLM\"\n",
    "env = vllm_config\n",
    "#env = trtllm_config\n",
    "\n",
    "# create Model\n",
    "print(model_name)\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    PrimaryContainer = {\n",
    "        \"Image\": image_uri,\n",
    "        \"Environment\": env,\n",
    "    }\n",
    ")\n",
    "model_arn = create_model_response[\"ModelArn\"]\n",
    "\n",
    "print(f\"Created Model: {model_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e451d37b-39e3-44df-86cd-f425b33fb261",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_name = f\"{model_name}-EP-config\"\n",
    "instance_type = \"ml.g5.2xlarge\"\n",
    "#\n",
    "# REQUIRED for TensorRT-LLM Just In Time (JIT) compilation\n",
    "#instance_type = \"ml.g5.16xlarge\"\n",
    "health_check_timeout = 600\n",
    "\n",
    "endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants = [\n",
    "        {\n",
    "            \"VariantName\": \"variant1\",\n",
    "            \"ModelName\": model_name,\n",
    "            \"InstanceType\": instance_type,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": health_check_timeout,\n",
    "            \"RoutingConfig\": {\n",
    "                'RoutingStrategy': 'LEAST_OUTSTANDING_REQUESTS'\n",
    "            },\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "endpoint_config_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9c1edd-768a-4fa5-abad-91807651512e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Create endpoint config\n",
    "#\n",
    "endpoint_name = f\"{model_name}-EP\"\n",
    "\n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName = endpoint_name, EndpointConfigName = endpoint_config_name\n",
    ")\n",
    "print(f\"Created Endpoint: {create_endpoint_response['EndpointArn']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c820a22a-29e4-4e9b-af26-ff100ea4878a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Using helper function to wait for the endpoint to be ready\n",
    "#\n",
    "sess.wait_for_endpoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe50cfa-65fa-4fa3-b05b-0c1465a9ca09",
   "metadata": {},
   "source": [
    "## Step 3: Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64184c88-ad55-461b-9509-752eaee1a2f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define payload\n",
    "#\n",
    "# This will give a weird output for streaming config (RUN 3.1)\n",
    "#\n",
    "prompt = \"\"\"You are an helpful Assistant, called Zephyr. Knowing everyting about AWS.\n",
    "\n",
    "User: Can you tell me something about Amazon SageMaker?\n",
    "Zephyr:\"\"\"\n",
    "\n",
    "# hyperparameters for llm (remove \"\\nUser:\" from stop conditions)\n",
    "payload = {\n",
    "  \"inputs\": prompt,\n",
    "  \"parameters\": {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.9,\n",
    "    \"temperature\": 0.8,\n",
    "    \"max_new_tokens\": 512,\n",
    "    \"repetition_penalty\": 1.03,\n",
    "    #\"stop\": [\"<|endoftext|>\",\"</s>\"]\n",
    "  }\n",
    "}\n",
    "\n",
    "response_model = smr_client.invoke_endpoint(\n",
    "    EndpointName = endpoint_name,\n",
    "    Body = json.dumps(payload),\n",
    "    ContentType = \"application/json\",\n",
    ")\n",
    "\n",
    "assistant = json.loads(response_model[\"Body\"].read().decode(\"utf8\"))[\"generated_text\"]\n",
    "#assistant = response_model[\"Body\"].read().decode(\"utf8\")\n",
    "print(assistant)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b74bcc-3f70-4ed2-9688-75681da3a1e7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## (Optional) Step 3.1: Run Inference (streaming)\n",
    "Require change to the model config above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a624455a-478f-455c-8ca5-357dcdebbf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LineIterator:\n",
    "    \"\"\"\n",
    "    A helper class for parsing the byte stream input.\n",
    "\n",
    "    The output of the model will be in the following format:\n",
    "    ```\n",
    "    b'{\"outputs\": [\" a\"]}\\n'\n",
    "    b'{\"outputs\": [\" challenging\"]}\\n'\n",
    "    b'{\"outputs\": [\" problem\"]}\\n'\n",
    "    ...\n",
    "    ```\n",
    "\n",
    "    While usually each PayloadPart event from the event stream will contain a byte array\n",
    "    with a full json, this is not guaranteed and some of the json objects may be split across\n",
    "    PayloadPart events. For example:\n",
    "    ```\n",
    "    {'PayloadPart': {'Bytes': b'{\"outputs\": '}}\n",
    "    {'PayloadPart': {'Bytes': b'[\" problem\"]}\\n'}}\n",
    "    ```\n",
    "\n",
    "    This class accounts for this by concatenating bytes written via the 'write' function\n",
    "    and then exposing a method which will return lines (ending with a '\\n' character) within\n",
    "    the buffer via the 'scan_lines' function. It maintains the position of the last read\n",
    "    position to ensure that previous bytes are not exposed again.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, stream):\n",
    "        self.byte_iterator = iter(stream)\n",
    "        self.buffer = io.BytesIO()\n",
    "        self.read_pos = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        while True:\n",
    "            self.buffer.seek(self.read_pos)\n",
    "            line = self.buffer.readline()\n",
    "            if line and line[-1] == ord('\\n'):\n",
    "                self.read_pos += len(line)\n",
    "                return line[:-1]\n",
    "            try:\n",
    "                chunk = next(self.byte_iterator)\n",
    "            except StopIteration:\n",
    "                if self.read_pos < self.buffer.getbuffer().nbytes:\n",
    "                    continue\n",
    "                raise\n",
    "            if 'PayloadPart' not in chunk:\n",
    "                print('Unknown event type:' + chunk)\n",
    "                continue\n",
    "            self.buffer.seek(0, io.SEEK_END)\n",
    "            self.buffer.write(chunk['PayloadPart']['Bytes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0914ab30-ce6f-4944-a75c-8c6b19a972ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "body = {\"inputs\": \"what is Amazon SageMaker?\", \"parameters\": {\"max_new_tokens\":400}}\n",
    "resp = smr_client.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps(body),\n",
    "    ContentType=\"application/json\"\n",
    ")\n",
    "event_stream = resp['Body']\n",
    "\n",
    "for line in LineIterator(event_stream):\n",
    "    resp = json.loads(line)\n",
    "#    #print(resp)\n",
    "    print(resp[\"token\"][\"text\"], end='')\n",
    "    #print(resp.get(\"outputs\")[0], end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8ada4e-0397-43ec-9178-7b603515c55f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70a56eac-1251-49c0-bbed-21a40367f767",
   "metadata": {},
   "source": [
    "## Step 3.2: Test inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9724a55d-e6cc-4b4f-997c-723d660c7e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Calculate runtime performance\n",
    "#\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# define payload\n",
    "prompt = \"\"\"You are an helpful Assistant, called Zephyr. Knowing everyting about AWS.\n",
    "\n",
    "User: Can you tell me something about Amazon SageMaker?\n",
    "Zephyr:\"\"\"\n",
    "\n",
    "# hyperparameters for llm (remove \"\\nUser:\" from stop conditions)\n",
    "payload = {\n",
    "  \"inputs\": prompt,\n",
    "  \"parameters\": {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.9,\n",
    "    \"temperature\": 0.8,\n",
    "    \"max_new_tokens\": 512,\n",
    "    \"repetition_penalty\": 1.03,\n",
    "    #\"stop\": [\"<|endoftext|>\",\"</s>\"]\n",
    "  }\n",
    "}\n",
    "\n",
    "results = []\n",
    "for i in range(0, 10):\n",
    "    start = time.time()\n",
    "    response_model = smr_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        Body=json.dumps(payload),\n",
    "        ContentType=\"application/json\",\n",
    "    )\n",
    "    results.append((time.time() - start) * 1000)\n",
    "\n",
    "print(\"\\nPredictions for model latency: \\n\")\n",
    "print(\"\\nP95: \" + str(np.percentile(results, 95)) + \" ms\\n\")\n",
    "print(\"P90: \" + str(np.percentile(results, 90)) + \" ms\\n\")\n",
    "print(\"Average: \" + str(np.average(results)) + \" ms\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4255ad-e07d-451a-945c-4b79889e8189",
   "metadata": {},
   "source": [
    "## Step 4: Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04cd1d9-d71f-409f-b6a4-d9a9624d6432",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sess.delete_endpoint(endpoint_name)\n",
    "sess.delete_endpoint_config(endpoint_config_name)\n",
    "sess.delete_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daaa34f-b756-42f0-b64b-24c17a68696c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
