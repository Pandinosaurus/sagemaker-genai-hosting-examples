{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94d68fb3-00f2-447c-b5e0-28fa18562392",
   "metadata": {},
   "source": [
    "# Deploy the Owlv2-base-patch16 for inference using Amazon SageMakerAI\n",
    "**Recommended kernel(s):** This notebook can be run with any Amazon SageMaker Studio kernel.\n",
    "\n",
    "In this notebook, you will learn how to deploy the Qwen3-VL-2B-Instruct model (HuggingFace model ID: [google/owlv2-base-patch16](https://huggingface.co/google/owlv2-base-patch16)) using Amazon SageMaker AI. \n",
    "\n",
    "Let's install or upgrade these dependencies using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed1c92f-9fbc-47c9-b940-7dd07f962ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -Uq huggingface==4.49 sagemaker transformers==4.57.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc56cad7-ea97-43c4-835d-a56729549023",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c128cf97-3f1f-4176-87b4-1193cd9e9c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import sagemaker\n",
    "import boto3\n",
    "import logging\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "import tarfile\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "print(sagemaker.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba6d4bf-5df9-4e3e-8a9e-a03897e0cb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "instance_type = \"ml.g5.4xlarge\"\n",
    "instance_count = 1\n",
    "\n",
    "model_id = \"google/owlv2-base-patch16\"\n",
    "model_id_filesafe = model_id.replace(\"/\", \"_\").replace(\".\", \"_\")\n",
    "endpoint_name = f\"{model_id_filesafe.replace(\"_\", \"-\")}-endpoint-{str(datetime.datetime.now().timestamp()).replace(\".\", \"-\")}\"\n",
    "print(endpoint_name)\n",
    "\n",
    "base_name = model_id.split('/')[-1].replace('.', '-').lower()\n",
    "model_lineage = model_id.split('/')[0]\n",
    "base_name\n",
    "\n",
    "bucket_name = session.default_bucket()\n",
    "default_prefix = session.default_bucket_prefix or f\"models/{model_id_filesafe}\"\n",
    "print(f\"Saving model artifacts to {bucket_name}/{default_prefix}\")\n",
    "\n",
    "os.makedirs(\"code\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7d4737-12ca-4cb7-aee4-06f7c0328b6d",
   "metadata": {},
   "source": [
    "## Local Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51db134d-19bb-4702-949e-cf8160fe917f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is adapted from https://huggingface.co/google/owlv2-base-patch16\n",
    "\n",
    "import requests\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoProcessor, Owlv2ForObjectDetection\n",
    "from transformers.utils.constants import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"google/owlv2-base-patch16\")\n",
    "model = Owlv2ForObjectDetection.from_pretrained(\"google/owlv2-base-patch16\")\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "texts = [[\"a photo of a cat\", \"a photo of a dog\"]]\n",
    "inputs = processor(text=texts, images=image, return_tensors=\"pt\")\n",
    "\n",
    "# forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Note: boxes need to be visualized on the padded, unnormalized image\n",
    "# hence we'll set the target image sizes (height, width) based on that\n",
    "\n",
    "def get_preprocessed_image(pixel_values):\n",
    "    pixel_values = pixel_values.squeeze().numpy()\n",
    "    unnormalized_image = (pixel_values * np.array(OPENAI_CLIP_STD)[:, None, None]) + np.array(OPENAI_CLIP_MEAN)[:, None, None]\n",
    "    unnormalized_image = (unnormalized_image * 255).astype(np.uint8)\n",
    "    unnormalized_image = np.moveaxis(unnormalized_image, 0, -1)\n",
    "    unnormalized_image = Image.fromarray(unnormalized_image)\n",
    "    return unnormalized_image\n",
    "\n",
    "unnormalized_image = get_preprocessed_image(inputs.pixel_values)\n",
    "\n",
    "target_sizes = torch.Tensor([unnormalized_image.size[::-1]])\n",
    "# Convert outputs (bounding boxes and class logits) to final bounding boxes and scores\n",
    "results = processor.post_process_object_detection(\n",
    "    outputs=outputs, threshold=0.2, target_sizes=target_sizes\n",
    ")\n",
    "\n",
    "i = 0  # Retrieve predictions for the first image for the corresponding text queries\n",
    "text = texts[i]\n",
    "boxes, scores, labels = results[i][\"boxes\"], results[i][\"scores\"], results[i][\"labels\"]\n",
    "\n",
    "for box, score, label in zip(boxes, scores, labels):\n",
    "    box = [round(i, 2) for i in box.tolist()]\n",
    "    print(f\"Detected {text[label]} with confidence {round(score.item(), 3)} at location {box}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7245e684-aa6e-44ca-9451-10e0bee4b96e",
   "metadata": {},
   "source": [
    "## Create SageMaker Model\n",
    "Here we define the custom requirements and inference logic to be run by this model. We download the model assets from HuggingFace, zip them up and upload them to S3. We then deploy the model as a `HuggingFaceModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410cb9a8-9fd0-408d-bebb-af8b8c09d398",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = {\n",
    "    'HF_MODEL_ID': model_id,\n",
    "    'HF_TASK':'image-text-to-text',\n",
    "    'SM_NUM_GPUS': json.dumps(1),\n",
    "    'OPTION_TRUST_REMOTE_CODE': 'true',\n",
    "    'OPTION_MODEL_LOADING_TIMEOUT': '3600',\n",
    "    \"OPTION_ROLLING_BATCH\": \"disable\",\n",
    "    \"OPTION_TENSOR_PARALLEL_DEGREE\": \"1\",\n",
    "    \"OPTION_MAX_MODEL_LEN\": \"5000\",\n",
    "    \"OPTION_ASYNC_MODE\": \"true\",\n",
    "    \"OPTION_TRUST_REMOTE_CODE\": \"true\",\n",
    "    \"SERVING_FAIL_FAST\": \"true\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d441971-621c-4d52-a7db-d67020a8667e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code/requirements.txt\n",
    "transformers==4.57.0\n",
    "torch\n",
    "torchvision\n",
    "torchaudio\n",
    "pillow\n",
    "requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6468cba-5d10-484d-898a-19802612925c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code/inference.py\n",
    "# This code comes from HuggingFace\n",
    "# https://huggingface.co/google/owlv2-base-patch16\n",
    "\n",
    "import logging\n",
    "import requests\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoProcessor, Owlv2ForObjectDetection\n",
    "from transformers.utils.constants import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "def model_fn(model_dir):\n",
    "\n",
    "    model = Owlv2ForObjectDetection.from_pretrained(\n",
    "        model_dir,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    processor = AutoProcessor.from_pretrained(\n",
    "        model_dir,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    return {\"processor\": processor, \"model\": model}\n",
    "\n",
    "\n",
    "def predict_fn(data, model_obj):\n",
    "    processor = model_obj[\"processor\"]\n",
    "    model = model_obj[\"model\"]\n",
    "    \n",
    "    url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "    image = Image.open(requests.get(url, stream=True).raw)\n",
    "    texts = [[\"a photo of a cat\", \"a photo of a dog\"]]\n",
    "    inputs = processor(text=texts, images=image, return_tensors=\"pt\")\n",
    "    \n",
    "    # forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Note: boxes need to be visualized on the padded, unnormalized image\n",
    "    # hence we'll set the target image sizes (height, width) based on that\n",
    "    \n",
    "    def get_preprocessed_image(pixel_values):\n",
    "        pixel_values = pixel_values.squeeze().numpy()\n",
    "        unnormalized_image = (pixel_values * np.array(OPENAI_CLIP_STD)[:, None, None]) + np.array(OPENAI_CLIP_MEAN)[:, None, None]\n",
    "        unnormalized_image = (unnormalized_image * 255).astype(np.uint8)\n",
    "        unnormalized_image = np.moveaxis(unnormalized_image, 0, -1)\n",
    "        unnormalized_image = Image.fromarray(unnormalized_image)\n",
    "        return unnormalized_image\n",
    "    \n",
    "    unnormalized_image = get_preprocessed_image(inputs.pixel_values)\n",
    "    \n",
    "    target_sizes = torch.Tensor([unnormalized_image.size[::-1]])\n",
    "    # Convert outputs (bounding boxes and class logits) to final bounding boxes and scores\n",
    "    results = processor.post_process_object_detection(\n",
    "        outputs=outputs, threshold=0.2, target_sizes=target_sizes\n",
    "    )\n",
    "    \n",
    "    i = 0  # Retrieve predictions for the first image for the corresponding text queries\n",
    "    text = texts[i]\n",
    "    boxes, scores, labels = results[i][\"boxes\"], results[i][\"scores\"], results[i][\"labels\"]\n",
    "    \n",
    "    for box, score, label in zip(boxes, scores, labels):\n",
    "        box = [round(i, 2) for i in box.tolist()]\n",
    "        print(f\"Detected {text[label]} with confidence {round(score.item(), 3)} at location {box}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba9db6b-3a56-462a-a513-770ed7f4bbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_function(tarinfo):\n",
    "    \"\"\"Filter function to exclude .cache files and directories\"\"\"\n",
    "    if '.cache' in tarinfo.name or '.gitattributes' in tarinfo.name:\n",
    "        return None\n",
    "    return tarinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9b8cdf-ebf9-4d05-837d-8a3f412b73d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "key = f\"{default_prefix}/model.tar.gz\"\n",
    "force_rebuild_tarball = True\n",
    "\n",
    "if force_rebuild_tarball or not s3_client.head_object(Bucket=bucket_name, Key=key):\n",
    "    try:\n",
    "        model_path = snapshot_download(repo_id=model_id, local_dir=\"./model\")\n",
    "        print(f\"Successfully downloaded to {model_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download after retries: {str(e)}\")\n",
    "    \n",
    "    print(\"Building gzipped tarball...\")\n",
    "    with tarfile.open(\"./model.tar.gz\", \"w:gz\") as tar:\n",
    "        tar.add(model_path, arcname=\".\", filter=filter_function)\n",
    "        tar.add(\"./code\", filter=filter_function)\n",
    "    print(\"Successfully tarred the ball.\")\n",
    "    \n",
    "    print(f\"Uploading tarball to {bucket_name}/{default_prefix}...\")\n",
    "    s3_client.upload_file(\"./model.tar.gz\", bucket_name, f\"{default_prefix}/model.tar.gz\")\n",
    "    shutil.rmtree(\"./model\")\n",
    "    os.remove(\"./model.tar.gz\")\n",
    "    print(\"Successfully uploaded, working directory cleaned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40af4b17-30e9-4138-93f3-0589e4770815",
   "metadata": {},
   "source": [
    "## Deploy Model to SageMaker Endpoint\n",
    "\n",
    "Now we'll deploy our model to a SageMaker endpoint for real-time inference. This is a significant step that:\n",
    "1. Provisions the specified compute resources (G5 instance)\n",
    "2. Deploys the model container\n",
    "3. Sets up the endpoint for API access\n",
    "\n",
    "### Deployment Configuration\n",
    "- **Instance Count**: 1 instance for single-node deployment\n",
    "- **Instance Type**: `ml.g5.4xlarge` for high-performance inference\n",
    "\n",
    "> ⚠️ **Important**: \n",
    "> - Deployment can take up to 15 minutes\n",
    "> - Monitor the CloudWatch logs for progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbeaf71-b51c-4b65-8196-ba0f403eb2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hub Model configuration. https://huggingface.co/models\n",
    "hub = {\n",
    "\t'HF_MODEL_ID':'google/owlv2-base-patch16',\n",
    "\t'HF_TASK':'zero-shot-object-detection'\n",
    "}\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    model_data=f\"s3://{bucket_name}/{default_prefix}/model.tar.gz\",\n",
    "\ttransformers_version='4.49.0',\n",
    "\tpytorch_version='2.6.0',\n",
    "\tpy_version='py312',\n",
    "\tenv=env,\n",
    "\trole=role, \n",
    "    entry_point=\"inference.py\",\n",
    "    enable_network_isolation=False\n",
    ")\n",
    "\n",
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "\tinitial_instance_count=1, # number of instances\n",
    "\tinstance_type='ml.m5.xlarge' # ec2 instance type\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ef9603-a16d-4195-9702-b920762e2e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DJL Serving\n",
    "# UNDER CONSTRUCTION\n",
    "\n",
    "# image_uri = \"763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.34.0-lmi16.0.0-cu128-v1.2\"\n",
    "\n",
    "# model = HuggingFaceModel(\n",
    "#     model_data=f\"s3://{bucket_name}/{default_prefix}/model.tar.gz\",\n",
    "#     image_uri=image_uri,\n",
    "#     env=env,\n",
    "#     role=role,\n",
    "#     entry_point=\"inference.py\",\n",
    "#     enable_network_isolation=False\n",
    "# )\n",
    "\n",
    "# predictor = model.deploy(\n",
    "#     initial_instance_count=instance_count,\n",
    "#     instance_type=instance_type,\n",
    "#     endpoint_name=endpoint_name\n",
    "# )\n",
    "\n",
    "# predictor.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b85c32-d4df-45f1-84f4-86a296ad1c0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T19:14:57.135928Z",
     "iopub.status.busy": "2025-09-15T19:14:57.135661Z",
     "iopub.status.idle": "2025-09-15T19:14:57.139468Z",
     "shell.execute_reply": "2025-09-15T19:14:57.138566Z",
     "shell.execute_reply.started": "2025-09-15T19:14:57.135907Z"
    }
   },
   "source": [
    "# Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcef37c7-6b47-4c3b-b6b7-b266245b492e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint(True)\n",
    "huggingface_model.delete_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac15a012-fa80-43d0-a8d2-cdfbf2082e2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
