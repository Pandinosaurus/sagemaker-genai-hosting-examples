{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94d68fb3-00f2-447c-b5e0-28fa18562392",
   "metadata": {},
   "source": [
    "# How to deploy the VaultGemma 1B for inference using Amazon SageMakerAI\n",
    "**Recommended kernel(s):** This notebook can be run with any Amazon SageMaker Studio kernel.\n",
    "\n",
    "In this notebook, you will learn how to deploy the Vault Gemma 1B model (HuggingFace model ID: [google/vaultgemma-1b](https://huggingface.co/google/vaultgemma-1b)) using Amazon SageMaker AI. \n",
    "\n",
    "VaultGemma is a variant of the Gemma family of lightweight, state-of-the-art open models from Google. It is pre-trained from the ground up using Differential Privacy (DP). This provides strong, mathematically-backed privacy guarantees for its training data, limiting the extent to which the model's outputs can reveal information about any single training example.\n",
    "\n",
    "VaultGemma uses a similar architecture as Gemma 2. VaultGemma is a pretrained model that can be instruction tuned for a variety of language understanding and generation tasks. Its relatively small size (< 1B parameters) makes it possible to deploy in environments with limited resources, democratizing access to state-of-the-art AI models that are built with privacy at their core.\n",
    "\n",
    "### License agreement\n",
    "* This model is gated on HuggingFace, please refer to the original [model card](https://huggingface.co/google/vaultgemma-1b) for license.\n",
    "* This notebook is a sample notebook and not intended for production use.\n",
    "\n",
    "### Execution environment setup\n",
    "This notebook requires the following third-party Python dependencies:\n",
    "* AWS [`sagemaker`](https://sagemaker.readthedocs.io/en/stable/index.html) with a version greater than or equal to 2.242.0\n",
    "\n",
    "Let's install or upgrade these dependencies using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed1c92f-9fbc-47c9-b940-7dd07f962ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -Uq huggingface==4.49 sagemaker transformers==4.57.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc56cad7-ea97-43c4-835d-a56729549023",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c128cf97-3f1f-4176-87b4-1193cd9e9c0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T20:33:42.848333Z",
     "iopub.status.busy": "2025-10-28T20:33:42.848186Z",
     "iopub.status.idle": "2025-10-28T20:33:42.851101Z",
     "shell.execute_reply": "2025-10-28T20:33:42.850698Z",
     "shell.execute_reply.started": "2025-10-28T20:33:42.848317Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.253.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import sagemaker\n",
    "import boto3\n",
    "import logging\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "import tarfile\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "print(sagemaker.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bba6d4bf-5df9-4e3e-8a9e-a03897e0cb0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T20:38:20.172482Z",
     "iopub.status.busy": "2025-10-28T20:38:20.172255Z",
     "iopub.status.idle": "2025-10-28T20:38:20.624643Z",
     "shell.execute_reply": "2025-10-28T20:38:20.624085Z",
     "shell.execute_reply.started": "2025-10-28T20:38:20.172467Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google-vaultgemma-1b-endpoint-1761683900-420591\n",
      "Saving model artifacts to sagemaker-us-east-1-329542461890/models/google_vaultgemma-1b\n"
     ]
    }
   ],
   "source": [
    "session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "instance_type = \"ml.m5.xlarge\"\n",
    "instance_count = 1\n",
    "\n",
    "HUGGING_FACE_HUB_TOKEN = \"<REPLACE_ME>\"\n",
    "model_id = \"google/vaultgemma-1b\"\n",
    "model_id_filesafe = model_id.replace(\"/\", \"_\").replace(\".\", \"_\")\n",
    "endpoint_name = f\"{model_id_filesafe.replace(\"_\", \"-\")}-endpoint-{str(datetime.datetime.now().timestamp()).replace(\".\", \"-\")}\"\n",
    "print(endpoint_name)\n",
    "\n",
    "base_name = model_id.split('/')[-1].replace('.', '-').lower()\n",
    "model_lineage = model_id.split('/')[0]\n",
    "base_name\n",
    "\n",
    "bucket_name = session.default_bucket()\n",
    "default_prefix = session.default_bucket_prefix or f\"models/{model_id_filesafe}\"\n",
    "print(f\"Saving model artifacts to {bucket_name}/{default_prefix}\")\n",
    "\n",
    "os.makedirs(\"code\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c1ee59-9644-42d3-8f3f-45dd7e8c124d",
   "metadata": {},
   "source": [
    "### Local Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8b33210-6e04-4885-ad35-513f257124d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T20:42:00.175305Z",
     "iopub.status.busy": "2025-10-28T20:42:00.175098Z",
     "iopub.status.idle": "2025-10-28T20:42:00.193771Z",
     "shell.execute_reply": "2025-10-28T20:42:00.193345Z",
     "shell.execute_reply.started": "2025-10-28T20:42:00.175290Z"
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(HUGGING_FACE_HUB_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f67cb1f-77b4-4348-884e-cccf5a4f845a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T20:42:04.762066Z",
     "iopub.status.busy": "2025-10-28T20:42:04.761680Z",
     "iopub.status.idle": "2025-10-28T20:42:17.726388Z",
     "shell.execute_reply": "2025-10-28T20:42:17.725770Z",
     "shell.execute_reply.started": "2025-10-28T20:42:04.762034Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40cb769b1a894fc580d2b283d11ed516",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.37k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-28 20:42:07.834777: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1761684127.844092   65037 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1761684127.847260   65037 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-10-28 20:42:07.857283: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2af8604482e40b8bb00f613106f3df0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.08G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd6d67318675442491d3cf9f4a19f642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain in simple terms how differential privacy works.\n",
      "\n",
      "Explain in simple terms how differential privacy works.\n",
      "\n",
      "The following is a list of some of the functions of the nervous system:\n",
      "\n",
      "$\\begin{array}{lll} \\text { (a) } & \\text { muscle } & \\text { (b) } & \\text { nerve } \\\\ \\text { (c) } & \\text { gland } & \\text { (d) } & \\text { muscle } \\\\ \\text { (e) } & \\text { gland } & \\text { (f) } & \\text { gland } \\\\ \\text { (g) } & \\text { gland } & \\text { (h) } & \\text { gland } \\\\ \\text { (\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/vaultgemma-1b\", trust_remote_code=True, token=HUGGING_FACE_HUB_TOKEN)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/vaultgemma-1b\", device_map=\"auto\", dtype=\"auto\")\n",
    "\n",
    "prompt_text = \"Explain in simple terms how differential privacy works.\"\n",
    "input_ids = tokenizer(prompt_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_outputs = model.generate(\n",
    "    **input_ids,\n",
    "    max_new_tokens=150,\n",
    "    do_sample=True,\n",
    "    top_p=0.9,\n",
    "    temperature=0.8\n",
    ")\n",
    "\n",
    "response = tokenizer.decode(generated_outputs[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7245e684-aa6e-44ca-9451-10e0bee4b96e",
   "metadata": {},
   "source": [
    "## Create SageMaker Model\n",
    "\n",
    "#### HUGGING_FACE_HUB_TOKEN \n",
    "VaultGemma-1B is a gated model. Therefore, if you deploy model files hosted on the Hub, you need to provide your HuggingFace token as environment variable. This enables SageMaker AI to download the files at runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e70481-6eff-45d2-90a3-58d6736b67fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = {\n",
    "    'HF_MODEL_ID': model_id,\n",
    "    'HF_TOKEN': HUGGING_FACE_HUB_TOKEN,\n",
    "    'HF_TASK':'image-text-to-text',\n",
    "    'SM_NUM_GPUS': json.dumps(1),\n",
    "    'OPTION_TRUST_REMOTE_CODE': 'true',\n",
    "    'OPTION_MODEL_LOADING_TIMEOUT': '3600',\n",
    "    \"OPTION_ROLLING_BATCH\": \"vllm\",\n",
    "    \"OPTION_TENSOR_PARALLEL_DEGREE\": \"1\",\n",
    "    \"OPTION_MAX_MODEL_LEN\": \"5000\",\n",
    "    \"OPTION_ASYNC_MODE\": \"true\",\n",
    "    \"OPTION_TRUST_REMOTE_CODE\": \"true\",\n",
    "    \"SERVING_FAIL_FAST\": \"true\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954728b5-14c7-4ce0-9d3a-0ec1eab4414f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code/requirements.txt\n",
    "transformers==4.57.0\n",
    "huggingface==4.49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df67b145-1667-416e-b435-3a25c6384574",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code/inference.py\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "def model_fn(model_dir):\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"auto\", dtype=\"auto\")\n",
    "\n",
    "\n",
    "    return {\"tokenizer\": tokenizer, \"model\": model}\n",
    "\n",
    "\n",
    "def predict_fn(data, model_obj):\n",
    "    tokenizer = model_obj[\"tokenizer\"]\n",
    "    model = model_obj[\"model\"]\n",
    "    \n",
    "    prompt_text = \"Explain in simple terms how differential privacy works.\"\n",
    "    input_ids = tokenizer(prompt_text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    generated_outputs = model.generate(\n",
    "        **input_ids,\n",
    "        max_new_tokens=150,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        temperature=0.8\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(generated_outputs[0], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14da1022-ddfd-4e67-b51c-5db9eadf255e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_function(tarinfo):\n",
    "    \"\"\"Filter function to exclude .cache files and directories\"\"\"\n",
    "    if '.cache' in tarinfo.name or '.gitattributes' in tarinfo.name:\n",
    "        return None\n",
    "    return tarinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301cd83e-da6f-4091-9f54-4b20cc982d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "key = f\"{default_prefix}/model.tar.gz\"\n",
    "force_rebuild_tarball = True\n",
    "\n",
    "if force_rebuild_tarball or not s3_client.head_object(Bucket=bucket_name, Key=key):\n",
    "    try:\n",
    "        model_path = snapshot_download(repo_id=model_id, local_dir=\"./model\")\n",
    "        print(f\"Successfully downloaded to {model_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download after retries: {str(e)}\")\n",
    "    \n",
    "    print(\"Building gzipped tarball...\")\n",
    "    with tarfile.open(\"./model.tar.gz\", \"w:gz\") as tar:\n",
    "        tar.add(model_path, arcname=\".\", filter=filter_function)\n",
    "        tar.add(\"./code\", filter=filter_function)\n",
    "    print(\"Successfully tarred the ball.\")\n",
    "    \n",
    "    print(f\"Uploading tarball to {bucket_name}/{default_prefix}...\")\n",
    "    s3_client.upload_file(\"./model.tar.gz\", bucket_name, f\"{default_prefix}/model.tar.gz\")\n",
    "    # shutil.rmtree(\"./model\")\n",
    "    # os.remove(\"./model.tar.gz\")\n",
    "    print(\"Successfully uploaded, working directory cleaned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40af4b17-30e9-4138-93f3-0589e4770815",
   "metadata": {},
   "source": [
    "## Deploy Model to SageMaker Endpoint\n",
    "\n",
    "Now we'll deploy our model to a SageMaker endpoint for real-time inference. This is a significant step that:\n",
    "1. Provisions the specified compute resources (M5 instance)\n",
    "2. Deploys the model container\n",
    "3. Sets up the endpoint for API access\n",
    "\n",
    "### Deployment Configuration\n",
    "- **Instance Count**: 1 instance for single-node deployment\n",
    "- **Instance Type**: `ml.m5.xlarge` for high-performance inference\n",
    "\n",
    "> ⚠️ **Important**: \n",
    "> - Deployment can take up to 15 minutes\n",
    "> - Monitor the CloudWatch logs for progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794b5a0a-bfb4-4659-962b-9523ce38de1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hub Model configuration. https://huggingface.co/models\n",
    "hub = {\n",
    "\t'HF_MODEL_ID':'google/vaultgemma-1b',\n",
    "\t'HF_TASK':'image-text-to-text',\n",
    "    'HF_TOKEN': HUGGING_FACE_HUB_TOKEN,\n",
    "}\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    model_data=f\"s3://{bucket_name}/{default_prefix}/model.tar.gz\",\n",
    "\ttransformers_version='4.49.0',\n",
    "\tpytorch_version='2.6.0',\n",
    "\tpy_version='py312',\n",
    "\tenv=env,\n",
    "\trole=role, \n",
    "    entry_point=\"inference.py\",\n",
    "    enable_network_isolation=False\n",
    ")\n",
    "\n",
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "\tinitial_instance_count=1, # number of instances\n",
    "\tinstance_type='ml.m5.xlarge' # ec2 instance type\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e196ece-bada-486a-8f20-b78a381de41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DJL Serving\n",
    "# UNDER CONSTRUCTION\n",
    "# %%time\n",
    "\n",
    "# image_uri = \"763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.34.0-lmi16.0.0-cu128-v1.2\"\n",
    "# model = HuggingFaceModel(\n",
    "#     model_data=f\"s3://{bucket_name}/{default_prefix}/model.tar.gz\",\n",
    "#     image_uri=image_uri,\n",
    "#     env=env,\n",
    "#     role=role,\n",
    "#     entry_point=\"inference.py\",\n",
    "#     enable_network_isolation=False\n",
    "# )\n",
    "\n",
    "# predictor = model.deploy(\n",
    "#     initial_instance_count=instance_count,\n",
    "#     instance_type=instance_type,\n",
    "#     endpoint_name=endpoint_name\n",
    "# )\n",
    "\n",
    "# predictor.predict({\n",
    "# \t\"inputs\": \"Can you please let us know more details about your training using differential privacy?\",\n",
    "# })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b85c32-d4df-45f1-84f4-86a296ad1c0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T19:14:57.135928Z",
     "iopub.status.busy": "2025-09-15T19:14:57.135661Z",
     "iopub.status.idle": "2025-09-15T19:14:57.139468Z",
     "shell.execute_reply": "2025-09-15T19:14:57.138566Z",
     "shell.execute_reply.started": "2025-09-15T19:14:57.135907Z"
    }
   },
   "source": [
    "# Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcef37c7-6b47-4c3b-b6b7-b266245b492e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint(True)\n",
    "huggingface_model.delete_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
