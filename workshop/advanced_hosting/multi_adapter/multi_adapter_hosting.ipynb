{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71a329f0",
   "metadata": {},
   "source": [
    "# Using SageMaker Efficient Multi-Adapter Serving to host LoRA adapters at Scale\n",
    "\n",
    "Multi-Adapter serving allows for multiple fine-tuned models to be hosted in a cost efficient manner on a singular endpoint. Via a multi-adapter approach we can tackle multiple different tasks with a singular base LLM. In this example you will use a pre-trained LoRA adapter that was fine tuned from Llama 3.1 8B Instruct on the [ECTSum dataset](https://huggingface.co/datasets/mrSoul7766/ECTSum).\n",
    "\n",
    "You will also see how to dynamically load these adapters using [SageMaker Inference Components](https://aws.amazon.com/blogs/aws/amazon-sagemaker-adds-new-inference-capabilities-to-help-reduce-foundation-model-deployment-costs-and-latency/), in this example we specifically explore the Inference Component Adapter feature which will allow for us to load hundreds of adapters on a SageMaker real-time endpoint.\n",
    "\n",
    "![](./images/ic-adapter-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8974eee7-cc31-4dac-8795-aec6b8765051",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8b2b57-37e9-4c8b-882e-8c728c01dc70",
   "metadata": {},
   "source": [
    "### Fetch and import dependencies \n",
    "Ignore incompatability errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fa3208",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -Uq datasets==3.0.0 --no-warn-conflicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50ec728-086a-4ac7-8f8d-8e096a6d8891",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sagemaker --upgrade --quiet --no-warn-conflicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2f0c15-bb21-4643-b764-35b7cd9a6526",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!sudo apt-get install git-lfs\n",
    "#!git clone https://github.com/aws-samples/sagemaker-genai-hosting-examples.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3504dde-39c4-411f-86c9-c473d46b0831",
   "metadata": {},
   "source": [
    "## Restart kernel before continuing \n",
    "## Menu Bar > Kernel > Restart Kernel..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9ac353",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "print(f\"boto3 version: {boto3.__version__}\")\n",
    "print(f\"sagemaker version: {sagemaker.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a27613-3975-4ddd-80ee-d22990647978",
   "metadata": {},
   "source": [
    "### Configure development environment and boto3 clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94eba2f6-e1b6-41c6-94d1-2b2bfbe3308b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "region = sess._region_name\n",
    "\n",
    "sm_client = boto3.client(service_name=\"sagemaker\")\n",
    "sm_runtime = boto3.client(service_name=\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e58cf33",
   "metadata": {},
   "source": [
    "## Step 2: Deploy a model to SageMaker IC-based endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f52d91b-af19-4763-8584-0e1c8f3d531e",
   "metadata": {},
   "source": [
    "### Select a Large Model Inference (LMI) container image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbef0bb-14ce-47e9-b99f-7cdd4afe7f7c",
   "metadata": {},
   "source": [
    "Select one of the [available Large Model Inference (LMI) container images for hosting](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#large-model-inference-containers). Efficient adapter inference capability is available in `0.31.0-lmi13.0.0` and higher. Ensure that you are using the image URI for the region that corresponds with your deployment region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a174b36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#inference_image_uri = f\"763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.31.0-lmi13.0.0-cu124\"\n",
    "inference_image_uri = f\"763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.32.0-lmi14.0.0-cu124\"\n",
    "\n",
    "print(f\"Inference container image:: {inference_image_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09491292-b7cc-47ac-a1ca-136b5ad5fa86",
   "metadata": {},
   "source": [
    "### Configure model container environment\n",
    "\n",
    "Create an container environment for the hosting container. LMI container parameters can be found in the [LMI User Guides](https://docs.djl.ai/master/docs/serving/serving/docs/lmi/user_guides/index.html).\n",
    "\n",
    "By using the `OPTION_MAX_LORAS` and `OPTION_MAX_CPU_LORAS` parameters, you can control how adapters are loaded and unloaded into GPU/CPU memory. The `OPTION_MAX_LORAS` parameter defines the number of adapters that will be held in GPU memory. The `OPTION_MAX_CPU_LORAS` parameter controls the number of adapters that will be held in CPU memory. It is important to note that adapters which are loaded to GPU have to be precached in CPU memory and will occupy space in the CPU cache. This means `OPTION_MAX_CPU_LORAS` should be set to `OPTION_MAX_LORAS + <number of adapters you want to cache in CPU>`. Any adapters beyond this will be offloaded to local SSD. \n",
    "\n",
    "In the following example, the container will hold 30 adapters in GPU memory, and 70 adapters in CPU memory. Out of the 70, 30 will be precached adapters that already reside in GPU, leaving you with 40 slots free.\n",
    "\n",
    "```\n",
    "env = {\n",
    "    \"HF_MODEL_ID\": f\"{s3_model_path}\",\n",
    "    \"OPTION_ROLLING_BATCH\": \"lmi-dist\",\n",
    "    \"OPTION_MAX_ROLLING_BATCH_SIZE\": \"16\",\n",
    "    \"OPTION_TENSOR_PARALLEL_DEGREE\": \"max\",\n",
    "    \"OPTION_ENABLE_LORA\": \"true\",\n",
    "    \"OPTION_MAX_LORAS\": \"30\",\n",
    "    \"OPTION_MAX_CPU_LORAS\": \"70\",\n",
    "    \"OPTION_DTYPE\": \"fp16\",\n",
    "    \"OPTION_MAX_MODEL_LEN\": \"6000\"\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0189411-799d-48d4-9cb7-8935b609da22",
   "metadata": {},
   "source": [
    "Later in this workshop you will test scenarios where you will force adapters to swap between different tiers. To make this easier, you will set the `OPTION_MAX_LORAS` property to `1` and the `OPTION_MAX_CPU_LORAS` to `2`. This will allow you to hold 1 adapter in GPU memory and 1 in CPU memory (plus 1 precached from GPU) before moving adapters to disk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1717b6-3d6d-4170-ae4c-4a6e277ccc7d",
   "metadata": {},
   "source": [
    "---\n",
    "You can deploy a model on SageMaker endpoint from several sources:\n",
    "- SageMaker JumpStart\n",
    "- HuggingFace model hub\n",
    "- Amazon S3 bucker\n",
    "---\n",
    "\n",
    "#### Please choose only ONE deployment option below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b224dc8-ff1f-4ac1-a508-c5a9e1b75374",
   "metadata": {},
   "source": [
    "### Option 1: Deploy a model from SageMaker JumpStart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741d59fd-d135-462d-8611-9205032eeb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "from sagemaker.compute_resource_requirements.resource_requirements import ResourceRequirements\n",
    "\n",
    "model_id, model_version = \"meta-textgeneration-llama-3-1-8b-instruct\", \"2.7.2\"\n",
    "\n",
    "model_name = endpoint_name = sagemaker.utils.name_from_base(\"test\")\n",
    "base_inference_component_name = \"base-\" + model_name\n",
    "\n",
    "env = {\n",
    "    \"HF_MODEL_ID\": \"/opt/ml/model\",\n",
    "    \"OPTION_ROLLING_BATCH\": \"lmi-dist\",\n",
    "    \"OPTION_MAX_ROLLING_BATCH_SIZE\": \"16\",\n",
    "    \"OPTION_TENSOR_PARALLEL_DEGREE\": \"max\",\n",
    "    \"OPTION_ENABLE_LORA\": \"true\",\n",
    "    \"OPTION_MAX_LORAS\": \"1\",\n",
    "    \"OPTION_MAX_CPU_LORAS\": \"2\",\n",
    "    \"OPTION_DTYPE\": \"fp16\",\n",
    "    \"OPTION_MAX_MODEL_LEN\": \"6000\"\n",
    "}\n",
    "\n",
    "jumpstart_model = JumpStartModel(model_id=model_id,\n",
    "                                 model_version=model_version,\n",
    "                                 name=model_name,\n",
    "                                 image_uri=inference_image_uri,\n",
    "                                 env=env)\n",
    "\n",
    "jumpstart_model.deploy(\n",
    "    accept_eula=True,\n",
    "    instance_type=\"ml.g5.2xlarge\",\n",
    "    initial_instance_count=1,\n",
    "    container_startup_health_check_timeout=900,\n",
    "    endpoint_name=endpoint_name,\n",
    "    endpoint_type=sagemaker.enums.EndpointType.INFERENCE_COMPONENT_BASED,\n",
    "    inference_component_name=base_inference_component_name,\n",
    "    resources=ResourceRequirements(requests={\"num_accelerators\": 1, \"memory\": 4096, \"copies\": 1,}),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e49cdd8-03c9-48ee-966d-35760f8ef1ce",
   "metadata": {},
   "source": [
    "### Option 2: Deploy a model from HuggingFace model hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3f397a-b5bf-48da-acb1-b1564e842e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.compute_resource_requirements.resource_requirements import ResourceRequirements\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "model_name = endpoint_name = sagemaker.utils.name_from_base(\"test\")\n",
    "base_inference_component_name = \"base-\" + model_name\n",
    "\n",
    "env = {\n",
    "    \"HF_MODEL_ID\": model_id,\n",
    "    \"HF_TOKEN\": \"<YOUR_HF_TOKEN>\",\n",
    "    \"OPTION_ROLLING_BATCH\": \"lmi-dist\",\n",
    "    \"OPTION_MAX_ROLLING_BATCH_SIZE\": \"16\",\n",
    "    \"OPTION_TENSOR_PARALLEL_DEGREE\": \"max\",\n",
    "    \"OPTION_ENABLE_LORA\": \"true\",\n",
    "    \"OPTION_MAX_LORAS\": \"1\",\n",
    "    \"OPTION_MAX_CPU_LORAS\": \"2\",\n",
    "    \"OPTION_DTYPE\": \"fp16\",\n",
    "    \"OPTION_MAX_MODEL_LEN\": \"6000\"\n",
    "}\n",
    "\n",
    "lmi_model = sagemaker.Model(image_uri = inference_image_uri,\n",
    "                            env = env,\n",
    "                            role = role,\n",
    "                            name = model_name)\n",
    "\n",
    "\n",
    "lmi_model.deploy(instance_type = \"ml.g5.2xlarge\",\n",
    "                 initial_instance_count = 1,\n",
    "                 container_startup_health_check_timeout = 900,\n",
    "                 endpoint_name = endpoint_name,\n",
    "                 endpoint_type = sagemaker.enums.EndpointType.INFERENCE_COMPONENT_BASED,\n",
    "                 inference_component_name = base_inference_component_name,\n",
    "                 resources = ResourceRequirements(requests={\"num_accelerators\": 1, \"memory\": 4096, \"copies\": 1}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92e0abf-48a8-4c2c-9b13-952693b8fe4b",
   "metadata": {},
   "source": [
    "### Option 3: S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71882768-861c-4232-b1e7-1955864dc674",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.compute_resource_requirements.resource_requirements import ResourceRequirements\n",
    "\n",
    "model_id = \"s3://YOUR_BUCKET\"\n",
    "\n",
    "model_name = endpoint_name = sagemaker.utils.name_from_base(\"test\")\n",
    "base_inference_component_name = \"base-\" + model_name\n",
    "\n",
    "env = {\n",
    "    \"HF_MODEL_ID\": model_id,\n",
    "    \"OPTION_ROLLING_BATCH\": \"lmi-dist\",\n",
    "    \"OPTION_MAX_ROLLING_BATCH_SIZE\": \"16\",\n",
    "    \"OPTION_TENSOR_PARALLEL_DEGREE\": \"max\",\n",
    "    \"OPTION_ENABLE_LORA\": \"true\",\n",
    "    \"OPTION_MAX_LORAS\": \"1\",\n",
    "    \"OPTION_MAX_CPU_LORAS\": \"2\",\n",
    "    \"OPTION_DTYPE\": \"fp16\",\n",
    "    \"OPTION_MAX_MODEL_LEN\": \"6000\"\n",
    "}\n",
    "\n",
    "lmi_model = sagemaker.Model(image_uri = inference_image_uri,\n",
    "                            env = env,\n",
    "                            role = role,\n",
    "                            name = model_name)\n",
    "\n",
    "\n",
    "lmi_model.deploy(instance_type = \"ml.g5.2xlarge\",\n",
    "                 initial_instance_count = 1,\n",
    "                 container_startup_health_check_timeout = 900,\n",
    "                 endpoint_name = endpoint_name,\n",
    "                 endpoint_type = sagemaker.enums.EndpointType.INFERENCE_COMPONENT_BASED,\n",
    "                 inference_component_name = base_inference_component_name,\n",
    "                 resources = ResourceRequirements(requests={\"num_accelerators\": 1, \"memory\": 4096, \"copies\": 1}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761490c5-fd56-4251-88ca-4479a7254032",
   "metadata": {},
   "source": [
    "### View logs for the base inference component (and adapters after they're loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f5d691-7ca6-4947-8286-bade442fc5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "cw_path = urllib.parse.quote_plus(f'/aws/sagemaker/InferenceComponents/{base_inference_component_name}', safe='', encoding=None, errors=None)\n",
    "\n",
    "print(f'You can view your inference component logs here:\\n\\n https://{region}.console.aws.amazon.com/cloudwatch/home?region={region}#logsV2:log-groups/log-group/{cw_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff0f789-57fc-4016-8758-5b60ac7d797c",
   "metadata": {},
   "source": [
    "### Create the Inference Components (ICs) for the adapters\n",
    "\n",
    "In this example you’ll create a single adapter, but you could host up to hundreds of them per endpoint. They will need to be compressed and uploaded to S3.\n",
    "\n",
    "The adapter package has the following files at the root of the archive with no sub-folders:\n",
    "\n",
    "![](./images/adapter_files.png)\n",
    "\n",
    "For this example, an adapter was fine tuned using QLoRA and [Fully Sharded Data Parallel (FSDP)](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-v2.html) on the training split of the [ECTSum dataset](https://huggingface.co/datasets/mrSoul7766/ECTSum). Training took 21 minutes on a ml.p4d.24xlarge and cost ~$13 using current [on-demand pricing](https://aws.amazon.com/sagemaker/pricing/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ed24d7-5af5-4703-aaed-bc1ca2515de6",
   "metadata": {},
   "source": [
    "#### Compress and copy local adapter to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fc5c97-e2cd-4665-8174-33c718035f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "ectsum_adapter_filename = \"ectsum-adapter.tar.gz\"\n",
    "ectsum_adapter_local_path = \"~/sagemaker-genai-hosting-examples/genai-recipes/Multi-LoRA-Adapters/SM-Managed-Multi-Adapter-Deployment/adapters/ectsum-adapter/\"\n",
    "ectsum_adapter_s3_uri = f\"s3://{bucket}/adapters/{ectsum_adapter_filename}\"\n",
    "print(ectsum_adapter_s3_uri)\n",
    "\n",
    "!tar -cvzf {ectsum_adapter_filename} -C {ectsum_adapter_local_path} .\n",
    "\n",
    "!aws s3 cp ./{ectsum_adapter_filename} {ectsum_adapter_s3_uri}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a46534a-056a-4710-9cdd-d71bd42b883b",
   "metadata": {},
   "source": [
    "### Create ECTSum adapter inference component\n",
    "\n",
    "For each adapter you are going to deploy, you need to specify an `InferenceComponentName`, an `ArtifactUrl` with the S3 location of the adapter archive, and a `BaseInferenceComponentName` to create the connection between the base model IC and the new adapter ICs. You will repeat this process for each additional adapter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e64b79-605c-4888-9ec0-6e395550b13c",
   "metadata": {},
   "source": [
    "#### This step can take around 2 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ade7085-0d71-4ac3-a4cc-a96774a57f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ic1_adapter_name = f\"ic1-ectsum-{model_name}\"\n",
    "\n",
    "adapter_create_inference_component_response = sm_client.create_inference_component(\n",
    "    InferenceComponentName = ic1_adapter_name,\n",
    "    EndpointName = endpoint_name,\n",
    "    Specification={\n",
    "        \"BaseInferenceComponentName\": base_inference_component_name,\n",
    "        \"Container\": {\n",
    "            \"ArtifactUrl\": ectsum_adapter_s3_uri\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "sess.wait_for_inference_component(ic1_adapter_name)\n",
    "\n",
    "print(f\"\\nCreated Adapter inference component ARN: {adapter_create_inference_component_response['InferenceComponentArn']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a49ff5d-9ef8-4bd5-9537-bb057a6d941e",
   "metadata": {},
   "source": [
    "Look at base inference component logs again.\n",
    "\n",
    "It should show a line that looks like:\n",
    "\n",
    "`Registered adapter <ADAPTER_NAME> from /opt/ml/models/ ... successfully`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e3339c-b636-499b-95af-43498202a12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'You can view your inference component logs here:\\n\\n https://{region}.console.aws.amazon.com/cloudwatch/home?region={region}#logsV2:log-groups/log-group/{cw_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58be20f4-15ba-4659-b8a9-358e79e7c119",
   "metadata": {},
   "source": [
    "## Step 3: Invoking the Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfd6344-6ecc-4abd-90e5-1dac9c0c9d5d",
   "metadata": {},
   "source": [
    "First you will pull a random datapoint form the ECTSum test split. You'll use the `text` field to invoke the model and the `summary` filed to compare with ground truth later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b84715-2d62-4180-9ab9-4d257be94b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset_name = \"mrSoul7766/ECTSum\"\n",
    "\n",
    "test_dataset = load_dataset(dataset_name, split=\"test\")\n",
    "\n",
    "#due to GPU memory limitations on ml.g5.2xlarge, we have limited the max sequence length to 6000 tokens.\n",
    "#Some of the ECTSum samples are too large.\n",
    "#This code will loop until it gets a sample that is < 5500 so that inference does not throw errors.\n",
    "\n",
    "valid_test_value = False\n",
    "while not valid_test_value:\n",
    "    test_item = test_dataset.shuffle().select(range(1))\n",
    "    sample_size = len(test_item[\"text\"][0])/4\n",
    "    if sample_size > 5500:\n",
    "        print(f'sample size {sample_size} > 5500, fetching new sample.')\n",
    "    else:\n",
    "        print(f'sample_size {sample_size}')\n",
    "        valid_test_value = True\n",
    "\n",
    "ground_truth_response = test_item[\"summary\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b32ab6-38ec-4137-bf1c-0b30aa25f32a",
   "metadata": {},
   "source": [
    "Next you will build a prompt to invoke the model for earnings summarization, filling in the source text with a random item from the ECTSum dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a688e97f-92be-426b-9e34-cf90c8e51d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt =f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "                You are an AI assistant trained to summarize earnings calls. Provide a concise summary of the call, capturing the key points and overall context. Focus on quarter over quarter revenue, earnings per share, changes in debt, highlighted risks, and growth opportunities.\n",
    "                <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "                Summarize the following earnings call:\n",
    "\n",
    "                {test_item[\"text\"]}\n",
    "                <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df59c49c-e5d0-4567-b301-7d36bf4a70b9",
   "metadata": {},
   "source": [
    "### Plain base model with no adapters\n",
    "\n",
    "To test the base model, specify the `EndpointName` for the endpoint you created earlier and the name of the base inference component as `InferenceComponentName` along with your prompt and other inference parameters in the `Body` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993ac417-6a69-4ad9-a311-0bc7f0de5b72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "component_to_invoke = base_inference_component_name\n",
    "\n",
    "response_model = sm_runtime.invoke_endpoint(\n",
    "    EndpointName = endpoint_name,\n",
    "    InferenceComponentName = component_to_invoke,\n",
    "    Body = json.dumps(\n",
    "        {\n",
    "            \"inputs\": prompt,\n",
    "            \"parameters\": {\"do_sample\": True, \"top_p\": 0.9, \"temperature\": 0.9, \"max_new_tokens\": 125, \"temperature\":0.9}\n",
    "        }\n",
    "    ),\n",
    "    ContentType = \"application/json\",\n",
    ")\n",
    "\n",
    "base_response = json.loads(response_model[\"Body\"].read().decode(\"utf8\"))[\"generated_text\"]\n",
    "\n",
    "print(f'Ground Truth:\\n\\n {test_item[\"summary\"]}\\n\\n')\n",
    "print(f'Base Model Response:\\n\\n {base_response}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcad263-cf02-464b-ade8-6a9753a955b4",
   "metadata": {},
   "source": [
    "### Invoke ECTSum adapter\n",
    "\n",
    "To invoke the adapter, use the adapter inference component name in your `invoke_endpoint` call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500fed91-cdb1-4756-accb-621ca3c5fcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "component_to_invoke = ic1_adapter_name\n",
    "\n",
    "response_model = sm_runtime.invoke_endpoint(\n",
    "    EndpointName = endpoint_name,\n",
    "    InferenceComponentName = component_to_invoke,\n",
    "    Body = json.dumps(\n",
    "        {\n",
    "            \"inputs\": prompt,\n",
    "            \"parameters\": {\"do_sample\": True, \"top_p\": 0.9, \"temperature\": 0.9, \"max_new_tokens\": 125, \"temperature\":0.9}\n",
    "        }\n",
    "    ),\n",
    "    ContentType = \"application/json\",\n",
    ")\n",
    "\n",
    "adapter_response = json.loads(response_model[\"Body\"].read().decode(\"utf8\"))[\"generated_text\"]\n",
    "\n",
    "print(f'Ground Truth:\\n\\n {test_item[\"summary\"]}\\n\\n')\n",
    "print(f'Adapter Model Response:\\n\\n {adapter_response}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfac2aa-94b0-41db-9a41-e974c16ac1ba",
   "metadata": {},
   "source": [
    "### Compare outputs\n",
    "\n",
    "Compare the outputs of the base model and adapter to ground truth. In this test, notice that while the base model looks subjectively more visually attractive, the adapter response is significantly closer to ground truth; which is what you are looking for. This will be proven with metrics in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8480b7f7-0d1f-482e-bbfb-0bf2ea1e87a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Ground Truth:\\n\\n {test_item[\"summary\"][0]}\\n\\n')\n",
    "print(\"\\n----------------------------------\\n\")\n",
    "print(f'Base Model Response:\\n\\n {base_response}')\n",
    "print(\"\\n----------------------------------\\n\")\n",
    "print(f'Adapter Model Response:\\n\\n {adapter_response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8fab41-e8b0-45e3-b2ca-e3ac52762531",
   "metadata": {},
   "source": [
    "To validate the true adapter performance, you can use a tool like [fmeval](https://github.com/aws/fmeval) to run an evaluation of summarization accuracy. This will calculate the METEOR, ROUGE, and BertScore metrics for the adapter versus the base model. Doing so against the test split of ECTSum yields the following results:\n",
    "\n",
    "![](./images/fmeval-overall.png)\n",
    "\n",
    "The fine-tuned adapter shows a 59% increase in METEOR score, 159% increase in ROUGE score, and 8.6% in BertScore. The following diagram shows the frequency distribution of scores for the different metrics, with the adapter consistently scoring better more often in all metrics. \n",
    "\n",
    "Since the adapter is already loaded into GPU memory, model latency is largely unaffected, with only a difference of 2% between direct base model invocation and the adapter. If the adapter is loaded from CPU memory or disk, it will incur an cold start delay for the first load to GPU.\n",
    "\n",
    "![](./images/fmeval-histogram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7819727-29b5-4a66-9b42-96da93e5c7b6",
   "metadata": {},
   "source": [
    "## Step 4: Swapping adapters between GPU/CPU/disk\n",
    "\n",
    "To illustrate the swapping of adapters between different tiers, you will create 2 more adapter inference components. For simplicity, you can reuse the same adapter artifact code from earlier.\n",
    "\n",
    "When registering new adapters, the newest registration moves into GPU and if `OPTION_MAX_LORAS` is exceeded, will evict the least recently used (LRU) adapter to the CPU tier. If this move causes `OPTION_MAX_CPU_LORAS` to be exceeded, the LRU adapter from the CPU is then evicted to disk.\n",
    "\n",
    "Since you have set up `OPTION_MAX_LORAS` to `1` and `OPTION_MAX_CPU_LORAS` to `2` in the earlier section, the registration of IC2 in the next step will:\n",
    "- precache IC2 in CPU\n",
    "- load IC2 in GPU\n",
    "- evict IC1 to CPU\n",
    "\n",
    "The subsequent registration of IC3 will:\n",
    "- precache IC3 to CPU\n",
    "- evict IC1 from CPU (available from disk)\n",
    "- load IC3 in GPU\n",
    "- evict IC2 from GPU (already precached in CPU)\n",
    "\n",
    "Invoking adapters not currently in GPU will incur a cold start penalty on the first invocation. `max_new_tokens` is set to `1` on to focus on the cold start impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3b87e2-b952-4469-a65e-4bf6a7276d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ic2_adapter_name = f\"ic2-ectsum-{base_inference_component_name}\"\n",
    "\n",
    "adapter_create_inference_component_response = sm_client.create_inference_component(\n",
    "    InferenceComponentName = ic2_adapter_name,\n",
    "    EndpointName = endpoint_name,\n",
    "    Specification={\n",
    "        \"BaseInferenceComponentName\": base_inference_component_name,\n",
    "        \"Container\": {\n",
    "            \"ArtifactUrl\": ectsum_adapter_s3_uri\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "sess.wait_for_inference_component(ic2_adapter_name)\n",
    "\n",
    "print(f\"\\nCreated Adapter 2 inference component ARN: {adapter_create_inference_component_response['InferenceComponentArn']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6a39f2-80d4-4fc0-91b7-2aa114ee40a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ic3_adapter_name = f\"ic3-ectsum-{base_inference_component_name}\"\n",
    "\n",
    "adapter_create_inference_component_response = sm_client.create_inference_component(\n",
    "    InferenceComponentName = ic3_adapter_name,\n",
    "    EndpointName = endpoint_name,\n",
    "    Specification={\n",
    "        \"BaseInferenceComponentName\": base_inference_component_name,\n",
    "        \"Container\": {\n",
    "            \"ArtifactUrl\": ectsum_adapter_s3_uri\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "sess.wait_for_inference_component(ic3_adapter_name)\n",
    "\n",
    "print(f\"\\nCreated Adapter 3 inference component ARN: {adapter_create_inference_component_response['InferenceComponentArn']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144967e2-87ca-463d-9832-696135d056c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "#starting tier indexes 0 - GPU, 1 - CPU, 2 - Disk\n",
    "tiers = [ ic3_adapter_name, ic2_adapter_name, ic1_adapter_name ]\n",
    "\n",
    "cycles = 10\n",
    "\n",
    "invocation_order = [\n",
    "    ic3_adapter_name, #ic3 in GPU already.  GPU: ic3 CPU: ic2 DISK: ic1\n",
    "    ic2_adapter_name, #swap ic2 from CPU.   GPU: ic2 CPU: ic3 DISK: ic1\n",
    "    ic2_adapter_name, #ic2 is still in GPU. GPU: ic2 CPU: ic3 DISK: ic1\n",
    "    ic1_adapter_name, #swap ic1 from disk.  GPU: ic1 CPU: ic2 DISK: ic3\n",
    "    ic1_adapter_name, #ic1 is still in GPU. GPU: ic1 CPU: ic2 DISK: ic3\n",
    "    ic2_adapter_name, #swap ic2 from CPU.   GPU: ic2 CPU: ic1 DISK: ic3\n",
    "    ic3_adapter_name, #swap ic3 from disk.  GPU: ic3 CPU: ic2 DISK: ic1\n",
    "    # back to the starting configuration\n",
    "]\n",
    "\n",
    "no_swaps = []\n",
    "cpu_swaps = []\n",
    "disk_swaps = []\n",
    "\n",
    "swap_type = \"\"\n",
    "\n",
    "for cycle in range(cycles):\n",
    "    for invocation in invocation_order:\n",
    "\n",
    "        if invocation == base_inference_component_name or tiers.index(invocation) == 0:\n",
    "            #do nothing\n",
    "            swap_type = \"NONE\"\n",
    "            pass\n",
    "        elif tiers.index(invocation) == 1:\n",
    "            tiers[1] = tiers[0]\n",
    "            tiers[0] = invocation\n",
    "            swap_type = \"FROM_CPU\"\n",
    "        elif tiers.index(invocation) == 2:\n",
    "            tiers[2] = tiers[1]\n",
    "            tiers[1] = tiers[0]\n",
    "            tiers[0] = invocation\n",
    "            swap_type = \"FROM_DISK\"\n",
    "\n",
    "\n",
    "        component_to_invoke = invocation\n",
    "\n",
    "        start = time.time()*1000\n",
    "\n",
    "        response_model = sm_runtime.invoke_endpoint(\n",
    "            EndpointName = endpoint_name,\n",
    "            InferenceComponentName = component_to_invoke,\n",
    "            Body = json.dumps(\n",
    "                {\n",
    "                    \"inputs\": prompt,\n",
    "                    \"parameters\": {\"do_sample\": True, \"top_p\": 0.9, \"temperature\": 0.9, \"max_new_tokens\": 1, \"temperature\":0.9}\n",
    "                }\n",
    "            ),\n",
    "            ContentType = \"application/json\",\n",
    "        )\n",
    "\n",
    "        end = time.time()*1000\n",
    "\n",
    "        total = int(end - start)\n",
    "\n",
    "        if swap_type == \"NONE\":\n",
    "            no_swaps.append(total)\n",
    "        elif swap_type == \"FROM_CPU\":\n",
    "            cpu_swaps.append(total)\n",
    "        elif swap_type == \"FROM_DISK\":\n",
    "            disk_swaps.append(total)\n",
    "\n",
    "        print(f'call to [{invocation.split(\"-\")[0]}] {total} ms. swap: [{swap_type}] [ GPU: {tiers[0].split(\"-\")[0]} CPU: {tiers[1].split(\"-\")[0]} Disk: {tiers[2].split(\"-\")[0]} ]')\n",
    "\n",
    "no_swaps_count = len(no_swaps)\n",
    "no_swaps_avg = int(sum(no_swaps)/len(no_swaps))\n",
    "cpu_swaps_count = len(cpu_swaps)\n",
    "cpu_swaps_avg = int(sum(cpu_swaps)/len(cpu_swaps))\n",
    "disk_swaps_count = len(disk_swaps)\n",
    "disk_swaps_avg = int(sum(disk_swaps)/len(disk_swaps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085fb68e-0210-43d4-846e-78db1aaa4fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np_no_swaps = np.array(no_swaps)\n",
    "np_cpu_swaps = np.array(cpu_swaps)\n",
    "np_disk_swaps = np.array(disk_swaps)\n",
    "\n",
    "data = {\n",
    "    \"count\": [no_swaps_count, cpu_swaps_count, disk_swaps_count],\n",
    "    \"average\": [no_swaps_avg, cpu_swaps_avg, disk_swaps_avg],\n",
    "    \"+latency avg\": [0, cpu_swaps_avg-no_swaps_avg, disk_swaps_avg-no_swaps_avg],\n",
    "    \"+%latency avg\": [0, ((cpu_swaps_avg-no_swaps_avg)/no_swaps_avg)*100, ((disk_swaps_avg-no_swaps_avg)/no_swaps_avg)*100],\n",
    "    \"p50\": [int(np.percentile(np_no_swaps, 50)), int(np.percentile(np_cpu_swaps, 50)), int(np.percentile(np_disk_swaps, 50))],\n",
    "    \"p75\": [int(np.percentile(np_no_swaps, 75)), int(np.percentile(np_cpu_swaps, 75)), int(np.percentile(np_disk_swaps, 75))],\n",
    "    \"p99\": [int(np.percentile(np_no_swaps, 99)), int(np.percentile(np_cpu_swaps, 99)), int(np.percentile(np_disk_swaps, 99))]\n",
    "}\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data, index = [\"no swap\", \"cpu swap\", \"disk swap\"])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b79f1ab",
   "metadata": {},
   "source": [
    "If you were to run a similar test on 1000 cycles (7000 invocations), you'd see the following:\n",
    "\n",
    "![](./images/adapter-load-latency-1000.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb5a78f-de3d-4848-b3a1-659f4008acd6",
   "metadata": {},
   "source": [
    "## Step 5. Upload a new ECTSum adapter artifact and update the live adapter inference component\n",
    "\n",
    "Since adapters are managed as Inference Components, you can update them on a running endpoint. SageMaker handles the unloading/deregistering of the old adapter and loading/registering of the new adapter onto every base ICs on all of the instances that it is running on for this endpoint. To update an adapter IC, use the  update_inference_component  API and supply the existing IC name and the S3 path to the new compressed adapter archive. \n",
    "\n",
    "You can train a new adapter, or re-upload the existing adapter artifact to test this functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c2620d-8401-426f-9061-b32f7ef55b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ectsum_adapter_s3_uri = f\"s3://{bucket}/adapters/new-ectsum-adapter.tar.gz\"\n",
    "print(new_ectsum_adapter_s3_uri)\n",
    "\n",
    "!aws s3 cp ./ectsum-adapter.tar.gz {new_ectsum_adapter_s3_uri}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef03ba04-78ad-4281-9531-32c16033a4ae",
   "metadata": {},
   "source": [
    "#### This step can take around 5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c28ce6-28d0-466a-ae89-e44cb4c4ebcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "update_inference_component_response = sm_client.update_inference_component(\n",
    "    InferenceComponentName = ic1_adapter_name,\n",
    "    Specification={\n",
    "        \"Container\": {\n",
    "            \"ArtifactUrl\": new_ectsum_adapter_s3_uri\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "sess.wait_for_inference_component(ic1_adapter_name)\n",
    "\n",
    "print(f'\\nUpdated inference component adapter ARN: {update_inference_component_response[\"InferenceComponentArn\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8284ae38-cc9a-4a5a-8de0-7345e1cc00c8",
   "metadata": {},
   "source": [
    "If you view your inference component logs (link below), you will see log entries for the deregistration of the old adapter and the registration of the new one.\n",
    "\n",
    "You should see something similar to:\n",
    "`[INFO ] PyProcess - W-200-0d1e4741a42db26-stdout: [1,0]<stdout>:INFO::Unregistered adapter ic-ectsum-base-llama-3-1-8b-instruct-2024-11-25-20-41-07-401 successfully`\n",
    "\n",
    "`[INFO ] PyProcess - W-200-0d1e4741a42db26-stdout: [1,0]<stdout>:INFO::Registered adapter ic-ectsum-base-llama-3-1-8b-instruct-2024-11-25-20-41-07-401 from /opt/ml/models/container_340043819279-ic-ectsum-base-llama-3-1-8b-instruct-2024-11-25-20-41-07-401-1732570150851-MaeveWestworldService-1.0.9353.0 successfully`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e68a0fe-0bd8-4b8c-87c6-07ed1b9a1d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'You can view your inference component logs here:\\n\\n https://{region}.console.aws.amazon.com/cloudwatch/home?region={region}#logsV2:log-groups/log-group/{cw_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fbb4a9-1c67-433c-8329-b226d82043dd",
   "metadata": {},
   "source": [
    "### Retest with updated adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf29243c-423f-431b-a076-6ef8254ca1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "component_to_invoke = ic1_adapter_name\n",
    "\n",
    "response_model = sm_runtime.invoke_endpoint(\n",
    "    EndpointName = endpoint_name,\n",
    "    InferenceComponentName = component_to_invoke,\n",
    "    Body = json.dumps(\n",
    "        {\n",
    "            \"inputs\": prompt,\n",
    "            \"parameters\": {\"do_sample\": True, \"top_p\": 0.9, \"temperature\": 0.9, \"max_new_tokens\": 125, \"temperature\":0.9}\n",
    "        }\n",
    "    ),\n",
    "    ContentType = \"application/json\",\n",
    ")\n",
    "\n",
    "adapter_response = json.loads(response_model[\"Body\"].read().decode(\"utf8\"))[\"generated_text\"]\n",
    "\n",
    "print(f'Ground Truth:\\n\\n {test_item[\"summary\"][0]}\\n\\n')\n",
    "print(f'Updated Adapter Model Response:\\n\\n {adapter_response}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cd9042",
   "metadata": {},
   "source": [
    "## Step 6: Clean up the environment\n",
    "\n",
    "If you need to delete an adapter, call the `delete_inference_component` API with the IC name to remove it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b1abd0-eb91-4717-a116-a87f81654fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.delete_inference_component(ic1_adapter_name, wait = True)\n",
    "print(f'Adapter Component {ic1_adapter_name} deleted.')\n",
    "\n",
    "sess.delete_inference_component(ic2_adapter_name, wait = True)\n",
    "print(f'Adapter Component {ic2_adapter_name} deleted.')\n",
    "\n",
    "sess.delete_inference_component(ic3_adapter_name, wait = True)\n",
    "print(f'Adapter Component {ic3_adapter_name} deleted.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc65ab56-3290-4dde-84bb-4c8efcaf87d9",
   "metadata": {},
   "source": [
    "Deleting the base model IC will automatically delete the base IC and any associated adapter ICs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473d8b60-3aea-493b-b257-874a5e4c05b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.delete_inference_component(base_inference_component_name, wait = True)\n",
    "\n",
    "print(f'Base Component {base_inference_component_name} deleted.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c2a06b-7f59-4a12-be34-467e313d2ab0",
   "metadata": {},
   "source": [
    "Clean up the running endpoint and its configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d674b41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sess.delete_endpoint(endpoint_name)\n",
    "print(f'Endpoint {endpoint_name} deleted.')\n",
    "\n",
    "sess.delete_endpoint_config(endpoint_name)\n",
    "print(f'Endpoint Configuration {endpoint_name} deleted.')\n",
    "\n",
    "sess.delete_model(model_name)\n",
    "print(f'Model {model_name} deleted.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
