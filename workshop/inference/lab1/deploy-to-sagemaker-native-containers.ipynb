{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa6dc32e-95df-4c8b-9401-b5484fd5bf0b",
   "metadata": {},
   "source": [
    "# Deploying to SageMaker Native Containers\n",
    "\n",
    "Amazon SageMaker AI provides prebuilt Docker images that include deep learning frameworks and other dependencies needed for training and inference. \n",
    "\n",
    "With the SageMaker Python SDK, you can train and deploy models using these popular deep learning frameworks. For instructions on installing and using the SDK, see Amazon SageMaker Python SDK. The following is a list of the available frameworks:\n",
    "* TensorFlow\n",
    "* MXNet\n",
    "* PyTorch\n",
    "* Chainer\n",
    "* Hugging Face\n",
    "\n",
    "In this notebook, we will deploy Computer Vision models to a SageMaker prebuilt containers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9103276b-75e5-4c6a-b32d-6600721d2001",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d9761a1-a68d-42ab-9e63-1876b57a9b72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T15:58:37.816714Z",
     "iopub.status.busy": "2025-08-27T15:58:37.816392Z",
     "iopub.status.idle": "2025-08-27T15:58:37.901194Z",
     "shell.execute_reply": "2025-08-27T15:58:37.900514Z",
     "shell.execute_reply.started": "2025-08-27T15:58:37.816693Z"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from IPython.core.display import HTML\n",
    "import base64\n",
    "import json\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "endpoint_cleanup_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6e2c1c-da04-4754-aabf-d534d382a892",
   "metadata": {},
   "source": [
    "### Create helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3c201de-11bf-4493-b5f0-ee69fff98765",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T15:58:37.902607Z",
     "iopub.status.busy": "2025-08-27T15:58:37.902318Z",
     "iopub.status.idle": "2025-08-27T15:58:37.909996Z",
     "shell.execute_reply": "2025-08-27T15:58:37.909501Z",
     "shell.execute_reply.started": "2025-08-27T15:58:37.902587Z"
    }
   },
   "outputs": [],
   "source": [
    "def download_from_s3(key_filenames):\n",
    "    s3_bucket = f\"jumpstart-cache-prod-{region}\"\n",
    "    key_prefix = \"inference-notebook-assets\"\n",
    "    for key_filename in key_filenames:\n",
    "        s3.download_file(s3_bucket, f\"{key_prefix}/{key_filename}\", key_filename)\n",
    "\n",
    "\n",
    "def query_endpoint(endpoint_name, img, jumpstart_flag=True):\n",
    "    if jumpstart_flag:\n",
    "        content_type = \"application/x-image\"\n",
    "        accept = 'application/json;verbose'\n",
    "    else:\n",
    "        content_type = \"image/x-image\"\n",
    "        accept = 'application/json;'\n",
    "    client = boto3.client('runtime.sagemaker')\n",
    "    response = client.invoke_endpoint(EndpointName=endpoint_name, ContentType=content_type, Body=img, Accept=accept)\n",
    "    return response\n",
    "\n",
    "\n",
    "def parse_prediction(query_response, jumpstart_flag=True):\n",
    "    model_predictions = json.loads(query_response['Body'].read())\n",
    "    if jumpstart_flag:\n",
    "        predicted_label = model_predictions['predicted_label']\n",
    "        labels = model_predictions['labels']\n",
    "        probabilities = model_predictions['probabilities']\n",
    "        return predicted_label, probabilities, labels\n",
    "    else:\n",
    "        predicted_label = model_predictions[0][\"label\"]\n",
    "        labels = [i[\"label\"] for i in model_predictions]\n",
    "        probabilities = [i[\"score\"] for i in model_predictions]\n",
    "        return predicted_label, probabilities, labels\n",
    "\n",
    "\n",
    "def predict_top_k_labels(probabilities, labels,k, jumpstart_flag=True):\n",
    "    topk_prediction_ids = sorted(range(len(probabilities)), key=lambda index: probabilities[index], reverse=True)[:k]\n",
    "    topk_class_labels = \", \".join([labels[id] for id in topk_prediction_ids])\n",
    "    return topk_class_labels\n",
    "\n",
    "\n",
    "def display_images_with_prediction (endpoint_name, images: dict, jumpstart_flag = True):\n",
    "    for filename, img in images.items():\n",
    "        query_response = query_endpoint(endpoint_name, img, jumpstart_flag)\n",
    "        predicted_label, probabilities, labels = parse_prediction(query_response, jumpstart_flag)\n",
    "        top5_class_labels = predict_top_k_labels(probabilities, labels, 5, jumpstart_flag)\n",
    "        display(HTML(f'<img src={filename} alt={filename} align=\"left\" style=\"width: 250px;\"/>' \n",
    "                     f'<figcaption>Predicted Label is : {predicted_label}</figcaption>'\n",
    "                    f'<figcaption>Top-5 model predictions are: {top5_class_labels}</figcaption>'))\n",
    "\n",
    "\n",
    "def cleanup_endpoint(endpoint_name):\n",
    "    try:\n",
    "        sagemaker_client = boto3.client('sagemaker')\n",
    "    \n",
    "        endpoint_response = sagemaker_client.describe_endpoint(\n",
    "            EndpointName=endpoint_name\n",
    "        )\n",
    "        endpoint_config_name = endpoint_response['EndpointConfigName']\n",
    "    \n",
    "        sagemaker_client.delete_endpoint(\n",
    "                        EndpointName=endpoint_name\n",
    "                    )\n",
    "        sagemaker_client.delete_endpoint_config(\n",
    "                            EndpointConfigName=endpoint_config_name\n",
    "                        )\n",
    "    except:\n",
    "        print(\"Skipping deletion of \", endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7d1892-f412-4503-861a-e0507d199f37",
   "metadata": {},
   "source": [
    "### Prepare sample images for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2182dee2-5da8-45a9-92b7-96715e68bcfd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T15:58:37.910823Z",
     "iopub.status.busy": "2025-08-27T15:58:37.910575Z",
     "iopub.status.idle": "2025-08-27T15:58:38.476946Z",
     "shell.execute_reply": "2025-08-27T15:58:38.476404Z",
     "shell.execute_reply.started": "2025-08-27T15:58:37.910804Z"
    }
   },
   "outputs": [],
   "source": [
    "images_list = [\"cat.jpg\", \"dog.jpg\", \"boxer_dog.jpg\"]\n",
    "download_from_s3(key_filenames=images_list)\n",
    "images = {}\n",
    "for f in images_list:\n",
    "    with open(f, 'rb') as file: images[f] = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006be88c-c120-49d5-9270-92f87eb41e14",
   "metadata": {},
   "source": [
    "## 1. Deploy from SageMaker Jump Start\n",
    "Amazon SageMaker JumpStart provides a streamlined way to deploy and fine-tune machine learning models, including pre-trained foundation models and built-in algorithms. When you deploy a model through JumpStart, SageMaker handles the underlying infrastructure, including the use of Docker containers to package and run the model and its dependencies.\\n\n",
    "In this notebook, we will deploy EfficientNetV2-ImageNet21 model, which is an image classifier, to a CPU instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b0985f-e19b-429e-918e-a2f562d4e7f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T15:58:38.478284Z",
     "iopub.status.busy": "2025-08-27T15:58:38.477698Z",
     "iopub.status.idle": "2025-08-27T16:01:40.739477Z",
     "shell.execute_reply": "2025-08-27T16:01:40.738915Z",
     "shell.execute_reply.started": "2025-08-27T15:58:38.478258Z"
    }
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "from sagemaker import image_uris, model_uris, script_uris\n",
    "\n",
    "session = sagemaker.Session()\n",
    "\n",
    "model_id = \"tensorflow-ic-efficientnet-v2-imagenet21k-ft1k-m\"\n",
    "model_version = \"*\"\n",
    "instance_type = \"ml.c5.4xlarge\"\n",
    "\n",
    "# JumpStart\n",
    "model = JumpStartModel(\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    instance_type=instance_type,\n",
    "    sagemaker_session=session\n",
    ")\n",
    "\n",
    "# Deploy the model\n",
    "# The endpoint name will be automatically generated if not specified\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    wait=True\n",
    ")\n",
    "\n",
    "jumpstart_endpoint_name = predictor.endpoint_name\n",
    "endpoint_cleanup_list.append(jumpstart_endpoint_name)\n",
    "\n",
    "print(f\"Model deployed successfully!\")\n",
    "print(f\"Endpoint name: {jumpstart_endpoint_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53852b4-9396-4756-a9cf-01072934c514",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T16:01:40.741055Z",
     "iopub.status.busy": "2025-08-27T16:01:40.740781Z",
     "iopub.status.idle": "2025-08-27T16:01:45.842449Z",
     "shell.execute_reply": "2025-08-27T16:01:45.841920Z",
     "shell.execute_reply.started": "2025-08-27T16:01:40.741024Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "display_images_with_prediction(jumpstart_endpoint_name, images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1315be-91de-4dc2-afef-829451039471",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T19:01:20.277229Z",
     "iopub.status.busy": "2025-08-25T19:01:20.276571Z",
     "iopub.status.idle": "2025-08-25T19:01:20.280230Z",
     "shell.execute_reply": "2025-08-25T19:01:20.279467Z",
     "shell.execute_reply.started": "2025-08-25T19:01:20.277203Z"
    }
   },
   "source": [
    "## 2. Deploy Hugging Face models\n",
    "Amazon SageMaker AI lets customers train, fine-tune, and run inference using Hugging Face models for Natural Language Processing (NLP) on SageMaker AI. You can use Hugging Face for both training and inference. The following section provides information on Hugging Face models and includes reference material you can use to learn how to use Hugging Face with SageMaker AI.\n",
    "<br/><br/>\n",
    "This functionality is available through the development of Hugging Face [AWS Deep Learning Containers](https://docs.aws.amazon.com/deep-learning-containers/latest/devguide/what-is-dlc.html). These containers include Hugging Face Transformers, Tokenizers and the Datasets library, which allows you to use these resources for your training and inference jobs. For a list of the available Deep Learning Containers images, see [Available Deep Learning Containers Images](https://github.com/aws/deep-learning-containers/blob/master/available_images.md). These Deep Learning Containers images are maintained and regularly updated with security patches.\n",
    "<br/><br/>\n",
    "In this example, we will deploy ResNet-50 which is another image classification model, to Hugging Face container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bd8f73-7126-436c-8912-c22fa50056ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T16:01:45.843777Z",
     "iopub.status.busy": "2025-08-27T16:01:45.843494Z",
     "iopub.status.idle": "2025-08-27T16:04:48.061338Z",
     "shell.execute_reply": "2025-08-27T16:04:48.060774Z",
     "shell.execute_reply.started": "2025-08-27T16:01:45.843755Z"
    }
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "from  sagemaker.base_serializers import DataSerializer\n",
    "\n",
    "session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# Model parameters\n",
    "model_id = \"microsoft/resnet-50\"\n",
    "task = \"image-classification\"\n",
    "\n",
    "# Specify the container with HuggingFace transformers\n",
    "hub = {\n",
    "    'HF_MODEL_ID': model_id,\n",
    "    'HF_TASK': task,\n",
    "    'SAGEMAKER_CONTAINER_LOG_LEVEL': '20',\n",
    "    'SAGEMAKER_REGION': session.boto_region_name\n",
    "}\n",
    "\n",
    "# Create HuggingFace Model\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    env=hub,                      # Configuration for loading model from Hub\n",
    "    role=role,                    # IAM role with required permissions\n",
    "    transformers_version=\"4.26.0\", # Transformers version\n",
    "    pytorch_version=\"1.13.1\",     # PyTorch version\n",
    "    py_version=\"py39\",            # Python version\n",
    ")\n",
    "\n",
    "\n",
    "# Deploy the model\n",
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.c5.4xlarge\",  # You can change the instance type based on your needs\n",
    "    endpoint_name=f\"{model_id.replace('/','-')}-{session.boto_region_name}\",  # Optional: specify endpoint name\n",
    "    image_serializer=DataSerializer(content_type='image/x-image')\n",
    ")\n",
    "\n",
    "\n",
    "hf_endpoint_name = predictor.endpoint_name\n",
    "endpoint_cleanup_list.append(hf_endpoint_name)\n",
    "\n",
    "print(f\"Model deployed successfully!\")\n",
    "print(f\"Endpoint name: {hf_endpoint_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dc7066-910b-412e-8207-8733b2d80dfc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T16:04:48.062495Z",
     "iopub.status.busy": "2025-08-27T16:04:48.062118Z",
     "iopub.status.idle": "2025-08-27T16:04:48.961884Z",
     "shell.execute_reply": "2025-08-27T16:04:48.960947Z",
     "shell.execute_reply.started": "2025-08-27T16:04:48.062467Z"
    }
   },
   "outputs": [],
   "source": [
    "display_images_with_prediction(hf_endpoint_name, images, jumpstart_flag=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a715bfe-c65a-4e17-bc2c-ff82f4d0ccae",
   "metadata": {},
   "source": [
    "## Deploy to PyTorch Container\n",
    "In this example, we will deploy a model artifact to PyTorch container. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9043010-7a32-4596-a139-4f5d8929e2e2",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-27T16:04:48.962388Z",
     "iopub.status.idle": "2025-08-27T16:04:48.962632Z",
     "shell.execute_reply": "2025-08-27T16:04:48.962524Z",
     "shell.execute_reply.started": "2025-08-27T16:04:48.962513Z"
    }
   },
   "outputs": [],
   "source": [
    "# Download pretained weights\n",
    "\n",
    "import torchvision.models as models\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Download ResNet-50 with ImageNet pre-trained weights\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "os.makedirs(\"resnet50\", exist_ok=True)\n",
    "os.makedirs(\"resnet50/code\", exist_ok=True)\n",
    "# Save the model's state dictionary\n",
    "torch.save(model.state_dict(), 'resnet50/model.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14805050-0506-4317-8de0-b3651506e1c3",
   "metadata": {},
   "source": [
    "The inference.py file is essential for a SageMaker PyTorch container because it defines how your trained PyTorch model should be loaded and how it should perform inference when deployed as a SageMaker endpoint.\n",
    "<br/><br/>\n",
    "Key functions provided by inference.py:<br/>\n",
    "```model_fn(model_dir)```:<br/>\n",
    "This function is responsible for loading your serialized PyTorch model from the model_dir (where your model.tar.gz is extracted) into memory. This is where you would define the model architecture and load the saved weights.<br/>\n",
    "```input_fn(request_body, content_type)```:<br/>\n",
    "This function handles the deserialization of incoming inference requests. It takes the raw request body and its content type (e.g., JSON, CSV, NPY) and transforms it into a format suitable for your PyTorch model (e.g., a torch.Tensor).<br/>\n",
    "```predict_fn(input_object, model)```:<br/>\n",
    "This function performs the actual inference. It takes the preprocessed input from input_fn and the loaded model from model_fn, and then runs the prediction logic, returning the model's output.<br/>\n",
    "```output_fn(prediction, accept_type)```:<br/>\n",
    "This function serializes the model's predictions into the desired format for the client. It takes the output from predict_fn and the client's requested accept_type (e.g., JSON) and formats the response accordingly.<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f617e414-46f2-4ddd-aa22-4b832c32964f",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-27T16:04:48.963842Z",
     "iopub.status.idle": "2025-08-27T16:04:48.964084Z",
     "shell.execute_reply": "2025-08-27T16:04:48.963982Z",
     "shell.execute_reply.started": "2025-08-27T16:04:48.963972Z"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile resnet50/code/inference.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import io\n",
    "import os\n",
    "import json\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"\n",
    "    Load the PyTorch model from the model_dir.\n",
    "    \"\"\"\n",
    "    model = models.resnet50(pretrained=False) # Or load your specific ResNet50 architecture\n",
    "    # Load the state dictionary\n",
    "    with open(os.path.join(model_dir, \"resnet50\", 'model.pth'), 'rb') as f:\n",
    "        model.load_state_dict(torch.load(f))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def input_fn(request_body, content_type):\n",
    "    \"\"\"\n",
    "    Preprocess the input data for inference.\n",
    "    \"\"\"\n",
    "    if content_type == 'image/x-image':\n",
    "        return Image.open(io.BytesIO(request_body))\n",
    "    else:\n",
    "        raise Exception(f\"Unsupported content type: {content_type}\")\n",
    "\n",
    "def predict_fn(input_object, model):\n",
    "    \"\"\"\n",
    "    Perform inference on the input data.\n",
    "    \"\"\"\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    input_tensor = preprocess(input_object).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "    return output\n",
    "\n",
    "\n",
    "# The output_fn takes the prediction result and the requested content type\n",
    "def output_fn(prediction, accept_type):\n",
    "    \"\"\"\n",
    "    Formats the prediction output.\n",
    "    \"\"\"\n",
    "    # Assuming `prediction` is a tensor of shape (1, num_classes)\n",
    "    # Get the class with the highest probability\n",
    "    score, predicted_class = torch.max(prediction, 1)\n",
    "\n",
    "    # Get the human-readable class label (assuming you have a mapping)\n",
    "    # For a real-world scenario, you would load this from a file\n",
    "    # during model_fn.\n",
    "    # For this example, we will just return the class index.\n",
    "    imagenet_classes = models.ResNet50_Weights.DEFAULT.meta[\"categories\"]\n",
    "    output_dict = {\n",
    "        'label': imagenet_classes[predicted_class.item()],\n",
    "        'score': score.item()\n",
    "    }\n",
    "\n",
    "    if accept_type == \"application/json\":\n",
    "        return json.dumps([output_dict]), accept_type\n",
    "    else:\n",
    "        # Default to JSON for other unsupported types\n",
    "        return json.dumps([output_dict]), \"application/json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0549031-151f-45e0-837d-2fb06b383680",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-27T16:04:48.964777Z",
     "iopub.status.idle": "2025-08-27T16:04:48.965010Z",
     "shell.execute_reply": "2025-08-27T16:04:48.964906Z",
     "shell.execute_reply.started": "2025-08-27T16:04:48.964897Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a model.tar.gz file and upload to S3\n",
    "!tar -czvf model.tar.gz resnet50/*\n",
    "resnet50_model_data = sess.upload_data(\n",
    "        path=\"model.tar.gz\", bucket=sess.default_bucket(), key_prefix=\"model/pytorch\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33c33e7-d95a-42f2-aed4-6302dcdd1e87",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-27T16:04:48.966111Z",
     "iopub.status.idle": "2025-08-27T16:04:48.966346Z",
     "shell.execute_reply": "2025-08-27T16:04:48.966248Z",
     "shell.execute_reply.started": "2025-08-27T16:04:48.966238Z"
    }
   },
   "outputs": [],
   "source": [
    "print(resnet50_model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da3c564-291a-4826-9d3e-ac1e9915f1a4",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-27T16:04:48.967156Z",
     "iopub.status.idle": "2025-08-27T16:04:48.967389Z",
     "shell.execute_reply": "2025-08-27T16:04:48.967290Z",
     "shell.execute_reply.started": "2025-08-27T16:04:48.967280Z"
    }
   },
   "outputs": [],
   "source": [
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "model = PyTorchModel(\n",
    "    entry_point=\"inference.py\",\n",
    "    source_dir=\"resnet50/code\",\n",
    "    role=role,\n",
    "    model_data=resnet50_model_data,\n",
    "    framework_version=\"1.13.1\",\n",
    "    py_version=\"py39\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acd0e3d-5c9d-4796-a289-1425223e2428",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-27T16:04:48.967890Z",
     "iopub.status.idle": "2025-08-27T16:04:48.968135Z",
     "shell.execute_reply": "2025-08-27T16:04:48.968010Z",
     "shell.execute_reply.started": "2025-08-27T16:04:48.968001Z"
    }
   },
   "outputs": [],
   "source": [
    "PyTorchModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa44434-9dd8-4e07-ab8f-511450c315ce",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-27T16:04:48.968986Z",
     "iopub.status.idle": "2025-08-27T16:04:48.969235Z",
     "shell.execute_reply": "2025-08-27T16:04:48.969134Z",
     "shell.execute_reply.started": "2025-08-27T16:04:48.969124Z"
    }
   },
   "outputs": [],
   "source": [
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.base_serializers import DataSerializer\n",
    "\n",
    "# set local_mode to False if you want to deploy on a remote\n",
    "# SageMaker instance\n",
    "\n",
    "local_mode = False\n",
    "\n",
    "if local_mode:\n",
    "    instance_type = \"local\"\n",
    "else:\n",
    "    instance_type = \"ml.m5.xlarge\"\n",
    "\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    serializer=DataSerializer(content_type='image/x-image'),\n",
    "    deserializer=JSONDeserializer(),\n",
    ")\n",
    "\n",
    "pt_endpoint_name = predictor.endpoint_name\n",
    "endpoint_cleanup_list.append(pt_endpoint_name)\n",
    "\n",
    "print(f\"Model deployed successfully!\")\n",
    "print(f\"Endpoint name: {pt_endpoint_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c287d875-82a4-4df2-ad29-cd241c870f73",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-27T16:04:48.969741Z",
     "iopub.status.idle": "2025-08-27T16:04:48.970057Z",
     "shell.execute_reply": "2025-08-27T16:04:48.969952Z",
     "shell.execute_reply.started": "2025-08-27T16:04:48.969942Z"
    }
   },
   "outputs": [],
   "source": [
    "# Please note that top-5 list is not implemented in the inference.py file.\n",
    "from IPython.display import display, Markdown\n",
    "display(Markdown(\"### Please note that top-5 list is not implemented in the inference.py for simplicity of the lab.\"))\n",
    "display_images_with_prediction(pt_endpoint_name, images, jumpstart_flag=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42acfbc1-76c7-4b7c-afc9-15e426832522",
   "metadata": {},
   "source": [
    "# Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb67ec50-307e-4232-8812-3dd39f5216bf",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-27T16:04:48.970894Z",
     "iopub.status.idle": "2025-08-27T16:04:48.971312Z",
     "shell.execute_reply": "2025-08-27T16:04:48.971141Z",
     "shell.execute_reply.started": "2025-08-27T16:04:48.971125Z"
    }
   },
   "outputs": [],
   "source": [
    "for endpoint_name in endpoint_cleanup_list:\n",
    "    cleanup_endpoint(endpoint_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
