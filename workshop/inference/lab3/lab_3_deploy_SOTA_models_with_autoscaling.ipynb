{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "660292bc-cea8-4bb9-965c-684ef3368679",
   "metadata": {},
   "source": [
    "# Deploy Multiple SOTA LLMs on a single endpoint with Scale-to-Zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09668c9e-4794-4aa0-8a12-6fb134a66f8a",
   "metadata": {},
   "source": [
    "In this notebook, you will learn how to leverage Fast Model Loader to dramatically improve model loading times of SOTA models, how to configure model level autoscaling, including how you can scale in your SageMaker endpoint to zero instances during idle periods, eliminating the previous requirement of maintaining at least one running instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c926cc-d7b0-4eb7-8ac7-6d6d84c57c7d",
   "metadata": {},
   "source": [
    "## Set up\n",
    "\n",
    "- Fetch and import dependencies\n",
    "- Initialize SageMaker environment and required clients to access AWS services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4789fd-7ab2-469f-b335-bdeac2ae39be",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sagemaker --upgrade --quiet --no-warn-conflicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ffdf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import sys\n",
    "import boto3\n",
    "import logging\n",
    "import time\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "print(sagemaker.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d9a965-3f52-45cc-a891-3218bde16066",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    boto_region = boto3.Session().region_name\n",
    "    sagemaker_session = sagemaker.session.Session(boto_session=boto3.Session(region_name=boto_region))\n",
    "    role = sagemaker.get_execution_role()\n",
    "    sagemaker_client = boto3.client(\"sagemaker\", region_name=boto_region)\n",
    "    model_bucket = sagemaker_session.default_bucket()\n",
    "    \n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2419128f-f128-4848-92b2-f8f44349aed4",
   "metadata": {},
   "source": [
    "## Setup your SageMaker Real-time Endpoint "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfd40ef-abb4-42c9-b88f-a5e765d5abed",
   "metadata": {},
   "source": [
    "### Create a SageMaker endpoint configuration\n",
    "\n",
    "There are a few parameters we want to setup for our endpoint. We begin by creating the endpoint configuration and setting MinInstanceCount to 0. This allows the endpoint to scale in all the way down to zero instances when not in use, see the [blog](https://aws.amazon.com/blogs/machine-learning/unlock-cost-savings-with-the-new-scale-down-to-zero-feature-in-amazon-sagemaker-inference/). In addition we will use Managed Instance Scaling which allows SageMaker to scale the number of instances based on the requirements of the scaling of your inference components. Lastly, we set *RoutingStrategy* for the endpoint to optimally tune how to route requests to instances and inference components for the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da78e05-4db1-4e99-8247-c1824c517a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set an unique name for our endpoint config\n",
    "endpoint_config_name = sagemaker.utils.name_from_base(\"workshop-lab-3\")\n",
    "print(f\"Endpoint config name: {endpoint_config_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17780f9d-3c0a-4ece-99ec-689c5d561a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure variant name and instance type for hosting\n",
    "variant_name = \"AllTraffic\"\n",
    "gpu_instance_type = \"ml.g5.2xlarge\"\n",
    "\n",
    "model_data_download_timeout_in_seconds = 3600\n",
    "container_startup_health_check_timeout_in_seconds = 3600\n",
    "\n",
    "min_instance_count = 0 # Minimum instance must be set to 0\n",
    "max_instance_count = 2\n",
    "\n",
    "sagemaker_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": variant_name,\n",
    "            \"InstanceType\": gpu_instance_type,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelDataDownloadTimeoutInSeconds\": model_data_download_timeout_in_seconds,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": container_startup_health_check_timeout_in_seconds,\n",
    "            \"ManagedInstanceScaling\": {\n",
    "                \"Status\": \"ENABLED\",\n",
    "                \"MinInstanceCount\": min_instance_count,\n",
    "                \"MaxInstanceCount\": max_instance_count,\n",
    "            },\n",
    "            \"RoutingConfig\": {\"RoutingStrategy\": \"LEAST_OUTSTANDING_REQUESTS\"},\n",
    "        }\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9444afc2-029e-4028-9176-0c6091f8aabe",
   "metadata": {},
   "source": [
    "### Create the SageMaker endpoint\n",
    "Next, we create our endpoint using the above endpoint config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b795f8-e8e0-4374-a78e-2cbc0f57ca85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a unique endpoint name\n",
    "endpoint_name = sagemaker.utils.name_from_base(\"workshop-lab-3\")\n",
    "print(f\"Endpoint name: {endpoint_name}\")\n",
    "\n",
    "sagemaker_client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73358ee7-ba3e-4e85-87d5-f5a77ffed99b",
   "metadata": {},
   "source": [
    "#### We wait for our endpoint to go InService. This step can take ~4 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53335704-5989-43d8-a7af-875444d166e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session.wait_for_endpoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468267be-736e-4ffd-a8dc-7a7994acb63d",
   "metadata": {},
   "source": [
    "## Create Model Builder\n",
    "\n",
    "We'll make use of the ModelBuilder class to prepare and package the model inference components. In this example, we're using the Meta-Llama-3-8B-Instruct SageMaker JumpStart.\n",
    "\n",
    "Key configurations:\n",
    "- Model: Meta-Llama-3-8B-Instruct\n",
    "- Schema Builder: Defines input/output format\n",
    "- Set LMI image with fast model loader support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227073a7-2d4e-425e-a7f7-5c58659238c2",
   "metadata": {},
   "source": [
    "#### Select the container image to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f196bf-f1ac-4586-9426-9f6e2b7de828",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTAINER_VERSION = \"0.32.0-lmi14.0.0-cu126\"\n",
    "inference_image = \"763104351884.dkr.ecr.{}.amazonaws.com/djl-inference:{}\".format(sagemaker_session.boto_session.region_name, CONTAINER_VERSION)\n",
    "\n",
    "gpu_instance_type=\"ml.g5.2xlarge\"\n",
    "\n",
    "\n",
    "print(f\"Inference Image --> {inference_image}\")\n",
    "print(f\"GPU instance --> {gpu_instance_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7563cb8-de04-4e2e-a586-b56d90e67342",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serve.builder.model_builder import ModelBuilder\n",
    "from sagemaker.serve.builder.schema_builder import SchemaBuilder\n",
    "import logging\n",
    "\n",
    "prompt = \"Falcons are\"\n",
    "response = \"Falcons are small to medium-sized birds of prey related to hawks and eagles.\"\n",
    "\n",
    "model_id = \"meta-textgeneration-llama-3-1-8b-instruct\"\n",
    "model_name = sagemaker.utils.name_from_base(\"workshop-lab-3-llama\")\n",
    "\n",
    "llama_model_builder = ModelBuilder(\n",
    "    model=model_id,\n",
    "    name=model_name,\n",
    "    role_arn=role,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    schema_builder=SchemaBuilder(sample_input=prompt, sample_output=response),\n",
    "    log_level=logging.WARN\n",
    ")\n",
    "\n",
    "output_path = f\"s3://{model_bucket}/ws-llama3-1-8b-instruct/sharding\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a60edb3-579b-414e-8c16-ae3080f19f5f",
   "metadata": {},
   "source": [
    "## Model Optimization with Fast Model Loader\n",
    "\n",
    "Fast Model Loader streams model weights directly from S3 to GPU, bypassing traditional loading steps.\n",
    "In internal testing, this approach has shown up to 15x faster model loading compared to standard deployment.\n",
    "\n",
    "> **Note** : The optimization process may take a while to complete. The optimized model will be stored in the specified S3 output path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7657673-dcba-4da4-a05d-213432b22a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_model_builder.optimize(\n",
    "    instance_type=gpu_instance_type, \n",
    "    accept_eula=True, \n",
    "    output_path=output_path,\n",
    "    env_vars={\n",
    "        \"OPTION_MAX_MODEL_LEN\": \"12384\",\n",
    "    },\n",
    "    sharding_config={\n",
    "            \"Image\": inference_image,\n",
    "            \"OverrideEnvironment\": {\n",
    "                \"OPTION_TENSOR_PARALLEL_DEGREE\": \"1\" # Number of GPU available on the instance\n",
    "            }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0565b2-b01b-4c95-a144-fa4178bd808e",
   "metadata": {},
   "source": [
    "## Build the optimized model\n",
    "After optimization, we'll build the final model artifacts and get them ready to deploy to a SageMaker endpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a59ca66-a29c-447b-bf7b-3ac47ea7bc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_model = llama_model_builder.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e58f52-11f3-4c9f-afaf-9c8f4b81b8e6",
   "metadata": {},
   "source": [
    "## Deploy model using Inference Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38979b2b-7559-4d5a-87cc-283059b45efa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sagemaker.enums import EndpointType\n",
    "from sagemaker.enums import RoutingStrategy\n",
    "from sagemaker.compute_resource_requirements.resource_requirements import ResourceRequirements\n",
    "\n",
    "\n",
    "resources = ResourceRequirements(\n",
    "    requests = {\n",
    "        \"num_accelerators\": 1, # Number of accelerators required\n",
    "        \"memory\": 10409,  # Minimum memory required in Mb (required)\n",
    "        \"copies\": 1,\n",
    "    }\n",
    ")\n",
    "\n",
    "llama_model.deploy(\n",
    "    endpoint_name=endpoint_name,\n",
    "    accept_eula=True,\n",
    "    initial_instance_count=1,\n",
    "    instance_type=gpu_instance_type,\n",
    "    endpoint_type=EndpointType.INFERENCE_COMPONENT_BASED,\n",
    "    resources=resources,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106182c4-91a2-4586-9b67-12384cb6fad4",
   "metadata": {},
   "source": [
    "#### Test the model with a sample prompt\n",
    "Now we can invoke our endpoint with sample input to test its functionality and see the model's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899fae67-cc1a-45ec-8daa-9bf70fb38f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.predictor import retrieve_default \n",
    "\n",
    "endpoint_name = llama_model.endpoint_name \n",
    "llama_inference_component_name = llama_model.inference_component_name\n",
    "\n",
    "llama_predictor = retrieve_default(endpoint_name, inference_component_name=llama_inference_component_name) \n",
    "\n",
    "payload = {\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\"type\": \"text\", \"text\": \"What is deep learning?\"}\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "    \"max_tokens\": 100,\n",
    "    \"temperature\": 0.6,\n",
    "    \"top_p\": 0.9,\n",
    "}\n",
    "\n",
    "\n",
    "response = llama_predictor.predict(payload)\n",
    "print(response['choices'][0]['message']['content'])\n",
    "\n",
    "# Print usage statistics\n",
    "print(\"=== Token Usage ===\")\n",
    "usage = response['usage']\n",
    "print(f\"Prompt Tokens: {usage['prompt_tokens']}\")\n",
    "print(f\"Completion Tokens: {usage['completion_tokens']}\")\n",
    "print(f\"Total Tokens: {usage['total_tokens']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f00b512-7474-4497-afba-d7ca62a4de78",
   "metadata": {},
   "source": [
    "## Configure per model Autoscaling and Enable Cost-Saving Autoscaling with Scale-to-Zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ac2748-56a6-456d-afe4-f7af03149efc",
   "metadata": {},
   "source": [
    "### Scaling policies "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bde477-bb35-4ece-a03f-330ca2065883",
   "metadata": {},
   "source": [
    "Once our models are deployed and InService, we can then add the necessary scaling policies:\n",
    "\n",
    "* A [target tracking](https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-target-tracking.html) policy that can scale in the copy count for our inference component model copies to zero, and from 1 to n. \n",
    "* A [step scaling policy](https://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-step-scaling-policies.html) policy that will allow the endpoint to scale out from zero.\n",
    "\n",
    "These policies work together to provide cost-effective scaling - the endpoint can scale to zero when idle and automatically scale out as needed to handle incoming requests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72a8456-8815-4195-8de9-6ca93f4e3704",
   "metadata": {},
   "source": [
    "### Autoscaling Helper Function\n",
    "\n",
    "> **Note** ofr the target tracking policy, Application Auto Scaling creates two CloudWatch alarms per scaling target. The first triggers scale-out actions after 30 seconds (using 3 sub-minute data point), while the second triggers scale-in after 15 minutes (using 90 sub-minute data points). The time to trigger the scaling action is usually 1–2 minutes longer than those minutes because it takes time for the endpoint to publish metrics to CloudWatch, and it also takes time for AutoScaling to react. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8102a6-4156-40c8-91af-901e019a4e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "aas_client = sagemaker_session.boto_session.client(\"application-autoscaling\")\n",
    "cloudwatch_client = sagemaker_session.boto_session.client(\"cloudwatch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9beaef9-cb51-434c-9972-1d38ba4c9b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_autoscaling(\n",
    "    inference_component_name, \n",
    "    min_capacity=0, \n",
    "    max_capacity=2, \n",
    "    target_requests_per_copy=3\n",
    "):\n",
    "    \"\"\"\n",
    "    Configure complete autoscaling: scale-to-zero + scale-out\n",
    "    \n",
    "    Args:\n",
    "        inference_component_name (str): Name of the inference component to configure\n",
    "        min_capacity (int): Minimum number of model copies (set to 0 for scale-to-zero)\n",
    "        max_capacity (int): Maximum number of model copies this component can scale to\n",
    "        target_requests_per_copy (int): Concurrent requests threshold per copy before scaling out\n",
    "    \"\"\"\n",
    "    \n",
    "    resource_id = f\"inference-component/{inference_component_name}\"\n",
    "    \n",
    "    # 1. Register autoscaling target\n",
    "    aas_client.register_scalable_target(\n",
    "        ServiceNamespace=\"sagemaker\",\n",
    "        ResourceId=resource_id,\n",
    "        ScalableDimension=\"sagemaker:inference-component:DesiredCopyCount\",\n",
    "        MinCapacity=min_capacity,  # Configurable minimum (0 enables scale-to-zero)\n",
    "        MaxCapacity=max_capacity,  # Configurable maximum\n",
    "    )\n",
    "    \n",
    "    # 2. Target tracking policy (scales min_capacity+1 → max_capacity)\n",
    "    aas_client.put_scaling_policy(\n",
    "        PolicyName=f\"target-tracking-{inference_component_name}\",\n",
    "        PolicyType=\"TargetTrackingScaling\",\n",
    "        ServiceNamespace=\"sagemaker\",\n",
    "        ResourceId=resource_id,\n",
    "        ScalableDimension=\"sagemaker:inference-component:DesiredCopyCount\",\n",
    "        TargetTrackingScalingPolicyConfiguration={\n",
    "            \"PredefinedMetricSpecification\": {\n",
    "                \"PredefinedMetricType\": \"SageMakerInferenceComponentConcurrentRequestsPerCopyHighResolution\",\n",
    "            },\n",
    "            \"TargetValue\": target_requests_per_copy,\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    # 3. Step scaling policy (only needed if min_capacity = 0)\n",
    "    step_policy_response = aas_client.put_scaling_policy(\n",
    "        PolicyName=f\"step-scaling-{inference_component_name}\",\n",
    "        PolicyType=\"StepScaling\",\n",
    "        ServiceNamespace=\"sagemaker\",\n",
    "        ResourceId=resource_id,\n",
    "        ScalableDimension=\"sagemaker:inference-component:DesiredCopyCount\",\n",
    "        StepScalingPolicyConfiguration={\n",
    "            \"AdjustmentType\": \"ChangeInCapacity\",\n",
    "            \"MetricAggregationType\": \"Maximum\",\n",
    "            \"Cooldown\": 60,\n",
    "            \"StepAdjustments\": [{\"MetricIntervalLowerBound\": 0, \"ScalingAdjustment\": 1}]\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    # CloudWatch alarm for scale-out from zero\n",
    "    cloudwatch_client.put_metric_alarm(\n",
    "        AlarmName=f\"scale-from-zero-{inference_component_name}\",\n",
    "        AlarmActions=[step_policy_response['PolicyARN']],\n",
    "        MetricName='NoCapacityInvocationFailures',\n",
    "        Namespace='AWS/SageMaker',\n",
    "        Statistic='Maximum',\n",
    "        Dimensions=[{'Name': 'InferenceComponentName', 'Value': inference_component_name}],\n",
    "        Period=30,\n",
    "        EvaluationPeriods=1,\n",
    "        DatapointsToAlarm=1,\n",
    "        Threshold=1,\n",
    "        ComparisonOperator='GreaterThanOrEqualToThreshold',\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ Autoscaling configured for {inference_component_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74c210d-84f5-4c66-a7b5-52d66a1f70cd",
   "metadata": {},
   "source": [
    "### Setup autoscaling for LLama model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8512b92c-87e6-41f4-a7a3-d7f10cf3a0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the IC name\n",
    "llama_inference_component_name = llama_model.inference_component_name\n",
    "print(f\"LLama inference component: {llama_inference_component_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1d6eee-b134-49e5-9d1e-296cf2f69464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply autoscaling configuration\n",
    "setup_autoscaling(llama_inference_component_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c788cb3-ce02-475e-a97c-155573fde79a",
   "metadata": {},
   "source": [
    "## Testing scale to Zero behaviour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceeda9a3-8970-4dde-888b-cc313ce832b2",
   "metadata": {},
   "source": [
    "### IC copy count scales in to zero\n",
    "We'll pause for a few minutes without making any invocations to our model. Based on our target tracking policy, when our SageMaker endpoint doesn't receive requests for about 10 to 15 minutes, it will automatically scale down to zero the number of model copies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b7d95a-f238-4bfa-a0cc-6a8ce4067c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(900)\n",
    "start_time = time.time()\n",
    "while True:\n",
    "    desc = sagemaker_client.describe_inference_component(InferenceComponentName=llama_inference_component_name)\n",
    "    status = desc[\"InferenceComponentStatus\"]\n",
    "    print(status)\n",
    "    sys.stdout.flush()\n",
    "    if status in [\"InService\", \"Failed\"]:\n",
    "        break\n",
    "    time.sleep(30)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nTotal time taken: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")\n",
    "\n",
    "desc = sagemaker_client.describe_inference_component(InferenceComponentName=llama_inference_component_name)\n",
    "print(desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce47a8a7-e19f-4dca-aaac-91a31576fffb",
   "metadata": {},
   "source": [
    "### Endpoint's instances scale in to zero\n",
    "\n",
    "After 10 additional minutes of inactivity, SageMaker automatically terminates all underlying instances of the endpoint, eliminating all associated costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d447c5f4-7fa0-4fbb-9dbd-c470fe2bc8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after 10mins instances will scale down to 0\n",
    "time.sleep(600)\n",
    "# verify whether CurrentInstanceCount is zero\n",
    "sagemaker_session.wait_for_endpoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5b9dd1-270e-43cb-863b-5d7ea62f3746",
   "metadata": {},
   "source": [
    "#### Invoke llama model with a sample prompt\n",
    "\n",
    "If we try to invoke our endpoint while instances are scaled down to zero, we get a validation error: `An error occurred (ValidationError) when calling the InvokeEndpoint operation: Inference Component has no capacity to process this request. ApplicationAutoScaling may be in-progress (if configured) or try to increase the capacity by invoking UpdateInferenceComponentRuntimeConfig API.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcce608-9cc0-4b96-88f9-304ad0edfb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload={\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"What is deep learning?\"}\n",
    "    ],\n",
    "    \"max_tokens\": 100,\n",
    "}\n",
    "try:\n",
    "    response = llama_predictor.predict(payload)\n",
    "    print(response['choices'][0]['message']['content'])\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"   Reason: {str(e)}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c742af-0cbe-4b5d-ab4d-6fa7dd8f6eb1",
   "metadata": {},
   "source": [
    "### Scale out from zero kicks in\n",
    "However, after 1 minutes our step scaling policy should kick in. SageMaker will then start provisioning a new instance and deploy our inference component model copy to handle requests. This demonstrates the endpoint's ability to automatically scale out from zero when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a76297f-0bc3-4a9d-8dec-6c9ddb7dfe8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "while True:\n",
    "    desc = sagemaker_client.describe_inference_component(InferenceComponentName=llama_inference_component_name)\n",
    "    status = desc[\"InferenceComponentStatus\"]\n",
    "    print(status)\n",
    "    sys.stdout.flush()\n",
    "    if status in [\"InService\", \"Failed\"]:\n",
    "        break\n",
    "    time.sleep(30)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nTotal time taken: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d2978a-798b-4a2b-9f7a-fc87833d7176",
   "metadata": {},
   "source": [
    "#### verify that our endpoint has succesfully scaled out from zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dbe8e8-9706-4492-ab70-3929754b9554",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload={\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"What is deep learning?\"}\n",
    "    ],\n",
    "    \"max_tokens\": 100,\n",
    "}\n",
    "try:\n",
    "    response = llama_predictor.predict(payload)\n",
    "    print(response['choices'][0]['message']['content'])\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"   Reason: {str(e)}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7197f8d0-c960-4e2a-9ec6-6d276bdbd3c5",
   "metadata": {},
   "source": [
    "## Testing scaling behavior 1->n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd84134c-4c8b-43b8-a7b0-73ca327c782d",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf85641-d984-49a4-80f9-037122c1affe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def make_prediction(predictor, payload, request_id):\n",
    "    \"\"\"Make a single prediction and return results with timing\"\"\"\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        response = predictor.predict(payload)\n",
    "        duration = time.time() - start_time\n",
    "        return {\n",
    "            'request_id': request_id,\n",
    "            'success': True,\n",
    "            'duration': duration,\n",
    "            'response': response['choices'][0]['message']['content'][:50] + \"...\" if 'choices' in response else response[0]['generated_text'][:50] + \"...\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        duration = time.time() - start_time\n",
    "        return {\n",
    "            'request_id': request_id,\n",
    "            'success': False,\n",
    "            'duration': duration,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "def get_current_copy_count(inference_component_name):\n",
    "    \"\"\"Get the current number of inference component copies\"\"\"\n",
    "    try:\n",
    "        response = sagemaker_client.describe_inference_component(\n",
    "            InferenceComponentName=inference_component_name\n",
    "        )\n",
    "        return response['RuntimeConfig']['CurrentCopyCount']\n",
    "    except Exception as e:\n",
    "        return \"Error\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74e85a9-d038-40f0-b760-2aebad0fd6d2",
   "metadata": {},
   "source": [
    "### Autoscaling demonstration\n",
    "\n",
    "In the following code we will run 6 iterations with 6 concurrents request each.\n",
    "\n",
    "What we will observe:\n",
    "- Initial low component count\n",
    "- Concurrent load being applied\n",
    "- Component count increasing as autoscaling kicks in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4167e9de-146f-4cde-9c97-272617990912",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 6\n",
    "requests_per_iteration = 6\n",
    "wait_between_iterations = 10\n",
    "\n",
    "print(\"Starting Inference load\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for iteration in range(1, num_iterations + 1):\n",
    "    print(f\"\\nIteration {iteration}/{num_iterations}\")\n",
    "    \n",
    "    # Show current inference component count\n",
    "    current_copies = get_current_copy_count(llama_predictor.component_name)\n",
    "    print(f\"Current Inference Components: {current_copies}\")\n",
    "    \n",
    "    # Make concurrent requests\n",
    "    print(f\"Sending {requests_per_iteration} concurrent requests...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=requests_per_iteration) as executor:\n",
    "        futures = [\n",
    "            executor.submit(make_prediction, llama_predictor, payload, f\"{iteration}-{i+1}\") \n",
    "            for i in range(requests_per_iteration)\n",
    "        ]\n",
    "        \n",
    "        results = [future.result() for future in as_completed(futures)]\n",
    "    \n",
    "    # Summary\n",
    "    successful = sum(1 for r in results if r['success'])\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    if successful > 0:\n",
    "        avg_response_time = sum(r['duration'] for r in results if r['success']) / successful\n",
    "        print(f\"✅ Results: {successful}/{requests_per_iteration} successful\")\n",
    "        print(f\"⏱️ Total: {total_time:.1f}s, Average response: {avg_response_time:.1f}s\")\n",
    "    else:\n",
    "        print(f\"❌ All requests failed\")\n",
    "    \n",
    "    # Show any failures\n",
    "    failures = [r for r in results if not r['success']]\n",
    "    if failures:\n",
    "        print(f\"⚠️  {len(failures)} requests failed\")\n",
    "    \n",
    "    # Wait between iterations (except last one)\n",
    "    if iteration < num_iterations:\n",
    "        print(f\"⏳ Waiting {wait_between_iterations}s before next iteration...\")\n",
    "        time.sleep(wait_between_iterations)\n",
    "\n",
    "print(f\"\\n Inference load complete!\")\n",
    "print(\"Watch how the inference component count changed during the load test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ef5fc6-5207-413a-b8ff-032018821115",
   "metadata": {},
   "source": [
    "#### Check that our model has scaled to 2 copies\n",
    "\n",
    "> **Note** you can rerun the cell above and increase the number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd01da0-2ae4-4355-be66-c6591bfea02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    desc = sagemaker_client.describe_inference_component(InferenceComponentName=llama_inference_component_name)\n",
    "    status = desc[\"InferenceComponentStatus\"]\n",
    "    print(status)\n",
    "    sys.stdout.flush()\n",
    "    if status in [\"InService\", \"Failed\"]:\n",
    "        break\n",
    "    time.sleep(30)\n",
    "\n",
    "current_copies = get_current_copy_count(llama_predictor.component_name)\n",
    "print(f\"Current Inference Components: {current_copies}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c396fdc6-5f66-41d8-98ab-f403e8920fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload={\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"What is deep learning?\"}\n",
    "    ],\n",
    "    \"max_tokens\": 100,\n",
    "}\n",
    "try:\n",
    "    response = llama_predictor.predict(payload)\n",
    "    print(response['choices'][0]['message']['content'])\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"   Reason: {str(e)}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb051a35-69ad-42c4-875d-a9c21c35e112",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-24T21:46:50.557381Z",
     "iopub.status.busy": "2025-08-24T21:46:50.557056Z",
     "iopub.status.idle": "2025-08-24T21:46:50.560177Z",
     "shell.execute_reply": "2025-08-24T21:46:50.559678Z",
     "shell.execute_reply.started": "2025-08-24T21:46:50.557361Z"
    }
   },
   "source": [
    "### Note: \n",
    "**If you do not have a multi-GPU instance, you can skip to the cleanup section**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07573401-0367-49b0-9b62-6c352752a030",
   "metadata": {},
   "source": [
    "## (OPTIONAL MULTI-GPUs INSTANCE) Deploy Mistral 7B Instruct model on the same endpoint\n",
    "\n",
    "Now we will deploy Mistral 7B Instruct model on the same endpoint previously created with the llama model. \n",
    "> **Please note that you require a multi-GPU machine for this to work**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9776543-91fc-4a3f-9c2a-a453e1319eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serve.builder.model_builder import ModelBuilder\n",
    "from sagemaker.serve.builder.schema_builder import SchemaBuilder\n",
    "import logging\n",
    "\n",
    "prompt = \"Falcons are\"\n",
    "response = \"Falcons are small to medium-sized birds of prey related to hawks and eagles.\"\n",
    "\n",
    "model_id = \"huggingface-llm-mistral-7b-instruct\"\n",
    "\n",
    "model_name = sagemaker.utils.name_from_base(\"workshop-lab-3-mistral\")\n",
    "\n",
    "mistral_model_builder = ModelBuilder(\n",
    "    model=model_id,\n",
    "    name=model_name,\n",
    "    role_arn=role,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    schema_builder=SchemaBuilder(sample_input=prompt, sample_output=response),\n",
    "    log_level=logging.WARN\n",
    ")\n",
    "\n",
    "output_path = f\"s3://{model_bucket}/ws-mistral-7b-instruct/sharding\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e652e25-5ec3-49d1-9c2c-40ea23f41779",
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "\n",
    "# Set the mistral model image to LMI\n",
    "mistral_model_builder._get_default_vllm_image = types.MethodType(\n",
    "    lambda self, image: inference_image, \n",
    "    mistral_model_builder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab14aa1-bf72-4fdc-be9d-86caa52a9854",
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_model_builder.optimize(\n",
    "    instance_type=gpu_instance_type,\n",
    "    accept_eula=True, \n",
    "    output_path=output_path,\n",
    "    env_vars={\n",
    "        \"OPTION_MAX_MODEL_LEN\": \"12384\",\n",
    "        \"OPTION_GPU_MEMORY_UTILIZATION\": \"0.87\",\n",
    "    },\n",
    "    sharding_config={\n",
    "            \"Image\": inference_image,\n",
    "            \"OverrideEnvironment\": {\n",
    "                \"OPTION_TENSOR_PARALLEL_DEGREE\": \"1\" # Number of GPU available on the instance\n",
    "            }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3794a19e-5b90-4200-b827-7ee3b7f51728",
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_model = mistral_model_builder.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e0d171-b9e0-4077-ad69-c83d0e8529eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.enums import EndpointType\n",
    "from sagemaker.compute_resource_requirements.resource_requirements import ResourceRequirements\n",
    "\n",
    "\n",
    "resources = ResourceRequirements(\n",
    "    requests = {\n",
    "        \"num_accelerators\": 1, # Number of accelerators required\n",
    "        \"memory\": 10409,  # Minimum memory required in Mb (required)\n",
    "        \"copies\": 1,\n",
    "    }\n",
    ")\n",
    "\n",
    "mistral_model.deploy(\n",
    "    endpoint_name=llama_model.endpoint_name,\n",
    "    accept_eula=True,\n",
    "    initial_instance_count=1,\n",
    "    instance_type=gpu_instance_type,\n",
    "    endpoint_type=EndpointType.INFERENCE_COMPONENT_BASED,\n",
    "    resources=resources,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5a8c6a-e8fc-42ea-89d1-6bb2194d8042",
   "metadata": {},
   "source": [
    "#### Invoke the mistral model with a sample prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac89ba0-ce38-4755-a288-e0813496b3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "from sagemaker.predictor import retrieve_default \n",
    "\n",
    "endpoint_name = mistral_model.endpoint_name \n",
    "mistral_inference_component_name = mistral_model.inference_component_name\n",
    "\n",
    "mistral_predictor = retrieve_default(endpoint_name, inference_component_name=mistral_inference_component_name) \n",
    "\n",
    "# Define the prompt and other parameters\n",
    "prompt = \"\"\"\n",
    "<s>[INST] Below is the question based on the context. \n",
    "Question: Given a reference text about Lollapalooza, where does it take place, who started it and what is it?. \n",
    "Below is the given the context Lollapalooza /ˌlɒləpəˈluːzə/ (Lolla) is an annual American four-day music festival held in Grant Park in Chicago. \n",
    "It originally started as a touring event in 1991, but several years later, Chicago became its permanent location. Music genres include but are not limited to alternative rock, heavy metal, punk rock, hip hop, and electronic dance music. Lollapalooza has also featured visual arts, nonprofit organizations, and political organizations. \n",
    "The festival, held in Grant Park, hosts an estimated 400,000 people each July and sells out annually. Lollapalooza is one of the largest and most iconic music festivals in the world and one of the longest-running in the United States. Lollapalooza was conceived and created in 1991 as a farewell tour by Perry Farrell, singer of the group Jane's Addiction.. \n",
    "Write a response that appropriately completes the request.[/INST]\n",
    "\"\"\"\n",
    " \n",
    "max_tokens_to_sample = 200\n",
    "\n",
    "# parameters for llm\n",
    "parameters = {\n",
    "    \"max_new_tokens\": max_tokens_to_sample,\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.9,\n",
    "    \"temperature\": 0.5,\n",
    "    \"strean\": True,\n",
    "}\n",
    "\n",
    "contentType = 'application/json'\n",
    "\n",
    "payload = {\n",
    "    'inputs': prompt,\n",
    "    \"parameters\": parameters\n",
    "}\n",
    "\n",
    "response = mistral_predictor.predict(payload)\n",
    "print(response[\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab5f580-289a-4989-86d3-08a8721040fb",
   "metadata": {},
   "source": [
    "### Setup autoscaling for Mistral model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074c6a35-72b8-400f-bbdd-fadbcad393dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_inference_component_name = mistral_model.inference_component_name\n",
    "print(f\"Mistral inference component: {mistral_inference_component_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efecdcc-e804-4acb-bf09-c980c0b100ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply autoscaling configuration\n",
    "setup_autoscaling(mistral_inference_component_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de860dd7-eb9f-4f7f-ad48-72e410d1ef93",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_copies = get_current_copy_count(mistral_predictor.component_name)\n",
    "print(f\"Current Inference Components: {current_copies}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095d08e7-a532-4225-bdee-5463535fa3c3",
   "metadata": {},
   "source": [
    "### Understanding Model Copies and scale-to-zero\n",
    "\n",
    "**Important** To scale to zero on an endpoint with multiple inference components, all components must be either set to 0 or deleted.\n",
    "\n",
    "**Model Copy**: One loaded instance of your model in GPU memory\n",
    "- Copies can share GPU instances (depending on model size and available accelerators)\n",
    "\n",
    "**Instance**: The underlying compute resource (e.g., ml.g5.2xlarge)\n",
    "- SageMaker automatically manages instances based on copy demands\n",
    "- Multiple small model copies can share one instance\n",
    "- Large model copies might need dedicated instances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14ffab4-db77-467f-b472-a621c229adee",
   "metadata": {},
   "source": [
    "# Clean up the environment\n",
    "\n",
    "- Deregister scalable target\n",
    "- Delete cloudwatch alarms\n",
    "- Delete scaling policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149a50d9-a9e9-4bbc-bee0-bd919273a97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.cleanup import cleanup_autoscaling, cleanup_workshop_resources\n",
    "\n",
    "# Clean up autoscaling\n",
    "workshop_components = []\n",
    "if 'llama_inference_component_name' in locals():\n",
    "    workshop_components.append(llama_inference_component_name)\n",
    "if 'mistral_inference_component_name' in locals():\n",
    "    workshop_components.append(mistral_inference_component_name)\n",
    "\n",
    "if workshop_components:\n",
    "    cleanup_autoscaling(workshop_components, aas_client, cloudwatch_client)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f8d9e6-953d-47a8-94fd-4f142be39132",
   "metadata": {},
   "source": [
    "- Delete inference component\n",
    "- Delete endpoint\n",
    "- Delete endpoint-config\n",
    "- Delete model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53f2e16-f136-4b23-b4d3-187b31a0e500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up other resources\n",
    "\n",
    "cleanup_workshop_resources(llama_model.name, endpoint_config_name, endpoint_name, sagemaker_client)\n",
    "\n",
    "print(\"✅ Workshop cleanup complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
