{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "993dcf59-dd85-4a31-a8cc-02c5d4be5dfe",
   "metadata": {},
   "source": [
    "# Deploy a model on SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c01a4b2-b0d3-420c-8335-4efd984bb5dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sagemaker --upgrade --quiet --no-warn-conflicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9183bf15-3f3f-4d70-91da-fe269ff421b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "sagemaker role arn: arn:aws:iam::736221153822:role/SageMaker-ServiceRole-Default\n",
      "sagemaker bucket: sagemaker-us-east-1-736221153822\n",
      "sagemaker session region: us-east-1\n",
      "sagemaker version: 2.246.0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "\n",
    "sm_client = boto3.client(\"sagemaker\")  # client to intreract with SageMaker\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")  # client to intreract with SageMaker Endpoints\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n",
    "print(f\"sagemaker version: {sagemaker.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e063eecf-fc9f-4b64-8dfe-a394e178ec75",
   "metadata": {},
   "source": [
    "## HF container with default handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc183a7c-bb78-43c0-b477-7659ba3240df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "model_name = sagemaker.utils.name_from_base(\"model\")\n",
    "endpoint_name = model_name\n",
    "\n",
    "# Hub Model configuration. https://huggingface.co/models\n",
    "hub = {\n",
    "\t\"HF_MODEL_ID\": \"openai/whisper-large-v2\",\n",
    "\t\"HF_TASK\": \"automatic-speech-recognition\"\n",
    "}\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "\tname=model_name,\n",
    "    transformers_version='4.49.0',\n",
    "\tpytorch_version='2.6.0',\n",
    "\tpy_version='py312',\n",
    "\tenv=hub,\n",
    "\trole=role, \n",
    ")\n",
    "\n",
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "\tinitial_instance_count=1, # number of instances\n",
    "\tinstance_type='ml.g6e.2xlarge', # ec2 instance type\n",
    "    endpoint_name=endpoint_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4325ccec-5a67-4018-bdb2-bcb40c970812",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.jumpstart import utils\n",
    "\n",
    "# The wav files must be sampled at 16kHz (this is required by the automatic speech recognition models), so make sure to resample them if required. The input audio file must be less than 30 seconds.\n",
    "s3_bucket = utils.get_jumpstart_content_bucket()\n",
    "key_prefix = \"training-datasets/asr_notebook_data\"\n",
    "input_audio_file_name = \"sample1.wav\"\n",
    "\n",
    "s3_client.download_file(s3_bucket, f\"{key_prefix}/{input_audio_file_name }\", input_audio_file_name)\n",
    "\n",
    "input_audio_file_name = \"sample_french1.wav\"\n",
    "\n",
    "s3_client.download_file(s3_bucket, f\"{key_prefix}/{input_audio_file_name }\", input_audio_file_name )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "469cdedd-6b88-4335-bd70-d99e0b6e9b40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \" We are living in very exciting times with machine learning. The speed of ML model development will really actually increase, but you won't get to that end state that we want in the next coming years unless we actually make these models more accessible to everybody.\"}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.serializers import DataSerializer\n",
    "\t\n",
    "predictor.serializer = DataSerializer(content_type='audio/x-audio')\n",
    "predictor.content_type = \"audio/x-audio\"\n",
    "\n",
    "# Make sure the input file \"sample1.flac\" exists\n",
    "with open(input_audio_file_name, \"rb\") as f:\n",
    "\tdata = f.read()\n",
    "predictor.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ceb5da-3499-41cd-b532-0e43ed485d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.delete_endpoint(endpoint_name)\n",
    "sess.delete_endpoint_config(endpoint_name)\n",
    "sess.delete_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1836005c-711d-4bf8-93cf-5dda80ab00b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1634e71f-a190-408f-bcb9-a314cd057e30",
   "metadata": {},
   "source": [
    "## HF container with custom handler\n",
    "\n",
    "Model is deployed from HF hub. Custom handler is placed on S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1df4d0d2-95b3-4916-9985-810fc5624dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------!"
     ]
    }
   ],
   "source": [
    "# OVERRIDE:\n",
    "image_uri = f\"763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-inference:2.6.0-transformers4.49.0-gpu-py312-cu124-ubuntu22.04\"\n",
    "model_name = sagemaker.utils.name_from_base(\"model\")\n",
    "endpoint_name = model_name\n",
    "\n",
    "# sagemaker config\n",
    "instance_type = \"ml.g6e.4xlarge\"\n",
    "health_check_timeout = 900\n",
    "\n",
    "model = sagemaker.Model(\n",
    "\trole=role, \n",
    "    name=model_name,\n",
    "    image_uri=image_uri,\n",
    "    model_data={\n",
    "        'S3DataSource': {\n",
    "            'S3Uri': \"s3://dsoldat-ml/models/openai/whisper-large-v3-turbo/\",\n",
    "            'S3DataType': 'S3Prefix',\n",
    "            'CompressionType': 'None'\n",
    "        }\n",
    "    },\n",
    "    env={\n",
    "        \"HF_MODEL_ID\": \"openai/whisper-large-v3\"\n",
    "    },\n",
    "    sagemaker_session=sess,\n",
    ")\n",
    "\n",
    "# Deploy model to an endpoint\n",
    "model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    container_startup_health_check_timeout=health_check_timeout,\n",
    "    endpoint_name=endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8d0c37f0-6a2e-4a0c-8630-70429dacaabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_endpoint(body, content_type):\n",
    "    response = smr_client.invoke_endpoint(EndpointName=endpoint_name, ContentType=content_type, Body=body)\n",
    "    model_predictions = json.loads(response['Body'].read())\n",
    "    print(json.dumps(model_predictions, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "72c6a984-289d-44f2-9a42-5ecbf35744ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"text\": [\n",
      "    \" We are living in very exciting times with machine learning. The speed of ML model development will really actually increase. But you won't get to that end state that we want in the next coming years unless we actually make these models more accessible to everybody.\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "input_audio_file_name = \"sample1.wav\"\n",
    "\n",
    "with open(input_audio_file_name, \"rb\") as file:\n",
    "    wav_file_read = file.read()\n",
    "\n",
    "query_endpoint(wav_file_read, \"audio/wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "95772fc1-b49c-467e-86be-e4bf5df18dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"text\": [\n",
      "    \" Welcome to JPB Syst\\u00e8mes. Here, we have more than 150 employees, more than 90% of the turnover at export, and we have developed about 15 products.\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "input_audio_file_name = \"sample_french1.wav\"\n",
    "\n",
    "with open(input_audio_file_name, \"rb\") as file:\n",
    "    wav_file_read = file.read()\n",
    "\n",
    "payload = {\"audio_input\": wav_file_read.hex(),\n",
    "           \"language\": \"french\",\n",
    "           \"task\": \"translate\"}\n",
    "\n",
    "query_endpoint(json.dumps(payload).encode('utf-8'), \"application/json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d016907e-768c-4485-b49b-ad50a20a4e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.delete_endpoint(endpoint_name)\n",
    "sess.delete_endpoint_config(endpoint_name)\n",
    "sess.delete_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2e6bac-d260-4383-92f4-3393adfb3354",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
