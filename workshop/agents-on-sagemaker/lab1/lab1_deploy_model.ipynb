{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab6ee147-f1a8-475b-9d8d-f3924cbcd367",
   "metadata": {},
   "source": [
    "# re:Invent 2025 Workshop\n",
    "# AIM406 - Build agentic workflows with Small Language Models and SageMaker AI\n",
    "# Lab 1 - Model deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d17da10-653a-4e10-9c18-da23bfea450e",
   "metadata": {},
   "source": [
    "Leverage open-source models such as Qwen3 to build AI agents with frameworks like Langraph and Strands. Create autonomous agents that reason, plan, and execute complex tasks through advanced prompt engineering and state management. Get hands-on experience with tool calling and MCP integration patterns to manage context between different tools, enabling seamless multi-step workflows. Learn to use Bedrock AgentCore to deploy agents for real-time use cases with security, observability and scale. Integration with LLMs deployed in SageMaker AI, Bedrock AgentCore, and open-source frameworks, learn to build production-ready AI agents that deliver measurable business value while maintaining security, scalability, and cost efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9a94f3-f421-4e86-bb6e-2301c078f985",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T17:38:29.359715Z",
     "iopub.status.busy": "2025-10-16T17:38:29.359467Z",
     "iopub.status.idle": "2025-10-16T17:38:29.510579Z",
     "shell.execute_reply": "2025-10-16T17:38:29.509957Z",
     "shell.execute_reply.started": "2025-10-16T17:38:29.359694Z"
    }
   },
   "source": [
    "![AWS Agentic Portfolio](img/aim406_agentic_portfolio.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993dcf59-dd85-4a31-a8cc-02c5d4be5dfe",
   "metadata": {},
   "source": [
    "## Deploy a model on Amazon SageMaker AI endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41df58ed-7886-4525-b739-c576ee54f666",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T17:40:50.059771Z",
     "iopub.status.busy": "2025-10-16T17:40:50.059506Z",
     "iopub.status.idle": "2025-10-16T17:40:50.062541Z",
     "shell.execute_reply": "2025-10-16T17:40:50.062074Z",
     "shell.execute_reply.started": "2025-10-16T17:40:50.059746Z"
    }
   },
   "source": [
    "### Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c01a4b2-b0d3-420c-8335-4efd984bb5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sagemaker --upgrade --quiet --no-warn-conflicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9183bf15-3f3f-4d70-91da-fe269ff421b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "\n",
    "sm_client = boto3.client(\"sagemaker\")  # client to intreract with SageMaker\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")  # client to intreract with SageMaker Endpoints\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n",
    "print(f\"sagemaker version: {sagemaker.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c44ad6-de5f-42c9-acef-67d525741307",
   "metadata": {},
   "source": [
    "### Create a SageMaker model object and deploy it on endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c78dc13-9629-4f0e-9eb3-a62f03d4e26f",
   "metadata": {},
   "source": [
    "![SageMaker Endpoint](img/aim406_sagemaker_endpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5318e2c4-1af3-444f-9d69-104c6cf50c41",
   "metadata": {},
   "source": [
    "Select one of the available Large Model Inference (LMI) container images for hosting. In this workshop we are going to use latest (at the time of workshop) LMI v16 (`0.34.0-lmi16.0.0-cu-128`) container image. Ensure that you are using the image URI for the region that corresponds with your deployment region.\n",
    "\n",
    "When using LMI container we can set inference parameters using environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a45fc1b-2037-4321-90fa-a109f16bf5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTAINER_VERSION = \"0.34.0-lmi16.0.0-cu128\"\n",
    "inference_image = f\"763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:{CONTAINER_VERSION}\"\n",
    "\n",
    "model_id = \"Qwen/Qwen3-4B\"\n",
    "instance = {\"type\": \"ml.g5.2xlarge\", \"num_gpu\": 1}\n",
    "\n",
    "model_name = sagemaker.utils.name_from_base(\"model\", short=True)\n",
    "endpoint_name = model_name\n",
    "endpoint_config_name = model_name\n",
    "\n",
    "timeout = 600\n",
    "\n",
    "common_env = {\n",
    "    \"HF_MODEL_ID\": model_id,\n",
    "}\n",
    "lmi_env = {\n",
    "    \"SERVING_FAIL_FAST\": \"true\",\n",
    "    \"OPTION_ASYNC_MODE\": \"true\",\n",
    "    \"OPTION_ROLLING_BATCH\": \"disable\",\n",
    "    \"OPTION_ENTRYPOINT\": \"djl_python.lmi_vllm.vllm_async_service\",\n",
    "    \"OPTION_TENSOR_PARALLEL_DEGREE\": json.dumps(instance[\"num_gpu\"]),\n",
    "    \"OPTION_MAX_MODEL_LEN\": \"16384\",\n",
    "    \"OPTION_TRUST_REMOTE_CODE\": \"true\",\n",
    "    \"OPTION_ENABLE_AUTO_TOOL_CHOICE\": \"true\",\n",
    "    \"OPTION_TOOL_CALL_PARSER\": \"hermes\", #\"qwen3_xml\"\n",
    "}\n",
    "env = common_env | lmi_env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77878dc8-ad4c-4af3-a520-3aca150b59de",
   "metadata": {},
   "source": [
    "#### Deploy the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69320330-f4f2-43e5-bb09-d087515cb6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "lmi_model = sagemaker.Model(\n",
    "    image_uri=inference_image,\n",
    "    env=env,\n",
    "    role=role,\n",
    "    name=model_name,\n",
    ")\n",
    "\n",
    "lmi_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance[\"type\"],\n",
    "    container_startup_health_check_timeout=timeout,\n",
    "    endpoint_name=endpoint_name,\n",
    ")\n",
    "\n",
    "llm = sagemaker.Predictor(\n",
    "    endpoint_name = endpoint_name,\n",
    "    sagemaker_session = sess,\n",
    "    serializer = sagemaker.serializers.JSONSerializer(),\n",
    "    deserializer = sagemaker.deserializers.JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d690831e-c983-4fdb-8c1d-9dcba1224102",
   "metadata": {},
   "source": [
    "#### View logs for the base inference component (and adapters after they're loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ead4070-d04c-458e-928b-15cc1f168268",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "cw_path = urllib.parse.quote_plus(f'/aws/sagemaker/Endpoints/{endpoint_name}', safe='', encoding=None, errors=None)\n",
    "\n",
    "print(f'You can view your inference component logs here:\\n\\n https://{region}.console.aws.amazon.com/cloudwatch/home?region={region}#logsV2:log-groups/log-group/{cw_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8742295-1e55-467e-9700-a4f60681c9fc",
   "metadata": {},
   "source": [
    "### Test inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03335b9c-19eb-4a15-b5bd-6033f95cf6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload={\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"What is bigger 9.11 or 9.8?\"}\n",
    "    ],\n",
    "}\n",
    "res = llm.predict(payload)\n",
    "print(\"-----\\n\" + res[\"choices\"][0][\"message\"][\"content\"] + \"\\n-----\\n\")\n",
    "print(res[\"usage\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22205573-5583-4b13-bf01-a08a5ec7fb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import time\n",
    "import boto3\n",
    "from IPython.display import clear_output\n",
    "\n",
    "class LineIterator:\n",
    "    def __init__(self, stream):\n",
    "        self.byte_iterator = iter(stream)\n",
    "        self.buffer = io.BytesIO()\n",
    "        self.read_pos = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        while True:\n",
    "            self.buffer.seek(self.read_pos)\n",
    "            line = self.buffer.readline()\n",
    "            if line and line[-1] == ord(\"\\n\"):\n",
    "                self.read_pos += len(line)\n",
    "                return line[:-1]\n",
    "            try:\n",
    "                chunk = next(self.byte_iterator)\n",
    "            except StopIteration:\n",
    "                if self.read_pos < self.buffer.getbuffer().nbytes:\n",
    "                    continue\n",
    "                raise\n",
    "            if \"PayloadPart\" not in chunk:\n",
    "                print(\"Unknown event type:\" + chunk)\n",
    "                continue\n",
    "            self.buffer.seek(0, io.SEEK_END)\n",
    "            self.buffer.write(chunk[\"PayloadPart\"][\"Bytes\"])\n",
    "\n",
    "def stream_response(endpoint_name, inputs, max_tokens=8189, temperature=0.7, top_p=0.9):\n",
    "    body = {\n",
    "      \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": inputs}]}\n",
    "        ],\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p,\n",
    "        \"stream\": True,\n",
    "    }\n",
    "\n",
    "    resp = smr_client.invoke_endpoint_with_response_stream(\n",
    "        EndpointName=endpoint_name,\n",
    "        Body=json.dumps(body),\n",
    "        ContentType=\"application/json\",\n",
    "    )\n",
    "\n",
    "    event_stream = resp[\"Body\"]\n",
    "    start_json = b\"{\"\n",
    "    full_response = \"\"\n",
    "    start_time = time.time()\n",
    "    token_count = 0\n",
    "\n",
    "    for line in LineIterator(event_stream):\n",
    "        if line != b\"\" and start_json in line:\n",
    "            data = json.loads(line[line.find(start_json):].decode(\"utf-8\"))\n",
    "            token_text = data['choices'][0]['delta'].get('content', '')\n",
    "            full_response += token_text\n",
    "            token_count += 1\n",
    "\n",
    "            # Calculate tokens per second\n",
    "            elapsed_time = time.time() - start_time\n",
    "            tps = token_count / elapsed_time if elapsed_time > 0 else 0\n",
    "\n",
    "            # Clear the output and reprint everything\n",
    "            clear_output(wait=True)\n",
    "            print(full_response)\n",
    "            print(f\"\\nTokens per Second: {tps:.2f}\", end=\"\")\n",
    "\n",
    "    print(\"\\n\") # Add a newline after response is complete\n",
    "\n",
    "    return full_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5429f0db-e4fd-4492-ade1-113cd7b4b9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = \"What is greater 9.11 or 9.8?\"\n",
    "output = stream_response(endpoint_name, inputs, max_tokens=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3aa64be-c349-44ee-a7be-8e2200307f36",
   "metadata": {},
   "source": [
    "## (Optional) Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daccb9fd-5723-4fe7-b172-91f3bb72de89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T18:33:07.769095Z",
     "iopub.status.busy": "2025-10-16T18:33:07.768841Z",
     "iopub.status.idle": "2025-10-16T18:33:07.771621Z",
     "shell.execute_reply": "2025-10-16T18:33:07.771086Z",
     "shell.execute_reply.started": "2025-10-16T18:33:07.769076Z"
    }
   },
   "source": [
    "#### DO NOT RUN THE CELL BELOW IF YOU WANT TO RUN LAB2 IN THE WORKSHOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90af74a3-54f2-4576-8fac-9095ebb086cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.delete_endpoint(endpoint_name)\n",
    "sess.delete_endpoint_config(endpoint_name)\n",
    "sess.delete_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44b3b43-ba81-4fa7-a13c-8fc36c781c66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
