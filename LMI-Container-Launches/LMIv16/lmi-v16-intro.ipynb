{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d4e6de4-d521-4811-b964-e282760449d5",
   "metadata": {},
   "source": [
    "# LMI v16 Intro Notebook\n",
    "In this example we showcase how you can setup the LMI v16 container utilizing the vLLM backend. Note that there are more samples around some of the newer features/support with v16, that you can reference down below:\n",
    "\n",
    "- [Custom Input/Output Formatters](https://github.com/deepjavalibrary/djl-serving/blob/0.34.0-dlc/serving/docs/lmi/user_guides/input_formatter_schema.md)\n",
    "- [Multi-Adapter Inference](https://github.com/deepjavalibrary/djl-serving/blob/0.34.0-dlc/serving/docs/adapters.md)\n",
    "- [Sticky Session Routing](https://github.com/deepjavalibrary/djl-serving/blob/0.34.0-dlc/serving/docs/stateful_sessions.md)\n",
    "\n",
    "## Prerequisites\n",
    "To work with this notebook ensure that you have a [HuggingFace Access Token](https://huggingface.co/docs/hub/en/security-tokens) to access model artifacts. You can run this notebook in SM Classic Notebook Instances, SM Studio, or an IDE that has access to the necessary services we are working with.\n",
    "\n",
    "## Additional Resources\n",
    "A lot of this code is borrowed from other samples in this repository and adjusted for this specific model and container: https://github.com/aws-samples/sagemaker-genai-hosting-examples/blob/main/OpenAI/gpt-oss/deploy/openai_gpt_oss.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e0ce41-9028-4203-b738-4e3e2c76a76e",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a74ffa3-e9cf-4421-b093-0bc2967e5a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sagemaker --upgrade --quiet --no-warn-conflicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36ab6ce-e061-4d7f-8594-11c4ad791282",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "account_id = sess.account_id()\n",
    "\n",
    "sm_client = boto3.client(\"sagemaker\")  # client to intreract with SageMaker\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")  # client to intreract with SageMaker Endpoints\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n",
    "print(f\"sagemaker version: {sagemaker.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f51980-4279-4018-9653-a77ff98ce297",
   "metadata": {},
   "source": [
    "## Specify Container and vLLM Serving Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a761698-0a65-40b4-a13f-55066aeac220",
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify hardware\n",
    "instance_type = \"ml.g5.4xlarge\"\n",
    "num_gpu = 1\n",
    "\n",
    "# specify container LMIv16\n",
    "CONTAINER_VERSION = \"0.34.0-lmi16.0.0-cu128\"\n",
    "inference_image = f\"763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:{CONTAINER_VERSION}\"\n",
    "print(f\"Using image URI: {inference_image}\")\n",
    "\n",
    "#utilize the vLLM async handler: \n",
    "vllm_env = {\n",
    "    \"HF_MODEL_ID\": \"Qwen/Qwen3-1.7B\",\n",
    "    \"HF_TOKEN\": \"Enter HF token here\",\n",
    "    \"SERVING_FAIL_FAST\": \"true\",\n",
    "    \"OPTION_ASYNC_MODE\": \"true\",\n",
    "    \"OPTION_ROLLING_BATCH\": \"disable\",\n",
    "    \"OPTION_TENSOR_PARALLEL_DEGREE\": json.dumps(num_gpu),\n",
    "    \"OPTION_ENTRYPOINT\": \"djl_python.lmi_vllm.vllm_async_service\",\n",
    "    \"OPTION_TRUST_REMOTE_CODE\": \"true\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96476afd-3bec-43ca-8ba4-d1254953ec08",
   "metadata": {},
   "source": [
    "## Create Inference Component and Endpoint\n",
    "Here we utilize the higher level SageMaker Python SDK to create both an endpoint and an Inference Component, adjust the resource requirements depending on your model and use-case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecdf260-e3f8-4cbf-bb91-b66b468c32d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.compute_resource_requirements.resource_requirements import ResourceRequirements\n",
    "\n",
    "# SageMaker Constructs\n",
    "model_name = sagemaker.utils.name_from_base(\"model-lmi\")\n",
    "endpoint_name = model_name\n",
    "inference_component_name = f\"ic-{model_name}\"\n",
    "\n",
    "# SageMaker Model Object -> vLLM env\n",
    "lmi_model = sagemaker.Model(\n",
    "    image_uri=inference_image,\n",
    "    env=vllm_env,\n",
    "    role=role,\n",
    "    name=model_name,\n",
    ")\n",
    "\n",
    "lmi_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    container_startup_health_check_timeout=600,\n",
    "    endpoint_name=endpoint_name,\n",
    "    endpoint_type=sagemaker.enums.EndpointType.INFERENCE_COMPONENT_BASED,\n",
    "    inference_component_name=inference_component_name,\n",
    "    #check the memory available for your instance, g5.4xlarge for instance has 24GB GPU memory and 64GB memory\n",
    "    resources=ResourceRequirements(requests={\"num_accelerators\": 1, \"memory\": 1024*10, \"copies\": 1,}),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107103c7-2f52-44be-b433-550966736ebf",
   "metadata": {},
   "source": [
    "## Invoke Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df665c7c-3c16-4af3-8965-21416615610e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "content_type = \"application/json\"\n",
    "\n",
    "# Adjust payload and parameters as needed\n",
    "payload = \"What is the capitol of the United States?\"\n",
    "response = smr_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    InferenceComponentName=inference_component_name, #specify IC name\n",
    "    ContentType=content_type,\n",
    "    Accept=content_type,\n",
    "    Body=json.dumps(\n",
    "        {\n",
    "            \"inputs\": payload,\n",
    "            \"parameters\": {\n",
    "                \"max_new_tokens\": 200  # Adjust this value as needed\n",
    "                },\n",
    "        }\n",
    "    ),\n",
    ")\n",
    "result = json.loads(response[\"Body\"].read().decode())['generated_text']\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55daae4-83dd-46c9-be01-7472e093d13e",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "You can also delete via the Studio UI under deployments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3c61aa-01b0-4834-a088-e9163c4fbc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.delete_inference_component(inference_component_name, wait=True)\n",
    "sess.delete_endpoint(endpoint_name)\n",
    "sess.delete_endpoint_config(endpoint_name)\n",
    "sess.delete_model(model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
